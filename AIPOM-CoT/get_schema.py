import json
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict

from neo4j import GraphDatabase
from loguru import logger
import pandas as pd
from tqdm import tqdm


class Neo4jSchemaExtractor:
    """Neo4jçŸ¥è¯†å›¾è°±Schemaæå–å™¨"""

    def __init__(self, uri: str = "bolt://localhost:7687",
                 user: str = "neo4j",
                 password: str = "password",
                 database: str = "neo4j"):
        """
        åˆå§‹åŒ–è¿æ¥

        å‚æ•°:
            uri: Neo4j URI
            user: ç”¨æˆ·å
            password: å¯†ç 
            database: æ•°æ®åº“åç§°
        """
        self.uri = uri
        self.user = user
        self.password = password
        self.database = database
        self.driver = None

        # å­˜å‚¨schemaä¿¡æ¯
        self.schema = {
            'nodes': {},
            'relationships': {},
            'statistics': {}
        }

    def connect(self) -> bool:
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        try:
            self.driver = GraphDatabase.driver(
                self.uri,
                auth=(self.user, self.password)
            )
            # æµ‹è¯•è¿æ¥
            with self.driver.session(database=self.database) as session:
                result = session.run("RETURN 1 as test")
                result.single()
            logger.info(f"âœ“ æˆåŠŸè¿æ¥åˆ°Neo4j: {self.uri}/{self.database}")
            return True
        except Exception as e:
            logger.error(f"âœ— è¿æ¥å¤±è´¥: {e}")
            return False

    def close(self):
        """å…³é—­è¿æ¥"""
        if self.driver:
            self.driver.close()
            logger.info("âœ“ è¿æ¥å·²å…³é—­")

    def extract_node_labels(self) -> List[str]:
        """æå–æ‰€æœ‰èŠ‚ç‚¹æ ‡ç­¾"""
        logger.info("æå–èŠ‚ç‚¹æ ‡ç­¾...")

        with self.driver.session(database=self.database) as session:
            result = session.run("CALL db.labels()")
            labels = [record['label'] for record in result]

        logger.info(f"  æ‰¾åˆ° {len(labels)} ç§èŠ‚ç‚¹ç±»å‹")
        return sorted(labels)

    def extract_node_properties(self, label: str) -> Dict:
        """
        æå–æŒ‡å®šèŠ‚ç‚¹ç±»å‹çš„æ‰€æœ‰å±æ€§

        è¿”å›:
            {
                'properties': {prop_name: {type, sample_value, non_null_count}},
                'count': èŠ‚ç‚¹æ€»æ•°,
                'sample_nodes': æ ·ä¾‹èŠ‚ç‚¹åˆ—è¡¨
            }
        """
        logger.info(f"  åˆ†æèŠ‚ç‚¹ç±»å‹: {label}")

        with self.driver.session(database=self.database) as session:
            # 1. è·å–èŠ‚ç‚¹æ€»æ•°
            count_query = f"MATCH (n:{label}) RETURN count(n) as count"
            count_result = session.run(count_query)
            total_count = count_result.single()['count']

            # 2. è·å–æ ·ä¾‹èŠ‚ç‚¹ï¼ˆæœ€å¤š100ä¸ªï¼‰
            sample_query = f"MATCH (n:{label}) RETURN n LIMIT 100"
            sample_result = session.run(sample_query)
            sample_nodes = [dict(record['n']) for record in sample_result]

            # 3. åˆ†æå±æ€§
            properties = {}
            if sample_nodes:
                # æ”¶é›†æ‰€æœ‰å±æ€§å
                all_props = set()
                for node in sample_nodes:
                    all_props.update(node.keys())

                # åˆ†ææ¯ä¸ªå±æ€§
                for prop in all_props:
                    # ç»Ÿè®¡éç©ºå€¼æ•°é‡
                    non_null_query = f"""
                    MATCH (n:{label})
                    WHERE n.{prop} IS NOT NULL
                    RETURN count(n) as non_null_count
                    """
                    non_null_result = session.run(non_null_query)
                    non_null_count = non_null_result.single()['non_null_count']

                    # è·å–æ ·ä¾‹å€¼å’Œç±»å‹
                    sample_values = [node.get(prop) for node in sample_nodes if prop in node]
                    sample_value = sample_values[0] if sample_values else None

                    # æ¨æ–­ç±»å‹
                    prop_type = type(sample_value).__name__ if sample_value is not None else 'None'

                    properties[prop] = {
                        'type': prop_type,
                        'sample_value': sample_value,
                        'non_null_count': non_null_count,
                        'coverage': f"{non_null_count}/{total_count} ({non_null_count / total_count * 100:.1f}%)"
                    }

        return {
            'properties': properties,
            'count': total_count,
            'sample_nodes': sample_nodes[:3]  # åªä¿ç•™3ä¸ªæ ·ä¾‹
        }

    def extract_relationship_types(self) -> List[str]:
        """æå–æ‰€æœ‰å…³ç³»ç±»å‹"""
        logger.info("æå–å…³ç³»ç±»å‹...")

        with self.driver.session(database=self.database) as session:
            result = session.run("CALL db.relationshipTypes()")
            rel_types = [record['relationshipType'] for record in result]

        logger.info(f"  æ‰¾åˆ° {len(rel_types)} ç§å…³ç³»ç±»å‹")
        return sorted(rel_types)

    def extract_relationship_details(self, rel_type: str) -> Dict:
        """
        æå–æŒ‡å®šå…³ç³»ç±»å‹çš„è¯¦ç»†ä¿¡æ¯

        è¿”å›:
            {
                'count': å…³ç³»æ€»æ•°,
                'properties': {prop_name: {type, sample_value}},
                'patterns': [(source_label, target_label, count)]
                'sample_relationships': æ ·ä¾‹å…³ç³»åˆ—è¡¨
            }
        """
        logger.info(f"  åˆ†æå…³ç³»ç±»å‹: {rel_type}")

        with self.driver.session(database=self.database) as session:
            # 1. è·å–å…³ç³»æ€»æ•°
            count_query = f"MATCH ()-[r:{rel_type}]->() RETURN count(r) as count"
            count_result = session.run(count_query)
            total_count = count_result.single()['count']

            # 2. è·å–æ ·ä¾‹å…³ç³»
            sample_query = f"""
            MATCH (a)-[r:{rel_type}]->(b)
            RETURN labels(a) as source_labels, 
                   labels(b) as target_labels,
                   r,
                   a.name as source_name,
                   b.name as target_name
            LIMIT 100
            """
            sample_result = session.run(sample_query)
            sample_rels = []
            for record in sample_result:
                sample_rels.append({
                    'source_labels': record['source_labels'],
                    'target_labels': record['target_labels'],
                    'properties': dict(record['r']),
                    'source_name': record['source_name'],
                    'target_name': record['target_name']
                })

            # 3. åˆ†æå±æ€§
            properties = {}
            if sample_rels:
                all_props = set()
                for rel in sample_rels:
                    all_props.update(rel['properties'].keys())

                for prop in all_props:
                    sample_values = [
                        rel['properties'].get(prop)
                        for rel in sample_rels
                        if prop in rel['properties']
                    ]
                    sample_value = sample_values[0] if sample_values else None
                    prop_type = type(sample_value).__name__ if sample_value is not None else 'None'

                    # ç»Ÿè®¡éç©ºå€¼
                    non_null_query = f"""
                    MATCH ()-[r:{rel_type}]->()
                    WHERE r.{prop} IS NOT NULL
                    RETURN count(r) as non_null_count
                    """
                    non_null_result = session.run(non_null_query)
                    non_null_count = non_null_result.single()['non_null_count']

                    properties[prop] = {
                        'type': prop_type,
                        'sample_value': sample_value,
                        'non_null_count': non_null_count,
                        'coverage': f"{non_null_count}/{total_count} ({non_null_count / total_count * 100:.1f}%)"
                    }

            # 4. æå–å…³ç³»æ¨¡å¼ï¼ˆsource -> targetï¼‰
            pattern_query = f"""
            MATCH (a)-[r:{rel_type}]->(b)
            WITH labels(a)[0] as source, labels(b)[0] as target, count(r) as cnt
            RETURN source, target, cnt
            ORDER BY cnt DESC
            """
            pattern_result = session.run(pattern_query)
            patterns = [
                (record['source'], record['target'], record['cnt'])
                for record in pattern_result
            ]

        return {
            'count': total_count,
            'properties': properties,
            'patterns': patterns,
            'sample_relationships': sample_rels[:3]
        }

    def extract_full_schema(self):
        """æå–å®Œæ•´schema"""
        logger.info("=" * 80)
        logger.info("å¼€å§‹æå–å®Œæ•´Schema")
        logger.info("=" * 80)

        # 1. æå–èŠ‚ç‚¹ä¿¡æ¯
        logger.info("\n[1/3] æå–èŠ‚ç‚¹ä¿¡æ¯...")
        node_labels = self.extract_node_labels()

        for label in tqdm(node_labels, desc="åˆ†æèŠ‚ç‚¹"):
            self.schema['nodes'][label] = self.extract_node_properties(label)

        # 2. æå–å…³ç³»ä¿¡æ¯
        logger.info("\n[2/3] æå–å…³ç³»ä¿¡æ¯...")
        rel_types = self.extract_relationship_types()

        for rel_type in tqdm(rel_types, desc="åˆ†æå…³ç³»"):
            self.schema['relationships'][rel_type] = self.extract_relationship_details(rel_type)

        # 3. ç»Ÿè®¡ä¿¡æ¯
        logger.info("\n[3/3] è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
        self.calculate_statistics()

        logger.info("\nâœ“ Schemaæå–å®Œæˆ")

    def calculate_statistics(self):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        with self.driver.session(database=self.database) as session:
            # æ€»èŠ‚ç‚¹æ•°
            node_count_query = "MATCH (n) RETURN count(n) as count"
            total_nodes = session.run(node_count_query).single()['count']

            # æ€»å…³ç³»æ•°
            rel_count_query = "MATCH ()-[r]->() RETURN count(r) as count"
            total_rels = session.run(rel_count_query).single()['count']

            self.schema['statistics'] = {
                'total_nodes': total_nodes,
                'total_relationships': total_rels,
                'node_types': len(self.schema['nodes']),
                'relationship_types': len(self.schema['relationships']),
                'node_breakdown': {
                    label: info['count']
                    for label, info in self.schema['nodes'].items()
                },
                'relationship_breakdown': {
                    rel_type: info['count']
                    for rel_type, info in self.schema['relationships'].items()
                }
            }

    def print_schema_report(self):
        """æ‰“å°æ ¼å¼åŒ–çš„SchemaæŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("NeuroXiv 2.0 çŸ¥è¯†å›¾è°± Schema æŠ¥å‘Š")
        print("=" * 80)

        # ç»Ÿè®¡æ¦‚è§ˆ
        stats = self.schema['statistics']
        print(f"\nğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ:")
        print(f"  æ€»èŠ‚ç‚¹æ•°: {stats['total_nodes']:,}")
        print(f"  æ€»å…³ç³»æ•°: {stats['total_relationships']:,}")
        print(f"  èŠ‚ç‚¹ç±»å‹: {stats['node_types']}")
        print(f"  å…³ç³»ç±»å‹: {stats['relationship_types']}")

        # èŠ‚ç‚¹è¯¦æƒ…
        print("\n" + "=" * 80)
        print("ğŸ”µ èŠ‚ç‚¹ç±»å‹è¯¦æƒ…")
        print("=" * 80)

        for label, info in sorted(self.schema['nodes'].items()):
            print(f"\nã€{label}ã€‘")
            print(f"  æ•°é‡: {info['count']:,}")
            print(f"  å±æ€§: {len(info['properties'])}")

            if info['properties']:
                print("\n  å±æ€§åˆ—è¡¨:")
                for prop_name, prop_info in sorted(info['properties'].items()):
                    print(f"    â€¢ {prop_name}")
                    print(f"      - ç±»å‹: {prop_info['type']}")
                    print(f"      - è¦†ç›–ç‡: {prop_info['coverage']}")
                    if prop_info['sample_value'] is not None:
                        sample_str = str(prop_info['sample_value'])
                        if len(sample_str) > 50:
                            sample_str = sample_str[:50] + "..."
                        print(f"      - æ ·ä¾‹: {sample_str}")

        # å…³ç³»è¯¦æƒ…
        print("\n" + "=" * 80)
        print("ğŸ”— å…³ç³»ç±»å‹è¯¦æƒ…")
        print("=" * 80)

        for rel_type, info in sorted(self.schema['relationships'].items()):
            print(f"\nã€{rel_type}ã€‘")
            print(f"  æ•°é‡: {info['count']:,}")

            if info['patterns']:
                print("\n  å…³ç³»æ¨¡å¼:")
                for source, target, count in info['patterns']:
                    percentage = (count / info['count'] * 100) if info['count'] > 0 else 0
                    print(f"    â€¢ ({source})-[{rel_type}]->({target}): {count:,} ({percentage:.1f}%)")

            if info['properties']:
                print("\n  å…³ç³»å±æ€§:")
                for prop_name, prop_info in sorted(info['properties'].items()):
                    print(f"    â€¢ {prop_name}")
                    print(f"      - ç±»å‹: {prop_info['type']}")
                    print(f"      - è¦†ç›–ç‡: {prop_info['coverage']}")
                    if prop_info['sample_value'] is not None:
                        sample_str = str(prop_info['sample_value'])
                        if len(sample_str) > 50:
                            sample_str = sample_str[:50] + "..."
                        print(f"      - æ ·ä¾‹: {sample_str}")

        print("\n" + "=" * 80)

    def export_to_json(self, output_file: str = "schema.json"):
        """å¯¼å‡ºschemaåˆ°JSONæ–‡ä»¶"""
        logger.info(f"å¯¼å‡ºSchemaåˆ° {output_file}...")

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.schema, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"âœ“ Schemaå·²å¯¼å‡ºåˆ° {output_file}")

    def export_to_markdown(self, output_file: str = "neuroxiv_schema.md"):
        """å¯¼å‡ºschemaåˆ°Markdownæ–‡ä»¶"""
        logger.info(f"å¯¼å‡ºSchemaåˆ° {output_file}...")

        lines = []
        lines.append("# NeuroXiv 2.0 çŸ¥è¯†å›¾è°± Schema\n")

        # ç»Ÿè®¡æ¦‚è§ˆ
        stats = self.schema['statistics']
        lines.append("## ç»Ÿè®¡æ¦‚è§ˆ\n")
        lines.append(f"- **æ€»èŠ‚ç‚¹æ•°**: {stats['total_nodes']:,}")
        lines.append(f"- **æ€»å…³ç³»æ•°**: {stats['total_relationships']:,}")
        lines.append(f"- **èŠ‚ç‚¹ç±»å‹æ•°**: {stats['node_types']}")
        lines.append(f"- **å…³ç³»ç±»å‹æ•°**: {stats['relationship_types']}\n")

        # èŠ‚ç‚¹ç±»å‹
        lines.append("## èŠ‚ç‚¹ç±»å‹\n")
        for label, info in sorted(self.schema['nodes'].items()):
            lines.append(f"### {label}\n")
            lines.append(f"**æ•°é‡**: {info['count']:,}\n")

            if info['properties']:
                lines.append("**å±æ€§**:\n")
                lines.append("| å±æ€§å | ç±»å‹ | è¦†ç›–ç‡ | æ ·ä¾‹å€¼ |")
                lines.append("|--------|------|--------|--------|")

                for prop_name, prop_info in sorted(info['properties'].items()):
                    sample = str(prop_info['sample_value']) if prop_info['sample_value'] is not None else "N/A"
                    if len(sample) > 30:
                        sample = sample[:30] + "..."
                    lines.append(f"| `{prop_name}` | {prop_info['type']} | {prop_info['coverage']} | {sample} |")
                lines.append("")

        # å…³ç³»ç±»å‹
        lines.append("## å…³ç³»ç±»å‹\n")
        for rel_type, info in sorted(self.schema['relationships'].items()):
            lines.append(f"### {rel_type}\n")
            lines.append(f"**æ•°é‡**: {info['count']:,}\n")

            if info['patterns']:
                lines.append("**å…³ç³»æ¨¡å¼**:\n")
                for source, target, count in info['patterns']:
                    percentage = (count / info['count'] * 100) if info['count'] > 0 else 0
                    lines.append(f"- `({source})-[{rel_type}]->({target})`: {count:,} ({percentage:.1f}%)")
                lines.append("")

            if info['properties']:
                lines.append("**å…³ç³»å±æ€§**:\n")
                lines.append("| å±æ€§å | ç±»å‹ | è¦†ç›–ç‡ | æ ·ä¾‹å€¼ |")
                lines.append("|--------|------|--------|--------|")

                for prop_name, prop_info in sorted(info['properties'].items()):
                    sample = str(prop_info['sample_value']) if prop_info['sample_value'] is not None else "N/A"
                    if len(sample) > 30:
                        sample = sample[:30] + "..."
                    lines.append(f"| `{prop_name}` | {prop_info['type']} | {prop_info['coverage']} | {sample} |")
                lines.append("")

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        logger.info(f"âœ“ Schemaå·²å¯¼å‡ºåˆ° {output_file}")

    def generate_cypher_visualization(self, output_file: str = "schema_visualization.cypher"):
        """ç”ŸæˆCypheræŸ¥è¯¢ç”¨äºå¯è§†åŒ–schema"""
        logger.info(f"ç”Ÿæˆå¯è§†åŒ–æŸ¥è¯¢åˆ° {output_file}...")

        lines = []
        lines.append("// NeuroXiv 2.0 Schema å¯è§†åŒ–æŸ¥è¯¢")
        lines.append("// åœ¨Neo4j Browserä¸­è¿è¡Œä»¥ä¸‹æŸ¥è¯¢\n")

        # 1. æ˜¾ç¤ºæ‰€æœ‰èŠ‚ç‚¹ç±»å‹æ ·ä¾‹
        lines.append("// 1. æ˜¾ç¤ºæ¯ç§èŠ‚ç‚¹ç±»å‹çš„æ ·ä¾‹")
        for label in self.schema['nodes'].keys():
            lines.append(f"MATCH (n:{label}) RETURN n LIMIT 5;")
        lines.append("")

        # 2. æ˜¾ç¤ºå…³ç³»æ¨¡å¼
        lines.append("// 2. æ˜¾ç¤ºæ‰€æœ‰å…³ç³»æ¨¡å¼")
        for rel_type, info in self.schema['relationships'].items():
            if info['patterns']:
                source, target, _ = info['patterns'][0]
                lines.append(f"MATCH (a:{source})-[r:{rel_type}]->(b:{target}) RETURN a, r, b LIMIT 10;")
        lines.append("")

        # 3. ç»Ÿè®¡æŸ¥è¯¢
        lines.append("// 3. ç»Ÿè®¡æŸ¥è¯¢")
        lines.append("CALL db.labels() YIELD label")
        lines.append("CALL apoc.cypher.run('MATCH (:`'+label+'`) RETURN count(*) as count', {}) YIELD value")
        lines.append("RETURN label, value.count ORDER BY value.count DESC;")

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        logger.info(f"âœ“ å¯è§†åŒ–æŸ¥è¯¢å·²ç”Ÿæˆåˆ° {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='NeuroXiv 2.0 çŸ¥è¯†å›¾è°± Schema æå–å™¨')
    parser.add_argument('--uri', type=str, default='bolt://localhost:7687',
                        help='Neo4j URI')
    parser.add_argument('--user', type=str, default='neo4j',
                        help='Neo4jç”¨æˆ·å')
    parser.add_argument('--password', type=str, default='neuroxiv',required=True,
                        help='Neo4jå¯†ç ')
    parser.add_argument('--database', type=str, default='neo4j',
                        help='æ•°æ®åº“åç§°')
    parser.add_argument('--output-dir', type=str, default='./schema_output',
                        help='è¾“å‡ºç›®å½•')

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)

    # åˆå§‹åŒ–æå–å™¨
    extractor = Neo4jSchemaExtractor(
        uri=args.uri,
        user=args.user,
        password=args.password,
        database=args.database
    )

    try:
        # è¿æ¥æ•°æ®åº“
        if not extractor.connect():
            return

        # æå–schema
        extractor.extract_full_schema()

        # æ‰“å°æŠ¥å‘Š
        extractor.print_schema_report()

        # å¯¼å‡ºæ–‡ä»¶
        extractor.export_to_json(output_dir / "schema.json")
        extractor.export_to_markdown(output_dir / "neuroxiv_schema.md")
        extractor.generate_cypher_visualization(output_dir / "schema_visualization.cypher")

        logger.info("\n" + "=" * 80)
        logger.info("âœ“ æ‰€æœ‰æ–‡ä»¶å·²ç”Ÿæˆåœ¨ç›®å½•: " + str(output_dir))
        logger.info("=" * 80)

    finally:
        extractor.close()


if __name__ == "__main__":
    main()