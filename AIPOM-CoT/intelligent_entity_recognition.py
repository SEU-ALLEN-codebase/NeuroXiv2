import re
import time
from typing import List, Dict, Set, Tuple, Optional
from collections import defaultdict
from dataclasses import dataclass, field
import logging

from neo4j_exec import Neo4jExec
from aipom_cot_true_agent_v2 import RealSchemaCache

logger = logging.getLogger(__name__)


# ==================== Entity Data Structures ====================

@dataclass
class EntityMatch:
    """å®ä½“åŒ¹é…ç»“æœ"""
    text: str
    entity_type: str
    entity_id: str
    confidence: float
    match_type: str
    span: Tuple[int, int] = (0, 0)
    metadata: Dict = field(default_factory=dict)

    def __post_init__(self):
        """éªŒè¯å­—æ®µ"""
        if not 0.0 <= self.confidence <= 1.0:
            raise ValueError(f"Confidence must be between 0.0 and 1.0, got {self.confidence}")


@dataclass
class EntityCluster:
    """ç›¸å…³å®ä½“çš„èšåˆ"""
    primary_entity: EntityMatch
    related_entities: List[EntityMatch]
    cluster_type: str
    relevance_score: float


# ==================== Fixed Entity Recognizer ====================

class IntelligentEntityRecognizer:
    """
    æ™ºèƒ½å®ä½“è¯†åˆ«å™¨ï¼ˆä¿®å¤ç‰ˆï¼‰

    ğŸ”§ å…³é”®ä¿®å¤ï¼š
    1. è¶…ä¸¥æ ¼çš„åœç”¨è¯è¿‡æ»¤
    2. KGéªŒè¯å±‚ï¼ˆåªè¿”å›KGä¸­å­˜åœ¨çš„å®ä½“ï¼‰
    3. ä¸ä»ç­”æ¡ˆè‡ªåŠ¨æå–
    4. æ”¹è¿›çš„æ¨¡ç³ŠåŒ¹é…
    """

    def __init__(self, db: Neo4jExec, schema: RealSchemaCache):
        self.db = db
        self.schema = schema

        # ğŸ”§ è¶…ä¸¥æ ¼åœç”¨è¯é»‘åå•
        self.STOPWORDS = self._build_comprehensive_stopwords()

        # ç¼“å­˜
        self._entity_cache = {}
        self._last_cache_time = time.time()
        self._cache_ttl = 3600

    def _build_comprehensive_stopwords(self) -> Set[str]:
        """æ„å»ºè¶…å…¨é¢çš„åœç”¨è¯è¡¨"""
        stopwords = set()

        # ç–‘é—®è¯
        stopwords.update(['what', 'which', 'where', 'when', 'who', 'why', 'how'])

        # beåŠ¨è¯
        stopwords.update(['are', 'is', 'was', 'were', 'be', 'been', 'being', 'am'])

        # åŠ©åŠ¨è¯
        stopwords.update([
            'do', 'does', 'did', 'done', 'doing',
            'have', 'has', 'had', 'having',
            'can', 'could', 'will', 'would', 'shall', 'should',
            'may', 'might', 'must'
        ])

        # ä»‹è¯
        stopwords.update([
            'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',
            'into', 'onto', 'upon', 'off', 'out', 'over', 'under',
            'about', 'between', 'within', 'across', 'through'
        ])

        # è¿è¯
        stopwords.update(['and', 'or', 'but', 'so', 'yet', 'nor'])

        # å† è¯
        stopwords.update(['the', 'an', 'a'])

        # ä»£è¯
        stopwords.update([
            'it', 'its', 'they', 'their', 'them', 'this', 'that', 'these', 'those',
            'he', 'she', 'his', 'her', 'him', 'me', 'my', 'we', 'our', 'us',
            'you', 'your', 'i'
        ])

        # Context-sensitive exclusions: words that ARE valid KG entities but
        # should NOT be matched in common English phrases
        # These are 2-letter region acronyms that conflict with common words
        stopwords.update([
            'me',  # ME (Medial Entorhinal) conflicts with "tell me"
            'or',  # OR conflicts with conjunction "or"
            'an',  # AN conflicts with article "an"
            'as',  # AS conflicts with "as"
            'if',  # IF conflicts with "if"
            'so',  # SO conflicts with "so"
            'no',  # NO conflicts with "no"
        ])

        # å¸¸è§åŠ¨è¯
        stopwords.update([
            'get', 'got', 'give', 'gave', 'given', 'show', 'tell', 'told',
            'make', 'made', 'take', 'took', 'taken', 'come', 'came',
            'find', 'found', 'see', 'saw', 'seen'
        ])

        # å¸¸è§å½¢å®¹è¯/å‰¯è¯
        stopwords.update([
            'not', 'all', 'some', 'any', 'each', 'every', 'both', 'few', 'more',
            'most', 'other', 'such', 'no', 'nor', 'only', 'own', 'same', 'than',
            'too', 'very', 'just', 'now', 'then', 'also', 'here', 'there',
            'well', 'even', 'still', 'already', 'yet'
        ])

        # ç¥ç»ç§‘å­¦é€šç”¨è¯ï¼ˆä¸æ˜¯å®ä½“ï¼‰
        stopwords.update([
            'cells', 'neurons', 'brain', 'regions', 'region', 'area', 'areas',
            'types', 'type', 'kind', 'kinds', 'group', 'groups',
            'part', 'parts', 'system', 'systems'
        ])

        return stopwords

    def recognize_entities(self, question: str) -> List[EntityMatch]:
        """
        æ™ºèƒ½å®ä½“è¯†åˆ«ï¼ˆä¿®å¤ç‰ˆï¼‰

        ğŸ”§ ä¿®å¤ç­–ç•¥ï¼š
        1. ç²¾ç¡®åŒ¹é… + KGéªŒè¯
        2. æ¨¡ç³ŠåŒ¹é… + KGéªŒè¯
        3. æ­£åˆ™Fallback + KGéªŒè¯
        4. è¶…ä¸¥æ ¼åœç”¨è¯è¿‡æ»¤
        """
        logger.info(f"ğŸ” Recognizing entities in: {question}")

        matches = []

        # Step 1: ç²¾ç¡®åŒ¹é…
        exact_matches = self._exact_match_with_validation(question)
        matches.extend(exact_matches)

        if exact_matches:
            logger.info(f"   âœ“ Exact match: {len(exact_matches)} entities")

        # Step 2: æ¨¡ç³ŠåŒ¹é…ï¼ˆå¦‚æœç²¾ç¡®åŒ¹é…å¤ªå°‘ï¼‰
        if len(matches) < 2:
            fuzzy_matches = self._fuzzy_match_with_validation(question)

            existing_texts = set([m.text.lower() for m in matches])
            for fm in fuzzy_matches:
                if fm.text.lower() not in existing_texts:
                    matches.append(fm)

            if fuzzy_matches:
                logger.info(f"   âœ“ Fuzzy match: {len(fuzzy_matches)} entities")

        # Step 3: æ­£åˆ™Fallbackï¼ˆå¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼‰
        if not matches:
            logger.warning("   âš ï¸ Using regex fallback...")
            regex_matches = self._regex_fallback_with_validation(question)
            matches.extend(regex_matches)

            if regex_matches:
                logger.info(f"   âœ“ Regex fallback: {len(regex_matches)} entities")
            else:
                logger.warning(f"   âš ï¸ No entities found")

        # å»é‡å’Œæ’åº
        matches = self._deduplicate_and_rank(matches)

        if matches:
            logger.info(f"   ğŸ“Š Final: {len(matches)} entities")
            for m in matches[:5]:
                logger.info(f"      â€¢ {m.text} ({m.entity_type}) [{m.confidence:.2f}]")

        return matches

    def _exact_match_with_validation(self, question: str) -> List[EntityMatch]:
        """
        ç²¾ç¡®åŒ¹é… + KGéªŒè¯ï¼ˆä¿®å¤ç‰ˆï¼‰

        ğŸ”§ å…³é”®ä¿®å¤ï¼š
        1. âœ… å…ˆKGéªŒè¯ï¼Œå†å†³å®šæ˜¯å¦è¿‡æ»¤
        2. âœ… KGä¸­å­˜åœ¨çš„å®ä½“ï¼Œå³ä½¿æ˜¯åœç”¨è¯ä¹Ÿä¿ç•™
        3. âœ… KGä¸­ä¸å­˜åœ¨çš„ï¼Œè‡ªåŠ¨è¿‡æ»¤
        """
        matches = []

        entity_types = ['Region', 'GeneMarker']

        for entity_type in entity_types:
            entities = self._get_entities_of_type(entity_type)

            for entity in entities:
                names_to_check = []

                if 'acronym' in entity:
                    names_to_check.append(entity['acronym'])
                if 'name' in entity:
                    names_to_check.append(entity['name'])
                if 'gene' in entity:
                    names_to_check.append(entity['gene'])

                for name in names_to_check:
                    if not name or len(name) < 2:
                        continue

                    # ç²¾ç¡®åŒ¹é…
                    pattern = r'\b' + re.escape(name) + r'\b'

                    for match in re.finditer(pattern, question, re.IGNORECASE):
                        matched_text = match.group()

                        # ğŸ”§ Critical fix: Filter short stopwords even if they exist in KG
                        # This prevents "me" in "tell me" from matching region "ME"
                        if matched_text.lower() in self.STOPWORDS and len(matched_text) <= 3:
                            continue

                        # ğŸ”§ å…³é”®ï¼šå…ˆKGéªŒè¯
                        validation = self._validate_entity_in_kg(entity_type, name)

                        if validation['exists']:
                            # âœ… KGä¸­å­˜åœ¨ â†’ ä¿ç•™
                            matches.append(EntityMatch(
                                text=matched_text,
                                entity_type=entity_type,
                                entity_id=validation.get('id', name),
                                confidence=1.0,
                                match_type='exact',
                                span=(match.start(), match.end()),
                                metadata=validation.get('data', {})
                            ))
                        # else: KGä¸­ä¸å­˜åœ¨ â†’ è‡ªåŠ¨è¿‡æ»¤

        return matches

    def _fuzzy_match_with_validation(self, question: str) -> List[EntityMatch]:
        """
        æ¨¡ç³ŠåŒ¹é… + KGéªŒè¯ï¼ˆä¿®å¤ç‰ˆï¼‰

        ğŸ”§ å…³é”®ä¿®å¤ï¼š
        1. âœ… ä¿ç•™åœç”¨è¯è¿‡æ»¤ï¼ˆé˜²æ­¢æ¨¡ç³ŠåŒ¹é…å¤ªå¤šå™ªéŸ³ï¼‰
        2. âœ… ä½†åœ¨KGéªŒè¯é€šè¿‡åå†å†³å®š
        """
        matches = []

        words = re.findall(r'\b[A-Za-z]{2,8}\b', question)

        entity_types = ['Region']

        for entity_type in entity_types:
            entities = self._get_entities_of_type(entity_type)

            for word in words:
                word_lower = word.lower()

                # âœ… æ¨¡ç³ŠåŒ¹é…ä»ç„¶éœ€è¦åœç”¨è¯è¿‡æ»¤
                # åŸå› ï¼šé¿å… "are" æ¨¡ç³ŠåŒ¹é…åˆ° "area"
                if word_lower in self.STOPWORDS:
                    continue

                if len(word) < 3:
                    continue

                for entity in entities:
                    names_to_check = []

                    if 'acronym' in entity:
                        names_to_check.append(entity['acronym'])

                    for name in names_to_check:
                        if not name:
                            continue

                        name_lower = name.lower()

                        if word_lower == name_lower:
                            continue  # å·²åœ¨exact matchå¤„ç†

                        # ğŸ”§ Fix: Don't match if region name is a strict substring of the query word
                        # This prevents matching "IP" when user asks about "HIP"
                        if name_lower in word_lower and name_lower != word_lower:
                            # The region name is a substring of the query word
                            # Skip this match - the user likely meant the full word
                            continue

                        # éƒ¨åˆ†åŒ¹é… (only allow query word as substring of region name)
                        if word_lower in name_lower:
                            confidence = 0.8
                        else:
                            similarity = self._string_similarity(word_lower, name_lower)
                            if similarity < 0.7:
                                continue
                            confidence = similarity

                        # ğŸ”§ KGéªŒè¯
                        validation = self._validate_entity_in_kg(entity_type, name)

                        if validation['exists']:
                            span_match = re.search(r'\b' + re.escape(word) + r'\b', question, re.IGNORECASE)
                            if span_match:
                                matches.append(EntityMatch(
                                    text=span_match.group(),
                                    entity_type=entity_type,
                                    entity_id=validation.get('id', name),
                                    confidence=confidence,
                                    match_type='fuzzy',
                                    span=(span_match.start(), span_match.end()),
                                    metadata=validation.get('data', {})
                                ))

        return matches

    def _regex_fallback_with_validation(self, question: str) -> List[EntityMatch]:
        """
        æ­£åˆ™Fallback + KGéªŒè¯ï¼ˆä¿®å¤ç‰ˆï¼‰

        ğŸ”§ å…³é”®ä¿®å¤ï¼š
        1. âœ… ä¿ç•™åœç”¨è¯è¿‡æ»¤ï¼ˆé˜²æ­¢WHAT/WHEREç­‰è¯¯æŠ¥ï¼‰
        2. âœ… ä½†KGéªŒè¯æ˜¯æœ€ç»ˆå†³ç­–
        """
        matches = []

        # Pattern 1: è„‘åŒºç¼©å†™
        region_pattern = r'\b[A-Z]{2,5}\b'

        for match in re.finditer(region_pattern, question):
            text = match.group()

            # âœ… Regex fallbackä¿ç•™åœç”¨è¯è¿‡æ»¤
            if text.lower() in self.STOPWORDS:
                continue

            # ğŸ”§ KGéªŒè¯
            validation = self._validate_entity_in_kg('Region', text)

            if validation['exists']:
                matches.append(EntityMatch(
                    text=text,
                    entity_type='Region',
                    entity_id=validation.get('id', text),
                    confidence=0.6,
                    match_type='regex_fallback',
                    span=(match.start(), match.end()),
                    metadata=validation.get('data', {})
                ))
                logger.info(f"      Regex validated: {text}")

        # Pattern 2: åŸºå› å
        gene_pattern = r'\b[A-Z][a-z]{2,8}\d*\+?\b'

        for match in re.finditer(gene_pattern, question):
            text = match.group()
            gene_name = text.rstrip('+')

            # âœ… åŸºå› ä¹Ÿè¿‡æ»¤å¸¸è§å•è¯
            gene_stopwords = [
                'what', 'which', 'where', 'when', 'cells', 'neurons',
                'brain', 'regions', 'does', 'have', 'show', 'tell',
                'about', 'between', 'compare', 'difference'
            ]
            if gene_name.lower() in gene_stopwords:
                continue

            # ğŸ”§ KGéªŒè¯
            validation = self._validate_entity_in_kg('GeneMarker', gene_name)

            if validation['exists']:
                matches.append(EntityMatch(
                    text=text,
                    entity_type='GeneMarker',
                    entity_id=validation.get('id', gene_name),
                    confidence=0.5,
                    match_type='regex_fallback',
                    span=(match.start(), match.end()),
                    metadata=validation.get('data', {})
                ))
                logger.info(f"      Regex validated: {text}")

        return matches

    def _validate_entity_in_kg(self, entity_type: str, entity_name: str) -> Dict:
        """åœ¨KGä¸­éªŒè¯å®ä½“æ˜¯å¦å­˜åœ¨"""

        if entity_type == 'Region':
            query = """
            MATCH (r:Region)
            WHERE r.acronym = $name OR r.name = $name
            RETURN r.acronym AS id, r.name AS name, r AS data
            LIMIT 1
            """
        elif entity_type == 'GeneMarker':
            query = """
            MATCH (c:Cluster)
            WHERE c.markers CONTAINS $name
            RETURN $name AS id, c AS data
            LIMIT 1
            """
        else:
            return {'exists': False}

        result = self.db.run(query, {'name': entity_name})

        if result['success'] and result['data']:
            row = result['data'][0]
            return {
                'exists': True,
                'id': row.get('id', entity_name),
                'data': row.get('data', {})
            }
        else:
            return {'exists': False}

    def _get_entities_of_type(self, entity_type: str) -> List[Dict]:
        """è·å–æŒ‡å®šç±»å‹çš„æ‰€æœ‰å®ä½“"""
        cache_key = f"entities_{entity_type}"

        if cache_key in self._entity_cache:
            cache_time = self._entity_cache[cache_key].get('time', 0)
            if time.time() - cache_time < self._cache_ttl:
                return self._entity_cache[cache_key]['data']

        if entity_type == 'Region':
            query = """
            MATCH (r:Region)
            RETURN r.acronym AS acronym, r.name AS name
            LIMIT 500
            """
        elif entity_type == 'GeneMarker':
            query = """
            MATCH (c:Cluster)
            WHERE c.markers IS NOT NULL
            WITH split(c.markers, ',') AS marker_list
            UNWIND marker_list AS marker
            RETURN DISTINCT trim(marker) AS gene
            LIMIT 1000
            """
        else:
            return []

        result = self.db.run(query)

        if result['success'] and result['data']:
            entities = result['data']

            self._entity_cache[cache_key] = {
                'data': entities,
                'time': time.time()
            }

            return entities
        else:
            return []

    def _string_similarity(self, s1: str, s2: str) -> float:
        """è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦"""
        if s1 == s2:
            return 1.0

        if len(s1) == 0 or len(s2) == 0:
            return 0.0

        # Longest common substring ratio
        max_len = max(len(s1), len(s2))

        lcs_len = 0
        for i in range(len(s1)):
            for j in range(len(s2)):
                k = 0
                while (i + k < len(s1) and
                       j + k < len(s2) and
                       s1[i + k] == s2[j + k]):
                    k += 1
                lcs_len = max(lcs_len, k)

        return lcs_len / max_len

    def _deduplicate_and_rank(self, matches: List[EntityMatch]) -> List[EntityMatch]:
        """å»é‡å’Œæ’åº"""
        seen = {}
        for match in matches:
            key = (match.text.lower(), match.entity_type)

            if key not in seen:
                seen[key] = match
            else:
                if match.confidence > seen[key].confidence:
                    seen[key] = match

        unique_matches = list(seen.values())
        unique_matches.sort(key=lambda m: m.confidence, reverse=True)

        return unique_matches


# ==================== Entity Clustering ====================

class EntityClusteringEngine:
    """å®ä½“èšç±»å¼•æ“"""

    def __init__(self, db: Neo4jExec, schema: RealSchemaCache):
        self.db = db
        self.schema = schema

    def cluster_entities(self,
                         matches: List[EntityMatch],
                         question: str) -> List[EntityCluster]:
        """èšç±»å®ä½“"""
        clusters = []

        # æŒ‰ç±»å‹åˆ†ç»„
        genes = [m for m in matches if m.entity_type == 'GeneMarker']
        regions = [m for m in matches if m.entity_type == 'Region']

        # åˆ›å»ºclusters
        if genes:
            cluster = self._create_gene_cluster(genes, regions, question)
            if cluster:
                clusters.append(cluster)

        if regions and not genes:
            cluster = self._create_region_cluster(regions, question)
            if cluster:
                clusters.append(cluster)

        clusters.sort(key=lambda c: c.relevance_score, reverse=True)

        return clusters

    def _create_gene_cluster(self,
                             genes: List[EntityMatch],
                             regions: List[EntityMatch],
                             question: str) -> Optional[EntityCluster]:
        """åˆ›å»ºåŸºå› cluster"""
        if not genes:
            return None

        primary_gene = genes[0]

        related = list(regions)

        relevance = 0.9
        question_lower = question.lower()
        if any(kw in question_lower for kw in ['gene', 'marker', 'express']):
            relevance *= 1.2

        return EntityCluster(
            primary_entity=primary_gene,
            related_entities=related,
            cluster_type='gene_marker',
            relevance_score=min(1.0, relevance)
        )

    def _create_region_cluster(self,
                               regions: List[EntityMatch],
                               question: str) -> Optional[EntityCluster]:
        """åˆ›å»ºregion cluster"""
        if not regions:
            return None

        primary_region = regions[0]

        relevance = 0.85
        question_lower = question.lower()
        if any(kw in question_lower for kw in ['region', 'area', 'brain']):
            relevance *= 1.2

        return EntityCluster(
            primary_entity=primary_region,
            related_entities=regions[1:],
            cluster_type='region',
            relevance_score=min(1.0, relevance)
        )