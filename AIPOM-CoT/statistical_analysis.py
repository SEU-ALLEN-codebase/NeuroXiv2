import numpy as np
import pandas as pd
import json
import logging
from pathlib import Path
from scipy import stats
from typing import Dict, List, Tuple, Optional
from collections import defaultdict

logger = logging.getLogger(__name__)


# ==================== Statistical Analyzer ====================

class StatisticalAnalyzer:
    """
    ç»Ÿè®¡åˆ†æå™¨ï¼ˆv3.0 - å…¬å¹³ä¸”ä¸¥è°¨ï¼‰

    ğŸ”§ å…³é”®ä¿®å¤ï¼š
    - æ­£ç¡®å¤„ç†Noneå€¼
    - é€‰æ‹©åˆé€‚çš„ç»Ÿè®¡æ£€éªŒ
    - FDRæ ¡æ­£
    - åˆ†å±‚æ¯”è¾ƒ
    """

    def __init__(self, results_dir: str = "./benchmark_results"):
        self.results_dir = Path(results_dir)
        self.results_file = self.results_dir / "detailed_results.json"

        if not self.results_file.exists():
            raise FileNotFoundError(f"Results file not found: {self.results_file}")

        with open(self.results_file, 'r') as f:
            self.raw_results = json.load(f)

        logger.info(f"âœ… Loaded results from {self.results_file}")

        # ğŸ”§ åŠ è½½è¯„ä¼°é…ç½®
        from evaluators import EVALUATION_CONFIG
        self.eval_config = EVALUATION_CONFIG

    def run_full_analysis(self) -> pd.DataFrame:
        """è¿è¡Œå®Œæ•´ç»Ÿè®¡åˆ†æ"""

        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š STATISTICAL ANALYSIS (v3.0 - Fair & Rigorous)")
        logger.info("="*80)

        # æå–æŒ‡æ ‡æ•°æ®
        metrics_data = self._extract_metrics_with_none_handling()

        # è¿è¡Œå¯¹æ¯”æ£€éªŒ
        comparisons = []

        aipom_scores = metrics_data.get('AIPOM-CoT', {})

        baseline_methods = ['Direct GPT-4o', 'Template-KG', 'RAG', 'ReAct']

        for method in baseline_methods:
            if method not in metrics_data:
                logger.warning(f"  Method '{method}' not found in results")
                continue

            method_scores = metrics_data[method]

            # ğŸ”§ åˆ†å±‚æ¯”è¾ƒï¼šæ ¸å¿ƒæŒ‡æ ‡ vs ç³»ç»ŸæŒ‡æ ‡
            comparable_metrics = self._get_comparable_metrics(
                aipom_scores,
                method_scores,
                method
            )

            logger.info(f"\nğŸ”¬ Comparing AIPOM-CoT vs {method}:")
            logger.info(f"  Comparable metrics: {comparable_metrics}")

            for metric_name in comparable_metrics:
                comparison = self.compare_methods_robust(
                    aipom_scores[metric_name],
                    method_scores[metric_name],
                    'AIPOM-CoT',
                    method,
                    metric_name
                )
                comparisons.append(comparison)

        # è½¬ä¸ºDataFrame
        df = pd.DataFrame(comparisons)

        if len(df) == 0:
            logger.warning("âš ï¸  No valid comparisons generated")
            return df

        # ğŸ”§ å¤šé‡æ¯”è¾ƒæ ¡æ­£
        df = self._apply_fdr_correction(df)

        # ä¿å­˜
        output_file = self.results_dir / "statistical_analysis.csv"
        df.to_csv(output_file, index=False)

        logger.info(f"\nâœ… Statistical analysis saved to: {output_file}")

        # ç”ŸæˆLaTeXè¡¨æ ¼
        self._generate_latex_table(df)

        # æ‰“å°æ‘˜è¦
        self._print_summary(df)

        return df

    def _extract_metrics_with_none_handling(self) -> Dict[str, Dict[str, List]]:
        """
        ğŸ”§ æå–æŒ‡æ ‡ï¼Œæ­£ç¡®å¤„ç†Noneå€¼

        Returns:
            {
                'method_name': {
                    'metric_name': [score1, score2, ...],  # Noneä¿ç•™ä¸ºNone
                }
            }
        """

        metrics_data = defaultdict(lambda: defaultdict(list))

        for method, results_list in self.raw_results.items():
            for result in results_list:
                metrics = result.get('metrics', {})

                # æå–æ‰€æœ‰æŒ‡æ ‡
                metric_names = [
                    'entity_f1',
                    'factual_accuracy',
                    'answer_completeness',
                    'scientific_rigor',
                    'depth_matching_accuracy',
                    'plan_coherence',
                    'modality_coverage',
                    'closed_loop_achieved',
                ]

                for metric_name in metric_names:
                    value = getattr(metrics, metric_name, None)

                    # ğŸ”§ å¤„ç†closed_loop_achieved (bool â†’ float)
                    if metric_name == 'closed_loop_achieved' and value is not None:
                        value = 1.0 if value else 0.0

                    # ä¿ç•™Noneï¼ˆä¸è½¬ä¸º0ï¼‰
                    metrics_data[method][metric_name].append(value)

        return dict(metrics_data)

    def _get_comparable_metrics(self,
                                scores_a: Dict[str, List],
                                scores_b: Dict[str, List],
                                method_b_name: str) -> List[str]:
        """è·å–å¯æ¯”è¾ƒçš„æŒ‡æ ‡ï¼ˆv3.1ï¼‰"""

        # 1. æ ¸å¿ƒæŒ‡æ ‡ï¼šæ‰€æœ‰æ–¹æ³•éƒ½æ¯”è¾ƒ
        core_metrics = list(self.eval_config['core_metrics'].keys())
        comparable = set(core_metrics)

        # 2. ç³»ç»ŸæŒ‡æ ‡ï¼šæ£€æŸ¥æ–¹æ³•æ˜¯å¦é€‚ç”¨
        for metric_name, config in self.eval_config['system_metrics'].items():
            applicable_methods = config['methods']

            # ğŸ”§ reasoning_depthå¯¹æ‰€æœ‰æ–¹æ³•éƒ½é€‚ç”¨
            if metric_name == 'reasoning_depth' and applicable_methods == 'all':
                comparable.add(metric_name)

            # å…¶ä»–ç³»ç»ŸæŒ‡æ ‡æŒ‰åŸé€»è¾‘
            elif 'AIPOM-CoT' in applicable_methods and \
                    (method_b_name in applicable_methods or applicable_methods == 'all'):
                comparable.add(metric_name)

        # 3. æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
        valid_comparable = []
        for metric_name in comparable:
            actual_field = self._map_metric_name(metric_name)

            scores_a_list = scores_a.get(actual_field, [])
            scores_b_list = scores_b.get(actual_field, [])

            valid_a = [s for s in scores_a_list if s is not None]
            valid_b = [s for s in scores_b_list if s is not None]

            if len(valid_a) >= 3 and len(valid_b) >= 3:
                valid_comparable.append(actual_field)

        return valid_comparable

    def _map_metric_name(self, metric_name: str) -> str:
        """æ˜ å°„metricåç§°ï¼ˆv3.1ï¼‰"""
        mapping = {
            'entity_f1': 'entity_f1',
            'factual_accuracy': 'factual_accuracy',
            'answer_completeness': 'answer_completeness',
            'scientific_rigor': 'scientific_rigor',
            'reasoning_depth': 'reasoning_depth',  # ğŸ”§ æ”¹å
            'plan_coherence': 'plan_coherence',
            'modality_coverage': 'modality_coverage',
            'closed_loop': 'closed_loop_achieved',
        }
        return mapping.get(metric_name, metric_name)

    def compare_methods_robust(self,
                               scores_a: List[Optional[float]],
                               scores_b: List[Optional[float]],
                               method_a: str,
                               method_b: str,
                               metric_name: str) -> Dict:
        """
        ğŸ”§ ç¨³å¥çš„æ–¹æ³•å¯¹æ¯”

        å¤„ç†ï¼š
        - Noneå€¼
        - é…å¯¹vsç‹¬ç«‹æ£€éªŒ
        - å°æ ·æœ¬
        """

        # ğŸ”§ è¿‡æ»¤Noneå€¼ï¼Œåˆ›å»ºé…å¯¹æ•°æ®
        paired_scores = []
        for a, b in zip(scores_a, scores_b):
            if a is not None and b is not None:
                paired_scores.append((a, b))

        n_pairs = len(paired_scores)

        if n_pairs < 3:
            # æ ·æœ¬å¤ªå°‘
            return {
                'metric': metric_name,
                'method_a': method_a,
                'method_b': method_b,
                'n': n_pairs,
                'mean_a': np.nan,
                'mean_b': np.nan,
                'p_value': np.nan,
                'p_value_raw': np.nan,
                'significance': 'insufficient_data',
                'cohens_d': np.nan,
                'test_type': 'none',
            }

        scores_a_clean, scores_b_clean = zip(*paired_scores)
        scores_a_clean = np.array(scores_a_clean)
        scores_b_clean = np.array(scores_b_clean)

        # åŸºæœ¬ç»Ÿè®¡
        mean_a = np.mean(scores_a_clean)
        mean_b = np.mean(scores_b_clean)
        std_a = np.std(scores_a_clean, ddof=1)
        std_b = np.std(scores_b_clean, ddof=1)

        # ğŸ”§ é€‰æ‹©ç»Ÿè®¡æ£€éªŒ
        # å¦‚æœæ˜¯åŒæ ·çš„é—®é¢˜ï¼ˆé…å¯¹ï¼‰ï¼Œç”¨paired t-test
        # å¦åˆ™ç”¨independent t-test

        try:
            # å°è¯•paired t-testï¼ˆå‡è®¾é…å¯¹ï¼‰
            if len(scores_a_clean) == len(scores_b_clean):
                t_stat, p_value = stats.ttest_rel(scores_a_clean, scores_b_clean)
                test_type = 'paired_t'
            else:
                # Fallback: independent
                t_stat, p_value = stats.ttest_ind(scores_a_clean, scores_b_clean)
                test_type = 'independent_t'

        except Exception as e:
            logger.warning(f"  t-test failed for {metric_name}: {e}")
            # Fallback: Mann-Whitney U test (éå‚æ•°)
            try:
                u_stat, p_value = stats.mannwhitneyu(scores_a_clean, scores_b_clean, alternative='two-sided')
                test_type = 'mann_whitney'
                t_stat = u_stat
            except:
                return {
                    'metric': metric_name,
                    'method_a': method_a,
                    'method_b': method_b,
                    'n': n_pairs,
                    'mean_a': mean_a,
                    'mean_b': mean_b,
                    'p_value': np.nan,
                    'p_value_raw': np.nan,
                    'significance': 'test_failed',
                    'cohens_d': np.nan,
                    'test_type': 'failed',
                }

        # Cohen's d
        pooled_std = np.sqrt((std_a**2 + std_b**2) / 2)
        cohens_d = (mean_a - mean_b) / pooled_std if pooled_std > 0 else 0.0

        # 95% CI for difference
        diff = scores_a_clean - scores_b_clean
        se = np.std(diff, ddof=1) / np.sqrt(len(diff))
        ci_95_lower = np.mean(diff) - 1.96 * se
        ci_95_upper = np.mean(diff) + 1.96 * se

        # Significance (before FDR correction)
        if p_value < 0.001:
            significance = '***'
        elif p_value < 0.01:
            significance = '**'
        elif p_value < 0.05:
            significance = '*'
        else:
            significance = 'ns'

        # Effect size label
        abs_d = abs(cohens_d)
        if abs_d < 0.2:
            effect_size_label = 'negligible'
        elif abs_d < 0.5:
            effect_size_label = 'small'
        elif abs_d < 0.8:
            effect_size_label = 'medium'
        else:
            effect_size_label = 'large'

        return {
            'metric': metric_name,
            'method_a': method_a,
            'method_b': method_b,
            'n': n_pairs,
            'mean_a': mean_a,
            'std_a': std_a,
            'mean_b': mean_b,
            'std_b': std_b,
            'mean_diff': mean_a - mean_b,
            't_statistic': t_stat,
            'p_value': p_value,
            'p_value_raw': p_value,
            'significance': significance,
            'cohens_d': cohens_d,
            'effect_size': effect_size_label,
            'ci_95_lower': ci_95_lower,
            'ci_95_upper': ci_95_upper,
            'test_type': test_type,
        }

    def _apply_fdr_correction(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ğŸ”§ åº”ç”¨FDRå¤šé‡æ¯”è¾ƒæ ¡æ­£ï¼ˆBenjamini-Hochbergï¼‰
        """

        from statsmodels.stats.multitest import multipletests

        # æå–æ‰€æœ‰på€¼
        p_values = df['p_value_raw'].fillna(1.0).values

        if len(p_values) == 0:
            return df

        # FDRæ ¡æ­£
        try:
            reject, p_corrected, alpha_sidak, alpha_bonf = multipletests(
                p_values,
                alpha=0.05,
                method='fdr_bh'
            )

            # æ·»åŠ æ ¡æ­£åçš„åˆ—
            df['p_value_fdr'] = p_corrected
            df['significant_fdr'] = reject

            # æ›´æ–°significanceæ ‡è®°
            df['significance_fdr'] = df['p_value_fdr'].apply(
                lambda p: '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'
            )

            logger.info(f"\nğŸ”§ FDR Correction Applied:")
            logger.info(f"  Total comparisons: {len(df)}")
            logger.info(f"  Significant before FDR: {sum(df['p_value_raw'] < 0.05)}")
            logger.info(f"  Significant after FDR: {sum(reject)}")

        except Exception as e:
            logger.error(f"  FDR correction failed: {e}")
            df['p_value_fdr'] = df['p_value_raw']
            df['significant_fdr'] = df['p_value_raw'] < 0.05
            df['significance_fdr'] = df['significance']

        return df

    def _print_summary(self, df: pd.DataFrame):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""

        print("\n" + "="*80)
        print("ğŸ“Š KEY FINDINGS (with FDR correction)")
        print("="*80)

        # æŒ‰æ–¹æ³•åˆ†ç»„
        for method_b in df['method_b'].unique():
            method_df = df[df['method_b'] == method_b]

            print(f"\n{'AIPOM-CoT vs ' + method_b}")
            print("-" * 60)

            # æŒ‰æŒ‡æ ‡åˆ†ç±»
            core_metrics = ['entity_f1', 'factual_accuracy', 'answer_completeness', 'scientific_rigor']
            system_metrics = ['depth_matching_accuracy', 'plan_coherence', 'modality_coverage', 'closed_loop_achieved']

            print("  Core Metrics:")
            for _, row in method_df[method_df['metric'].isin(core_metrics)].iterrows():
                self._print_comparison_row(row, indent="    ")

            print("\n  System Metrics:")
            for _, row in method_df[method_df['metric'].isin(system_metrics)].iterrows():
                self._print_comparison_row(row, indent="    ")

        print("\n" + "="*80)
        print("Legend:")
        print("  Significance: *** p<0.001, ** p<0.01, * p<0.05, ns p>=0.05 (FDR-corrected)")
        print("  Effect size: Cohen's d (negligible:<0.2, small:<0.5, medium:<0.8, large:â‰¥0.8)")
        print("="*80)

    def _print_comparison_row(self, row: pd.Series, indent: str = ""):
        """æ‰“å°å•ä¸ªæ¯”è¾ƒç»“æœ"""

        metric = row['metric'].replace('_', ' ').title()
        mean_a = row['mean_a']
        mean_b = row['mean_b']
        diff = row['mean_diff']
        p_fdr = row['p_value_fdr']
        sig = row['significance_fdr']
        cohens_d = row['cohens_d']
        effect = row['effect_size']

        # è®¡ç®—æå‡ç™¾åˆ†æ¯”
        if mean_b > 0:
            improvement = (diff / mean_b) * 100
        else:
            improvement = 0

        print(f"{indent}{metric:25s}: {mean_a:.3f} vs {mean_b:.3f} "
              f"(Î”={diff:+.3f}, {improvement:+.1f}%)")
        print(f"{indent}{'':25s}  p_FDR={p_fdr:.4f}{sig}, d={cohens_d:.2f} ({effect})")

    def _generate_latex_table(self, df: pd.DataFrame):
        """ç”ŸæˆLaTeXè¡¨æ ¼"""

        output_file = self.results_dir / "statistical_analysis.tex"

        latex_lines = []
        latex_lines.append(r"\begin{table}[htbp]")
        latex_lines.append(r"\centering")
        latex_lines.append(r"\caption{Statistical Comparison of AIPOM-CoT vs Baselines (FDR-corrected)}")
        latex_lines.append(r"\label{tab:statistical_analysis}")
        latex_lines.append(r"\begin{tabular}{llccccc}")
        latex_lines.append(r"\toprule")
        latex_lines.append(r"Metric & Baseline & AIPOM-CoT & Baseline & $p_{FDR}$ & Cohen's $d$ & Effect Size \\")
        latex_lines.append(r"\midrule")

        # æŒ‰metricå’Œmethodåˆ†ç»„
        for metric in df['metric'].unique():
            metric_df = df[df['metric'] == metric]

            metric_name = metric.replace('_', ' ').title()

            for idx, row in metric_df.iterrows():
                method_b = row['method_b'].replace('GPT-4o', r'GPT-4\textit{o}')
                mean_a = row['mean_a']
                mean_b = row['mean_b']
                p_fdr = row['p_value_fdr']
                sig = row['significance_fdr'].replace('***', r'$^{***}$').replace('**', r'$^{**}$').replace('*', r'$^{*}$')
                cohens_d = row['cohens_d']
                effect = row['effect_size']

                if idx == metric_df.index[0]:
                    latex_lines.append(f"{metric_name} & {method_b} & {mean_a:.3f} & {mean_b:.3f} & {p_fdr:.4f}{sig} & {cohens_d:.2f} & {effect} \\\\")
                else:
                    latex_lines.append(f"& {method_b} & {mean_a:.3f} & {mean_b:.3f} & {p_fdr:.4f}{sig} & {cohens_d:.2f} & {effect} \\\\")

            latex_lines.append(r"\midrule")

        latex_lines.append(r"\bottomrule")
        latex_lines.append(r"\end{tabular}")
        latex_lines.append(r"\begin{tablenotes}")
        latex_lines.append(r"\small")
        latex_lines.append(r"\item $^{***}$p<0.001, $^{**}$p<0.01, $^{*}$p<0.05 (FDR-corrected)")
        latex_lines.append(r"\item Effect size: Cohen's d (small: 0.2, medium: 0.5, large: 0.8)")
        latex_lines.append(r"\end{tablenotes}")
        latex_lines.append(r"\end{table}")

        with open(output_file, 'w') as f:
            f.write('\n'.join(latex_lines))

        logger.info(f"  LaTeX table saved to: {output_file}")

    def generate_performance_summary_table(self) -> pd.DataFrame:
        """
        ç”Ÿæˆæ€§èƒ½æ‘˜è¦è¡¨ï¼ˆç”¨äºè®ºæ–‡ï¼‰

        Returns:
            DataFrame with mean Â± std for each method and metric
        """

        metrics_data = self._extract_metrics_with_none_handling()

        summary_rows = []

        metric_names = [
            'entity_f1',
            'factual_accuracy',
            'answer_completeness',
            'scientific_rigor',
            'reasoning_depth',
            'modality_coverage',
        ]

        for metric_name in metric_names:
            row = {'Metric': metric_name.replace('_', ' ').title()}

            for method in ['AIPOM-CoT', 'Direct GPT-4o', 'Template-KG', 'RAG', 'ReAct']:
                if method not in metrics_data:
                    row[method] = 'N/A'
                    continue

                scores = metrics_data[method].get(metric_name, [])
                valid_scores = [s for s in scores if s is not None]

                if len(valid_scores) == 0:
                    row[method] = 'N/A'
                else:
                    mean = np.mean(valid_scores)
                    std = np.std(valid_scores, ddof=1)
                    row[method] = f"{mean:.3f} Â± {std:.3f}"

            summary_rows.append(row)

        df = pd.DataFrame(summary_rows)

        # ä¿å­˜
        output_file = self.results_dir / "performance_summary.csv"
        df.to_csv(output_file, index=False)

        logger.info(f"\nğŸ“Š Performance summary saved to: {output_file}")

        return df

    def analyze_by_complexity(self) -> pd.DataFrame:
        """
        ğŸ”§ æŒ‰å¤æ‚åº¦ç­‰çº§åˆ†ææ€§èƒ½

        Returns:
            DataFrame with performance by complexity level
        """

        complexity_results = defaultdict(lambda: defaultdict(list))

        for method, results_list in self.raw_results.items():
            for result in results_list:
                # è·å–å¤æ‚åº¦ç­‰çº§
                question_data = result.get('question_data', {})
                tier = question_data.get('tier', 'unknown')

                # è·å–overall score
                metrics = result.get('metrics', {})
                overall = getattr(metrics, 'overall_score', None)

                if overall is not None:
                    complexity_results[method][tier].append(overall)

        # è½¬ä¸ºDataFrame
        summary_rows = []

        for method in ['AIPOM-CoT', 'Direct GPT-4o', 'Template-KG', 'RAG', 'ReAct']:
            if method not in complexity_results:
                continue

            row = {'Method': method}

            for tier in ['simple', 'medium', 'deep', 'screening']:
                scores = complexity_results[method].get(tier, [])

                if len(scores) == 0:
                    row[tier.capitalize()] = 'N/A'
                else:
                    mean = np.mean(scores)
                    std = np.std(scores, ddof=1)
                    row[tier.capitalize()] = f"{mean:.3f} Â± {std:.3f}"

            summary_rows.append(row)

        df = pd.DataFrame(summary_rows)

        # ä¿å­˜
        output_file = self.results_dir / "performance_by_complexity.csv"
        df.to_csv(output_file, index=False)

        logger.info(f"\nğŸ“Š Complexity analysis saved to: {output_file}")

        return df


# ==================== Convenience Functions ====================

def run_statistical_analysis(results_dir: str = "./benchmark_results") -> pd.DataFrame:
    """
    ä¾¿æ·å‡½æ•°ï¼šè¿è¡Œå®Œæ•´ç»Ÿè®¡åˆ†æ

    Usage:
        from statistical_analysis import run_statistical_analysis
        df = run_statistical_analysis("./benchmark_results")
    """

    analyzer = StatisticalAnalyzer(results_dir)

    # ä¸»è¦ç»Ÿè®¡åˆ†æ
    df = analyzer.run_full_analysis()

    # ç”Ÿæˆæ‘˜è¦è¡¨
    summary_df = analyzer.generate_performance_summary_table()

    # å¤æ‚åº¦åˆ†æ
    complexity_df = analyzer.analyze_by_complexity()

    return df


# ==================== Test ====================

if __name__ == "__main__":
    import sys

    print("\n" + "="*80)
    print("ğŸ“Š Statistical Analysis Tool v3.0 (Fair & Rigorous)")
    print("="*80)

    if len(sys.argv) > 1:
        results_dir = sys.argv[1]
    else:
        results_dir = "./benchmark_results"

    print(f"\nAnalyzing results from: {results_dir}")

    try:
        analyzer = StatisticalAnalyzer(results_dir)

        print("\nğŸ”¬ Running statistical analysis...")
        df = analyzer.run_full_analysis()

        print("\nğŸ“Š Generating performance summary...")
        summary_df = analyzer.generate_performance_summary_table()

        print("\nğŸ“ˆ Analyzing by complexity...")
        complexity_df = analyzer.analyze_by_complexity()

        print("\n" + "="*80)
        print("âœ… Analysis complete!")
        print("="*80)
        print("\nGenerated files:")
        print(f"  - {results_dir}/statistical_analysis.csv")
        print(f"  - {results_dir}/statistical_analysis.tex")
        print(f"  - {results_dir}/performance_summary.csv")
        print(f"  - {results_dir}/performance_by_complexity.csv")
        print("="*80)

    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)