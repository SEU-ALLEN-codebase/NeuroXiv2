import json
import logging
import os
import time
import argparse
import random
from typing import Any, Dict, List, Optional
from dataclasses import dataclass, field
import re
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from scipy import stats
from neo4j_exec import Neo4jExec

# Intent routing and provenance (CC_SPEC_MS additions)
from intent_router import IntentRouter, IntentType, get_budget_for_intent, get_smalltalk_response, BudgetLimits
from provenance import ProvenanceLogger, create_provenance_logger, EventType
from evidence_buffer import EvidenceBuffer
from adaptive_planner import AdaptivePlanner, AnalysisDepth, AnalysisState
from aipom_cot_true_agent_v2 import (
    RealSchemaCache,
    StatisticalTools,
    AgentPhase,
    AgentState,
    ReasoningStep
)

# å¯¼å…¥æ–°ç»„ä»¶
from intelligent_entity_recognition import (
    IntelligentEntityRecognizer,
    EntityClusteringEngine
)
from schema_path_planner import DynamicSchemaPathPlanner
from structured_reflection import StructuredReflector

try:
    from openai import OpenAI
except ImportError:
    raise ImportError("Please install openai: pip install openai")

logger = logging.getLogger(__name__)


# ==================== Enhanced Agent State ====================

@dataclass
class EnhancedAgentState(AgentState):
    """æ‰©å±•çš„AgentçŠ¶æ€"""

    # æ–°å¢å­—æ®µ
    entity_matches: List = field(default_factory=list)  # EntityMatchåˆ—è¡¨
    entity_clusters: List = field(default_factory=list)  # EntityClusteråˆ—è¡¨
    structured_reflections: List = field(default_factory=list)  # StructuredReflectionåˆ—è¡¨
    schema_paths_used: List = field(default_factory=list)  # ä½¿ç”¨çš„schemaè·¯å¾„

class RealFingerprintAnalyzer:
    """
    Multi-modal fingerprint analysis adapted to REAL schema

    Key changes from V8:
    - Molecular: Use Cluster nodes and HAS_CLUSTER relationships
    - Morphological: Aggregate from Neuron nodes via LOCATE_AT
    - Projection: Use PROJECT_TO (unchanged, but verify properties)
    """

    def __init__(self, db: Neo4jExec, schema: RealSchemaCache):
        self.db = db
        self.schema = schema
        self._cluster_cache = None
        self._target_cache = None

    def compute_region_fingerprint(self, region: str) -> Optional[Dict[str, np.ndarray]]:
        """
        Compute tri-modal fingerprint for a region

        Returns:
            {
                'molecular': np.ndarray,    # Cluster composition
                'morphological': np.ndarray, # Aggregated neuron features
                'projection': np.ndarray     # Target distribution
            }
        """
        fingerprint = {}

        # Molecular fingerprint
        mol_fp = self.compute_molecular_fingerprint(region)
        if mol_fp is not None:
            fingerprint['molecular'] = mol_fp

        # Morphological fingerprint
        mor_fp = self.compute_morphological_fingerprint(region)
        if mor_fp is not None:
            fingerprint['morphological'] = mor_fp

        # Projection fingerprint
        proj_fp = self.compute_projection_fingerprint(region)
        if proj_fp is not None:
            fingerprint['projection'] = proj_fp

        return fingerprint if len(fingerprint) > 0 else None

    def compute_molecular_fingerprint(self, region: str) -> Optional[np.ndarray]:
        """
        è®¡ç®—å•ä¸ªè„‘åŒºçš„åˆ†å­æŒ‡çº¹ (Figure 4æ–¹æ³•)

        ğŸ¯ åˆ†å­æŒ‡çº¹ = Subclassç»„æˆç™¾åˆ†æ¯”

        ä½¿ç”¨å…³ç³»: Region -[HAS_SUBCLASS]-> Subclass
        """
        query = """
        MATCH (r:Region {acronym: $acronym})-[hs:HAS_SUBCLASS]->(sc:Subclass)
        RETURN
          sc.name AS subclass_name,
          hs.pct_cells AS pct_cells
        ORDER BY sc.name
        """

        result = self.db.run(query, {'acronym': region})

        if not result['success'] or not result['data']:
            logger.warning(f"No molecular data for {region}")
            return None

        # æ„å»ºå­—å…¸
        data = {}
        for row in result['data']:
            subclass_name = row.get('subclass_name')
            pct_cells = row.get('pct_cells')
            if subclass_name and pct_cells is not None:
                data[subclass_name] = float(pct_cells)

        if not data:
            return None

        # è·å–å…¨å±€subclassåˆ—è¡¨
        all_subclasses = self._get_all_subclasses()  # è¿™ä¸ªæ–¹æ³•è¿”å›æ‰€æœ‰subclass names

        if not all_subclasses:
            logger.error("No global subclasses found")
            return None

        # æ„å»ºå›ºå®šç»´åº¦çš„å‘é‡
        signature = np.zeros(len(all_subclasses), dtype=float)
        for i, subclass in enumerate(all_subclasses):
            if subclass in data:
                signature[i] = data[subclass]

        # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯é›¶å‘é‡
        nonzero_count = np.count_nonzero(signature)
        total_pct = np.sum(signature)

        if nonzero_count == 0:
            logger.warning(f"{region}: molecular fingerprint is all zeros!")
            return None

        logger.debug(f"{region}: molecular FP - {nonzero_count}/{len(signature)} nonzero, sum={total_pct:.2f}")

        return signature

    def compute_morphological_fingerprint(self, region: str) -> Optional[np.ndarray]:
        """
        è®¡ç®—å•ä¸ªè„‘åŒºçš„å½¢æ€æŒ‡çº¹ (å¯¹é½Figure 4)

        ğŸ”§ å…³é”®æ”¹è¿›:
        1. ä»RegionèŠ‚ç‚¹çš„èšåˆå±æ€§è¯»å–ï¼ˆä¸æ˜¯å®æ—¶èšåˆï¼‰
        2. è¿”å›8ç»´å‘é‡ï¼ˆä¸æ˜¯6ç»´ï¼‰
        3. åŒ¹é…Figure 4çš„ç‰¹å¾é¡ºåº
        """
        query = """
        MATCH (r:Region {acronym: $acronym})
        RETURN
          r.axonal_bifurcation_remote_angle AS axonal_bifurcation_remote_angle,
          r.axonal_length AS axonal_length,
          r.axonal_branches AS axonal_branches,
          r.axonal_maximum_branch_order AS axonal_max_branch_order,
          r.dendritic_bifurcation_remote_angle AS dendritic_bifurcation_remote_angle,
          r.dendritic_length AS dendritic_length,
          r.dendritic_branches AS dendritic_branches,
          r.dendritic_maximum_branch_order AS dendritic_max_branch_order
        """

        result = self.db.run(query, {'acronym': region})

        if not result['success'] or not result['data'] or not result['data'][0]:
            return None

        record = result['data'][0]

        # æŒ‰ç…§å›ºå®šé¡ºåºæå–ç‰¹å¾å€¼
        features = [
            'axonal_bifurcation_remote_angle',
            'axonal_length',
            'axonal_branches',
            'axonal_max_branch_order',
            'dendritic_bifurcation_remote_angle',
            'dendritic_length',
            'dendritic_branches',
            'dendritic_max_branch_order'
        ]

        signature = np.array([
            record.get(feat) if record.get(feat) is not None else np.nan
            for feat in features
        ], dtype=float)

        return signature

    def compute_projection_fingerprint(self, region: str) -> Optional[np.ndarray]:
        """
        è®¡ç®—æŠ•å°„æŒ‡çº¹ (å¯¹é½Ground Truth - ä½¿ç”¨Neuron->Subregion)

        ğŸ”§ å…³é”®ä¿®å¤ï¼š
        1. ä»Neuronçº§åˆ«èšåˆ
        2. æŠ•å°„ç›®æ ‡æ˜¯Subregionï¼ˆä¸æ˜¯Regionï¼‰
        3. èšåˆä¸‰ç§locationå…³ç³»
        """
        query = """
        MATCH (r:Region {acronym: $acronym})

        // æ‰¾å±äºè¿™ä¸ªåŒºåŸŸçš„ç¥ç»å…ƒ
        OPTIONAL MATCH (n1:Neuron)-[:LOCATE_AT]->(r)
        OPTIONAL MATCH (n2:Neuron)-[:LOCATE_AT_SUBREGION]->(r)
        OPTIONAL MATCH (n3:Neuron)-[:LOCATE_AT_ME_SUBREGION]->(r)
        WITH r, (COLLECT(DISTINCT n1) + COLLECT(DISTINCT n2) + COLLECT(DISTINCT n3)) AS ns
        UNWIND ns AS n
        WITH DISTINCT n
        WHERE n IS NOT NULL

        // æ‰¾è¿™äº›ç¥ç»å…ƒçš„æŠ•å°„åˆ°Subregion
        MATCH (n)-[p:PROJECT_TO]->(t:Subregion)
        WHERE p.weight IS NOT NULL AND p.weight > 0

        WITH t.acronym AS tgt_subregion,
             SUM(p.weight) AS total_weight_to_tgt
        RETURN
          tgt_subregion,
          total_weight_to_tgt
        ORDER BY total_weight_to_tgt DESC
        """

        result = self.db.run(query, {'acronym': region})

        if not result['success'] or not result['data']:
            logger.warning(f"No projection data for {region}")
            return None

        # è·å–æ‰€æœ‰Subregion targets
        all_targets = self._get_all_targets()

        # æ„å»ºåŸå§‹æƒé‡å‘é‡
        target_dict = {row['tgt_subregion']: row['total_weight_to_tgt']
                       for row in result['data']}

        raw_values = np.array([target_dict.get(t, 0.0) for t in all_targets])

        # Logç¨³å®šåŒ–ï¼ˆå¯¹é½Ground Truthï¼‰
        log_values = np.log10(1 + raw_values)

        # å½’ä¸€åŒ–æˆæ¦‚ç‡åˆ†å¸ƒ
        total = log_values.sum()
        if total > 0:
            signature = log_values / (total + 1e-9)
        else:
            signature = log_values

        return signature

    def compute_similarity(self, fp1: np.ndarray, fp2: np.ndarray,
                          metric: str = 'cosine') -> float:
        """Compute similarity between fingerprints"""
        if metric == 'cosine':
            norm1, norm2 = np.linalg.norm(fp1), np.linalg.norm(fp2)
            if norm1 == 0 or norm2 == 0:
                return 0.0
            return float(np.dot(fp1, fp2) / (norm1 * norm2))
        elif metric == 'correlation':
            if len(fp1) < 2:
                return 0.0
            r, _ = stats.pearsonr(fp1, fp2)
            return float(r)
        else:
            return 0.0

    def compute_mismatch_index(self, region1: str, region2: str) -> Optional[Dict[str, float]]:
        """
        Compute cross-modal mismatch (Figure 4 metric)

        MM_GM = |sim_molecular - sim_morphological|
        MM_GP = |sim_molecular - sim_projection|
        """
        fp1 = self.compute_region_fingerprint(region1)
        fp2 = self.compute_region_fingerprint(region2)

        if fp1 is None or fp2 is None:
            return None

        sim_mol = self.compute_similarity(fp1['molecular'], fp2['molecular'])
        sim_mor = self.compute_similarity(fp1['morphological'], fp2['morphological'])
        sim_proj = self.compute_similarity(fp1['projection'], fp2['projection'])

        return {
            'sim_molecular': sim_mol,
            'sim_morphological': sim_mor,
            'sim_projection': sim_proj,
            'mismatch_GM': abs(sim_mol - sim_mor),
            'mismatch_GP': abs(sim_mol - sim_proj),
            'mismatch_MP': abs(sim_mor - sim_proj)
        }

    def _get_all_subclasses(self) -> List[str]:
        """
        è·å–æ‰€æœ‰subclass names (ç”¨äºåˆ†å­æŒ‡çº¹)

        ğŸ”§ æ³¨æ„: è¿™é‡Œåº”è¯¥æŸ¥è¯¢Subclassï¼Œä¸æ˜¯Cluster
        """
        if self._cluster_cache is not None:
            return self._cluster_cache

        query = """
        MATCH (sc:Subclass)
        RETURN DISTINCT sc.name AS name
        ORDER BY name
        """

        result = self.db.run(query)

        if result['success'] and result['data']:
            self._cluster_cache = [row['name'] for row in result['data']]
            logger.info(f"Found {len(self._cluster_cache)} subclasses for molecular fingerprint")
        else:
            self._cluster_cache = []
            logger.error("No subclasses found in database!")

        return self._cluster_cache

    def _get_all_targets(self) -> List[str]:
        """
        è·å–æ‰€æœ‰æŠ•å°„ç›®æ ‡Subregion (å¯¹é½Ground Truth)

        ğŸ”§ ä¿®å¤ï¼šä»Subregionè·å–ï¼Œä¸æ˜¯Region
        """
        if self._target_cache is not None:
            return self._target_cache

        query = """
        MATCH ()-[:PROJECT_TO]->(t:Subregion)
        WHERE t.acronym IS NOT NULL
        RETURN DISTINCT t.acronym AS target
        ORDER BY target
        LIMIT 500
        """

        result = self.db.run(query)

        if result['success'] and result['data']:
            self._target_cache = [row['target'] for row in result['data']]
            logger.info(f"Found {len(self._target_cache)} Subregion projection targets")
        else:
            self._target_cache = []
            logger.error("No Subregion targets found!")

        return self._target_cache

    def get_region_fingerprint(self, region: str) -> Dict:
        """
        è·å–å•ä¸ªregionçš„å®Œæ•´fingerprint

        ğŸ†• æ–°å¢æ–¹æ³• - æ”¯æŒé«˜æ€§èƒ½ç‰ˆæœ¬çš„æ‰¹é‡è®¡ç®—

        Args:
            region: è„‘åŒºacronym

        Returns:
            {
                'molecular': [array],
                'morphological': [array],
                'projection': [array]
            }
        """
        try:
            # è®¡ç®—ä¸‰ç§fingerprint
            molecular = self.compute_molecular_fingerprint(region)
            morphological = self.compute_morphological_fingerprint(region)
            projection = self.compute_projection_fingerprint(region)

            # éªŒè¯
            if molecular is None or morphological is None or projection is None:
                return None

            # è½¬æ¢ä¸ºlist (ç¡®ä¿JSONå¯åºåˆ—åŒ–)
            return {
                'molecular': molecular.tolist() if hasattr(molecular, 'tolist') else list(molecular),
                'morphological': morphological.tolist() if hasattr(morphological, 'tolist') else list(morphological),
                'projection': projection.tolist() if hasattr(projection, 'tolist') else list(projection)
            }

        except Exception as e:
            logger.error(f"Failed to get fingerprint for {region}: {e}")
            return None

    def standardize_morphology_globally(self, regions: List[str]):
        """
        å…¨å±€Z-scoreæ ‡å‡†åŒ–å½¢æ€æŒ‡çº¹ï¼ˆå¯¹é½Ground Truthæ–¹æ³•ï¼‰

        ğŸ¯ å…³é”®ï¼šåœ¨è®¡ç®—mismatchå‰ï¼Œå¯¹æ‰€æœ‰regionsçš„å½¢æ€æ•°æ®åšä¸€æ¬¡æ€§å…¨å±€æ ‡å‡†åŒ–

        Args:
            regions: éœ€è¦æ ‡å‡†åŒ–çš„regionåˆ—è¡¨
        """
        logger.info("   Performing global morphology standardization...")

        # æ”¶é›†æ‰€æœ‰regionsçš„å½¢æ€æŒ‡çº¹
        all_morph = []
        valid_regions = []

        for region in regions:
            morph = self.compute_morphological_fingerprint(region)
            if morph is not None:
                all_morph.append(morph)
                valid_regions.append(region)

        if len(all_morph) < 2:
            logger.warning("   Insufficient morphology data for standardization")
            return

        all_morph = np.array(all_morph)  # (N_regions, 8)

        logger.info(f"      Morphology array shape: {all_morph.shape}")

        # å¤„ç†dendriticç‰¹å¾çš„0å€¼ (ç´¢å¼•4-7)
        dendritic_indices = [4, 5, 6, 7]
        for i in dendritic_indices:
            col = all_morph[:, i].copy()
            zero_mask = np.abs(col) < 1e-6
            n_zeros = zero_mask.sum()
            if n_zeros > 0:
                logger.debug(f"      Dendritic feature {i}: excluding {n_zeros}/{len(col)} zeros")
                col[zero_mask] = np.nan
                all_morph[:, i] = col

        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è¿›è¡Œz-score
        from scipy.stats import zscore
        for i in range(all_morph.shape[1]):
            col = all_morph[:, i]
            valid = ~np.isnan(col)
            if valid.sum() > 1:
                col[valid] = zscore(col[valid])
                all_morph[:, i] = col

        # ç¼“å­˜ç»“æœ
        self._morph_cache = {}
        for idx, region in enumerate(valid_regions):
            self._morph_cache[region] = all_morph[idx]

        logger.info(f"      âœ“ Global standardization complete for {len(valid_regions)} regions")


class Figure4PlottingTool:
    """
    Figure 4ç»˜å›¾å·¥å…·

    å°è£…äº†signaturev4.pyçš„æ ¸å¿ƒç»˜å›¾åŠŸèƒ½ï¼Œä¾›Agentè°ƒç”¨
    """

    def __init__(self, output_dir: str = "./figure4_results"):
        """
        åˆå§‹åŒ–ç»˜å›¾å·¥å…·

        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)

        # è®¾ç½®ç»˜å›¾æ ·å¼
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False

        logger.info(f"Figure 4 plotting tool initialized. Output: {self.output_dir}")

    def plot_from_agent_data(self,
                             agent_result: Dict,
                             fingerprint_data: Optional[Dict] = None) -> Dict[str, str]:
        """
        ä»Agentç»“æœç”Ÿæˆæ‰€æœ‰Figure 4å›¾è¡¨

        Args:
            agent_result: Agentçš„è¿”å›ç»“æœï¼ˆåŒ…å«mismatchæ•°æ®ï¼‰
            fingerprint_data: å¯é€‰çš„fingerprintåŸå§‹æ•°æ®

        Returns:
            ç”Ÿæˆçš„å›¾ç‰‡æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        logger.info("Starting Figure 4 visualization from agent data...")

        output_files = {}

        # 1. ä»Agentç»“æœä¸­æå–æ•°æ®
        extracted = self._extract_data_from_agent_result(agent_result)

        if not extracted:
            logger.error("Failed to extract data from agent result")
            return {}

        regions = extracted['regions']
        mismatch_pairs = extracted['mismatch_pairs']

        logger.info(f"Extracted {len(regions)} regions and {len(mismatch_pairs)} mismatch pairs")

        # 2. æ„å»ºçŸ©é˜µ
        matrices = self._build_matrices_from_pairs(regions, mismatch_pairs)

        if not matrices:
            logger.error("Failed to build matrices")
            return {}

        # 3. ç»˜åˆ¶similarityçŸ©é˜µ (3ä¸ª)
        logger.info("Plotting similarity matrices...")
        similarity_files = self._plot_similarity_matrices(
            matrices['mol_sim'],
            matrices['morph_sim'],
            matrices['proj_sim'],
            regions
        )
        output_files.update(similarity_files)

        # 4. ç»˜åˆ¶mismatchçŸ©é˜µ (2ä¸ª)
        logger.info("Plotting mismatch matrices...")
        mismatch_files = self._plot_mismatch_matrices(
            matrices['mismatch_GM'],
            matrices['mismatch_GP'],
            regions
        )
        output_files.update(mismatch_files)

        # 5. è¯†åˆ«top pairs
        top_pairs = self._identify_top_pairs(
            matrices['mismatch_GM'],
            matrices['mismatch_GP'],
            regions,
            n=5
        )

        # 6. ç»˜åˆ¶top pairsçš„è¯¦ç»†å¯¹æ¯” (å¯é€‰ï¼Œå¦‚æœæœ‰fingerprintæ•°æ®)
        if fingerprint_data:
            logger.info("Plotting detailed comparisons for top pairs...")
            detail_files = self._plot_detailed_comparisons(
                top_pairs,
                fingerprint_data,
                regions
            )
            output_files.update(detail_files)

        logger.info(f"âœ… Generated {len(output_files)} figures")
        for name, path in output_files.items():
            logger.info(f"   â€¢ {name}: {path}")

        return output_files

    def _extract_data_from_agent_result(self, agent_result: Dict) -> Optional[Dict]:
        """
        ä»Agentç»“æœä¸­æå–ç»˜å›¾æ‰€éœ€æ•°æ® (å¢å¼ºç‰ˆ - æ”¯æŒå¤šç§æ•°æ®ä½ç½®)

        Returns:
            {
                'regions': List[str],
                'mismatch_pairs': List[Dict]
            }
        """
        regions = []
        mismatch_pairs = []

        logger.info("Extracting data from agent result...")

        # ğŸ” ç­–ç•¥1: ä»executed_stepsçš„actual_resultæå–
        for step in agent_result.get('executed_steps', []):
            purpose = step.get('purpose', '').lower()

            # å°è¯•è·å–actual_result
            actual_result = step.get('actual_result')

            if not actual_result:
                # å¦‚æœæ²¡æœ‰actual_resultï¼Œè·³è¿‡
                logger.debug(f"Step '{purpose[:40]}' has no actual_result")
                continue

            if not actual_result.get('success'):
                continue

            data = actual_result.get('data', [])

            if not data:
                continue

            logger.debug(f"Step '{purpose[:50]}': {len(data)} rows")

            # æå–regions
            if any(kw in purpose for kw in ['region', 'identify', 'top', 'neuron']):
                for row in data:
                    region = row.get('region') or row.get('acronym') or row.get('region_name')
                    if region and region not in regions:
                        regions.append(region)

                if regions:
                    logger.info(f"  Found {len(regions)} regions from: {purpose[:50]}")

            # æå–mismatch pairs
            if 'mismatch' in purpose:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«mismatchå­—æ®µ
                if data and isinstance(data[0], dict):
                    has_mismatch = any(
                        key in data[0]
                        for key in ['mismatch_combined', 'mismatch_GM', 'mismatch_GP']
                    )

                    if has_mismatch:
                        mismatch_pairs.extend(data)
                        logger.info(f"  Found {len(data)} mismatch pairs from: {purpose[:50]}")

        # ğŸ” ç­–ç•¥2: ä»intermediate_dataæå–ï¼ˆFallbackï¼‰
        if not regions or not mismatch_pairs:
            logger.info("Strategy 1 failed, trying intermediate_data...")

            intermediate = agent_result.get('intermediate_data', {})

            for key, data in intermediate.items():
                if not data or not isinstance(data, list):
                    continue

                if not data:
                    continue

                first_row = data[0] if isinstance(data, list) and data else {}

                # æŸ¥æ‰¾regions
                if not regions:
                    if isinstance(first_row, dict) and ('region' in first_row or 'acronym' in first_row):
                        for row in data:
                            region = row.get('region') or row.get('acronym')
                            if region and region not in regions:
                                regions.append(region)

                        if regions:
                            logger.info(f"  Found {len(regions)} regions from {key}")

                # æŸ¥æ‰¾mismatch pairs
                if not mismatch_pairs:
                    if isinstance(first_row, dict) and any(
                            k in first_row
                            for k in ['mismatch_combined', 'mismatch_GM', 'region1', 'region2']
                    ):
                        mismatch_pairs.extend(data)
                        logger.info(f"  Found {len(data)} mismatch pairs from {key}")

        # æœ€ç»ˆéªŒè¯
        if not regions:
            logger.error("âŒ No regions found in agent result")
            logger.error(f"   Available keys: {list(agent_result.keys())}")

            if 'executed_steps' in agent_result:
                logger.error(f"   Executed steps: {len(agent_result['executed_steps'])}")
                for i, step in enumerate(agent_result['executed_steps'], 1):
                    purpose = step.get('purpose', 'Unknown')
                    has_actual = 'actual_result' in step
                    logger.error(f"     Step {i}: {purpose[:40]} - has_actual_result: {has_actual}")

            return None

        if not mismatch_pairs:
            logger.error("âŒ No mismatch pairs found in agent result")
            return None

        logger.info(f"âœ… Successfully extracted: {len(regions)} regions, {len(mismatch_pairs)} pairs")

        return {
            'regions': regions,
            'mismatch_pairs': mismatch_pairs
        }

    def _build_matrices_from_pairs(self,
                                   regions: List[str],
                                   pairs: List[Dict]) -> Optional[Dict]:
        """
        ä»pairåˆ—è¡¨æ„å»ºçŸ©é˜µ

        Returns:
            {
                'mol_sim': DataFrame,
                'morph_sim': DataFrame,
                'proj_sim': DataFrame,
                'mismatch_GM': DataFrame,
                'mismatch_GP': DataFrame
            }
        """
        n = len(regions)
        region_to_idx = {r: i for i, r in enumerate(regions)}

        # åˆå§‹åŒ–çŸ©é˜µ
        mol_sim = np.full((n, n), np.nan)
        morph_sim = np.full((n, n), np.nan)
        proj_sim = np.full((n, n), np.nan)
        mismatch_GM = np.full((n, n), np.nan)
        mismatch_GP = np.full((n, n), np.nan)

        # å¯¹è§’çº¿è®¾ä¸º1ï¼ˆè‡ªå·±å’Œè‡ªå·±ç›¸ä¼¼åº¦=1ï¼‰
        np.fill_diagonal(mol_sim, 1.0)
        np.fill_diagonal(morph_sim, 1.0)
        np.fill_diagonal(proj_sim, 1.0)
        np.fill_diagonal(mismatch_GM, 0.0)
        np.fill_diagonal(mismatch_GP, 0.0)

        # å¡«å……æ•°æ®
        for pair in pairs:
            r1 = pair.get('region1')
            r2 = pair.get('region2')

            if not r1 or not r2:
                continue

            if r1 not in region_to_idx or r2 not in region_to_idx:
                continue

            i = region_to_idx[r1]
            j = region_to_idx[r2]

            # ç›¸ä¼¼åº¦
            mol_sim[i, j] = mol_sim[j, i] = pair.get('sim_molecular', np.nan)
            morph_sim[i, j] = morph_sim[j, i] = pair.get('sim_morphological', np.nan)
            proj_sim[i, j] = proj_sim[j, i] = pair.get('sim_projection', np.nan)

            # Mismatch
            mismatch_GM[i, j] = mismatch_GM[j, i] = pair.get('mismatch_GM', np.nan)
            mismatch_GP[i, j] = mismatch_GP[j, i] = pair.get('mismatch_GP', np.nan)

        # è½¬æ¢ä¸ºDataFrame
        return {
            'mol_sim': pd.DataFrame(mol_sim, index=regions, columns=regions),
            'morph_sim': pd.DataFrame(morph_sim, index=regions, columns=regions),
            'proj_sim': pd.DataFrame(proj_sim, index=regions, columns=regions),
            'mismatch_GM': pd.DataFrame(mismatch_GM, index=regions, columns=regions),
            'mismatch_GP': pd.DataFrame(mismatch_GP, index=regions, columns=regions)
        }

    def _plot_similarity_matrices(self,
                                  mol_sim: pd.DataFrame,
                                  morph_sim: pd.DataFrame,
                                  proj_sim: pd.DataFrame,
                                  regions: List[str]) -> Dict[str, str]:
        """
        ç»˜åˆ¶3ä¸ªsimilarityçŸ©é˜µ

        Returns:
            æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        output_files = {}

        # 1. Molecular Similarity
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(mol_sim, ax=ax, cmap='RdYlBu_r', vmin=0, vmax=1,
                    square=True, cbar_kws={'label': 'Similarity'},
                    xticklabels=True, yticklabels=True)
        ax.set_title('Molecular Fingerprint Similarity', fontsize=20, fontweight='bold')
        ax.set_xlabel('Region', fontsize=16, fontweight='bold')
        ax.set_ylabel('Region', fontsize=16, fontweight='bold')
        plt.tight_layout()

        filepath = self.output_dir / '1_molecular_similarity.png'
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        output_files['molecular_similarity'] = str(filepath)

        # 2. Morphology Similarity
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(morph_sim, ax=ax, cmap='RdYlBu_r', vmin=0, vmax=1,
                    square=True, cbar_kws={'label': 'Similarity'},
                    xticklabels=True, yticklabels=True)
        ax.set_title('Morphology Fingerprint Similarity', fontsize=20, fontweight='bold')
        ax.set_xlabel('Region', fontsize=16, fontweight='bold')
        ax.set_ylabel('Region', fontsize=16, fontweight='bold')
        plt.tight_layout()

        filepath = self.output_dir / '2_morphology_similarity.png'
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        output_files['morphology_similarity'] = str(filepath)

        # 3. Projection Similarity
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(proj_sim, ax=ax, cmap='RdYlBu_r', vmin=0, vmax=1,
                    square=True, cbar_kws={'label': 'Similarity'},
                    xticklabels=True, yticklabels=True)
        ax.set_title('Projection Fingerprint Similarity', fontsize=20, fontweight='bold')
        ax.set_xlabel('Region', fontsize=16, fontweight='bold')
        ax.set_ylabel('Region', fontsize=16, fontweight='bold')
        plt.tight_layout()

        filepath = self.output_dir / '3_projection_similarity.png'
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        output_files['projection_similarity'] = str(filepath)

        return output_files

    def _plot_mismatch_matrices(self,
                                mismatch_GM: pd.DataFrame,
                                mismatch_GP: pd.DataFrame,
                                regions: List[str]) -> Dict[str, str]:
        """
        ç»˜åˆ¶2ä¸ªmismatchçŸ©é˜µ
        """
        output_files = {}

        # 1. Molecular-Morphology Mismatch
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(mismatch_GM, ax=ax, cmap='RdYlBu_r', vmin=0, vmax=1,
                    square=True, cbar_kws={'label': 'Mismatch'},
                    xticklabels=True, yticklabels=True)
        ax.set_title('Molecular-Morphology Mismatch', fontsize=20, fontweight='bold')
        ax.set_xlabel('Region', fontsize=16, fontweight='bold')
        ax.set_ylabel('Region', fontsize=16, fontweight='bold')
        plt.tight_layout()

        filepath = self.output_dir / '4_mol_morph_mismatch.png'
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        output_files['mol_morph_mismatch'] = str(filepath)

        # 2. Molecular-Projection Mismatch
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(mismatch_GP, ax=ax, cmap='RdYlBu_r', vmin=0, vmax=1,
                    square=True, cbar_kws={'label': 'Mismatch'},
                    xticklabels=True, yticklabels=True)
        ax.set_title('Molecular-Projection Mismatch', fontsize=20, fontweight='bold')
        ax.set_xlabel('Region', fontsize=16, fontweight='bold')
        ax.set_ylabel('Region', fontsize=16, fontweight='bold')
        plt.tight_layout()

        filepath = self.output_dir / '5_mol_proj_mismatch.png'
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        output_files['mol_proj_mismatch'] = str(filepath)

        return output_files

    def _identify_top_pairs(self,
                            mismatch_GM: pd.DataFrame,
                            mismatch_GP: pd.DataFrame,
                            regions: List[str],
                            n: int = 5) -> Dict:
        """
        è¯†åˆ«top N mismatch pairs

        Returns:
            {
                'mol_morph': [(r1, r2, mismatch_val), ...],
                'mol_proj': [(r1, r2, mismatch_val), ...]
            }
        """
        # Molecular-Morphology top pairs
        mm_values = []
        for i in range(len(regions)):
            for j in range(i + 1, len(regions)):
                val = mismatch_GM.iloc[i, j]
                if not np.isnan(val):
                    mm_values.append((regions[i], regions[j], val))

        mm_values.sort(key=lambda x: x[2], reverse=True)

        # Molecular-Projection top pairs
        mp_values = []
        for i in range(len(regions)):
            for j in range(i + 1, len(regions)):
                val = mismatch_GP.iloc[i, j]
                if not np.isnan(val):
                    mp_values.append((regions[i], regions[j], val))

        mp_values.sort(key=lambda x: x[2], reverse=True)

        return {
            'mol_morph': mm_values[:n],
            'mol_proj': mp_values[:n]
        }

    def _plot_detailed_comparisons(self,
                                   top_pairs: Dict,
                                   fingerprint_data: Dict,
                                   regions: List[str]) -> Dict[str, str]:
        """
        ç»˜åˆ¶top pairsçš„è¯¦ç»†å¯¹æ¯”å›¾ï¼ˆé›·è¾¾å›¾+æŸ±çŠ¶å›¾ï¼‰

        è¿™éœ€è¦åŸå§‹çš„fingerprintæ•°æ®
        """
        # TODO: å®ç°è¯¦ç»†å¯¹æ¯”å›¾
        # éœ€è¦ä»fingerprint_dataä¸­æå–å½¢æ€ç‰¹å¾å’ŒæŠ•å°„æ•°æ®
        logger.info("Detailed comparison plots not yet implemented")
        return {}


# ==================== Agenté›†æˆæ¥å£ ====================

def create_plotting_tool_for_agent(output_dir: str = "./figure4_agent_output") -> Figure4PlottingTool:
    """
    åˆ›å»ºä¾›Agentä½¿ç”¨çš„ç»˜å›¾å·¥å…·å®ä¾‹

    Args:
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        Figure4PlottingToolå®ä¾‹
    """
    return Figure4PlottingTool(output_dir)


def generate_figure4_from_agent_result(agent_result: Dict,
                                       output_dir: str = "./figure4_agent_output") -> Dict[str, str]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä»Agentç»“æœç›´æ¥ç”ŸæˆFigure 4æ‰€æœ‰å›¾è¡¨

    Args:
        agent_result: Agent.answer()çš„è¿”å›ç»“æœ
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        ç”Ÿæˆçš„å›¾ç‰‡æ–‡ä»¶è·¯å¾„å­—å…¸
    """
    tool = Figure4PlottingTool(output_dir)
    return tool.plot_from_agent_data(agent_result)
# ==================== Production Agent V10 ====================

class AIPOMCoTV10:
    """
    AIPOM-CoT V10 ç”Ÿäº§ç‰ˆæœ¬

    å®Œæ•´åŠŸèƒ½:
    1. æ™ºèƒ½å®ä½“è¯†åˆ« (æ— éœ€hardcodedåˆ—è¡¨)
    2. åŠ¨æ€Schemaè·¯å¾„è§„åˆ’ (å›¾ç®—æ³•)
    3. ç»“æ„åŒ–åæ€ (é‡åŒ–è¯„ä¼°)
    4. å®Œæ•´ç»Ÿè®¡å·¥å…·
    5. å¤šæ¨¡æ€åˆ†æ
    6. è‡ªé€‚åº”é‡è§„åˆ’
    """

    def __init__(self,
                 neo4j_uri: str,
                 neo4j_user: str,
                 neo4j_pwd: str,
                 database: str,
                 schema_json_path: str,
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4o"):

        # æ•°æ®åº“è¿æ¥
        self.db = Neo4jExec(neo4j_uri, neo4j_user, neo4j_pwd, database=database)

        # Schema
        self.schema = RealSchemaCache(schema_json_path)

        # ===== æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ– =====

        # P0-1: æ™ºèƒ½å®ä½“è¯†åˆ«
        logger.info("ğŸ” Initializing intelligent entity recognition...")
        self.entity_recognizer = IntelligentEntityRecognizer(self.db, self.schema)
        self.entity_clusterer = EntityClusteringEngine(self.db, self.schema)

        # P1-1: åŠ¨æ€Schemaè·¯å¾„è§„åˆ’
        logger.info("ğŸ—ºï¸  Initializing dynamic schema path planning...")
        self.path_planner = DynamicSchemaPathPlanner(self.schema)

        # P1-2: ç»“æ„åŒ–åæ€
        logger.info("ğŸ¤” Initializing structured reflection...")
        self.reflector = StructuredReflector()

        # åŸæœ‰ç»„ä»¶
        self.stats = StatisticalTools()
        self.fingerprint = RealFingerprintAnalyzer(self.db, self.schema)

        # OpenAI
        self.client = OpenAI(api_key=openai_api_key)
        self.model = model


        self.adaptive_planner = AdaptivePlanner(self.schema, self.path_planner,self.client)
        # ğŸ†• æ·»åŠ Focus-Driven Planner
        logger.info("ğŸ¯ Initializing focus-driven planning...")
        from focus_driven_planner import FocusDrivenPlanner
        self.focus_planner = FocusDrivenPlanner(self.schema, self.db)

        # ğŸ†• æ·»åŠ Comparative Analysis Planner
        logger.info("ğŸ“Š Initializing comparative analysis planning...")
        from comparative_analysis_planner import ComparativeAnalysisPlanner
        self.comparative_planner = ComparativeAnalysisPlanner(
            self.db,
            self.fingerprint,
            self.stats
        )

        logger.info("âœ… AIPOM-CoT V10 initialized successfully!")
        logger.info(f"   â€¢ Entity recognition: Ready")
        logger.info(f"   â€¢ Schema path planning: Ready")
        logger.info(f"   â€¢ Structured reflection: Ready")

    # ==================== Main Entry Point ====================

    """
    å®Œæ•´çš„answeræ–¹æ³•å®ç° - é›†æˆè‡ªé€‚åº”è§„åˆ’
    """

    def answer(self, question: str, max_iterations: int = 15) -> Dict[str, Any]:
        """
        ä¸»å…¥å£: å›ç­”é—®é¢˜ (å®Œæ•´ç‰ˆ)

        å®Œæ•´æµç¨‹:
        1. æ™ºèƒ½å®ä½“è¯†åˆ«
        2. å®ä½“èšç±»
        3. ç¡®å®šåˆ†ææ·±åº¦
        4. æ™ºèƒ½é€‰æ‹©è§„åˆ’å™¨ (Adaptive/Focus-Driven/Comparative)
        5. è‡ªé€‚åº”æ‰§è¡Œå¾ªç¯ (åŒ…å«ç»Ÿè®¡åˆ†æ)
        6. ç­”æ¡ˆåˆæˆ (ç§‘å­¦å™äº‹)
        """
        logger.info(f"ğŸ¯ Question: {question}")
        start_time = time.time()

        state = EnhancedAgentState(question=question)

        # ===== PHASE 1: INTELLIGENT PLANNING =====
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“‹ PHASE 1: INTELLIGENT PLANNING (Enhanced)")
        logger.info("=" * 70)

        state.phase = AgentPhase.PLANNING

        # Step 1-2: å®ä½“è¯†åˆ« + èšç±»
        logger.info("  [1/4] Intelligent entity recognition...")
        entity_matches = self.entity_recognizer.recognize_entities(question)
        state.entity_matches = entity_matches

        logger.info(f"     Found {len(entity_matches)} entity matches")
        for match in entity_matches[:5]:
            logger.info(f"       â€¢ {match.text} ({match.entity_type}) [{match.confidence:.2f}]")

        logger.info("  [2/4] Entity clustering...")
        entity_clusters = self.entity_clusterer.cluster_entities(entity_matches, question)
        state.entity_clusters = entity_clusters

        logger.info(f"     Created {len(entity_clusters)} entity clusters")
        for cluster in entity_clusters:
            logger.info(f"       â€¢ {cluster.cluster_type}: {cluster.primary_entity.text}")

        # ğŸ†• Step 3: ç¡®å®šåˆ†ææ·±åº¦
        from adaptive_planner import determine_analysis_depth, AnalysisState

        logger.info("  [3/4] Determining analysis depth...")
        target_depth = determine_analysis_depth(question)
        logger.info(f"     Target depth: {target_depth.value}")

        # ğŸ†• Step 4: åˆå§‹åŒ–åˆ†æçŠ¶æ€
        logger.info("  [4/4] Initializing analysis state...")

        analysis_state = AnalysisState(
            discovered_entities={},
            executed_steps=[],
            modalities_covered=[],
            current_focus='gene' if entity_clusters and entity_clusters[0].cluster_type == 'gene_marker' else 'region',
            target_depth=target_depth,
            question_intent=self._classify_question_intent(question)
        )

        # å¡«å……åˆå§‹å®ä½“
        for cluster in entity_clusters:
            entity_type = cluster.primary_entity.entity_type
            entity_id = cluster.primary_entity.entity_id

            analysis_state.discovered_entities.setdefault(entity_type, []).append(entity_id)

            for related in cluster.related_entities:
                analysis_state.discovered_entities.setdefault(
                    related.entity_type, []
                ).append(related.entity_id)

        # å…¼å®¹æ€§
        state.entities = [
            {'text': m.text, 'type': m.entity_type, 'confidence': m.confidence}
            for m in entity_matches[:10]
        ]

        # ğŸ†• å­˜å‚¨analysis_stateåˆ°state
        state.analysis_state = analysis_state

        logger.info(f"âœ… Planning complete")
        logger.info(f"   â€¢ Target depth: {target_depth.value}")
        logger.info(f"   â€¢ Initial entities: {list(analysis_state.discovered_entities.keys())}")

        # ===== PHASE 2: ADAPTIVE EXECUTION =====
        logger.info("\n" + "=" * 70)
        logger.info("âš™ï¸ PHASE 2: ADAPTIVE EXECUTION (Multi-Planner)")
        logger.info("=" * 70)

        state.phase = AgentPhase.EXECUTING

        iteration = 0
        while iteration < max_iterations:
            # ğŸ†• å†³å®šæ˜¯å¦ç»§ç»­
            if not self.adaptive_planner.should_continue(analysis_state, question):
                logger.info("ğŸ“Œ Analysis complete (adaptive decision)")
                break

            # ğŸ†• æ™ºèƒ½é€‰æ‹©è§„åˆ’å™¨
            planner_type = self._select_planner(analysis_state, question)

            if planner_type == 'focus_driven':
                logger.info(f"\nğŸ¯ Using FOCUS-DRIVEN planner (iteration {iteration + 1})...")
                next_steps = self.focus_planner.generate_focus_driven_plan(
                    analysis_state,
                    question
                )

            elif planner_type == 'comparative':
                logger.info(f"\nğŸ“Š Using COMPARATIVE planner (iteration {iteration + 1})...")
                next_steps = self.comparative_planner.generate_comparative_plan(
                    analysis_state,
                    question
                )

            else:
                logger.info(f"\nğŸ”„ Using ADAPTIVE planner (iteration {iteration + 1})...")
                next_steps = self.adaptive_planner.plan_next_steps(
                    analysis_state,
                    question,
                    max_steps=2
                )

            if not next_steps:
                logger.info("ğŸ“Œ No more steps available")
                break

            # æ‰§è¡Œè§„åˆ’çš„æ­¥éª¤
            for candidate_step in next_steps:
                if iteration >= max_iterations:
                    break

                logger.info(f"\nğŸ”¹ Step {iteration + 1}: {candidate_step.purpose}")
                logger.info(f"   Type: {candidate_step.step_type}")
                logger.info(f"   Priority: {candidate_step.priority:.1f}")
                if hasattr(candidate_step, 'llm_score') and candidate_step.llm_score > 0:
                    logger.info(f"   LLM score: {candidate_step.llm_score:.2f}")

                # ğŸ†• è½¬æ¢ä¸ºReasoningStep
                reasoning_step = self._convert_candidate_to_reasoning(
                    candidate_step,
                    iteration + 1,
                    analysis_state
                )

                # æ‰§è¡Œ
                exec_result = self._execute_step(reasoning_step, state)

                if not exec_result['success']:
                    logger.error(f"   âŒ Failed: {exec_result.get('error')}")

                    if state.replanning_count < state.max_replanning:
                        logger.info(f"   ğŸ”„ Replanning...")
                        state.replanning_count += 1

                    continue

                # ğŸ†• ç»“æ„åŒ–åæ€
                structured_reflection = self.reflector.reflect(
                    step_number=reasoning_step.step_number,
                    purpose=reasoning_step.purpose,
                    expected_result=reasoning_step.expected_result,
                    actual_result=reasoning_step.actual_result,
                    question_context=question
                )

                reasoning_step.reflection = structured_reflection.summary
                reasoning_step.validation_passed = (
                        structured_reflection.validation_status.value in ['passed', 'partial']
                )

                state.structured_reflections.append(structured_reflection)
                state.reflections.append(structured_reflection.summary)

                logger.info(f"   ğŸ“Š Reflection: {structured_reflection.summary}")
                logger.info(f"   ğŸ“ˆ Confidence: {structured_reflection.confidence_score:.3f}")

                # ğŸ†• æ›´æ–°åˆ†æçŠ¶æ€
                self._update_analysis_state(
                    analysis_state,
                    reasoning_step,
                    exec_result,
                    candidate_step
                )

                state.executed_steps.append(reasoning_step)
                iteration += 1

        # ===== PHASE 3: ANSWER SYNTHESIS =====
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“ PHASE 3: ANSWER SYNTHESIS")
        logger.info("=" * 70)

        final_answer = self._synthesize_answer(state)

        execution_time = time.time() - start_time

        # æ„å»ºè¿”å›ç»“æœ
        result = {
            'question': question,
            'answer': final_answer,

            'entities_recognized': [
                {
                    'text': m.text,
                    'type': m.entity_type,
                    'confidence': m.confidence,
                    'match_type': m.match_type
                }
                for m in state.entity_matches[:10]
            ],

            'reasoning_plan': [self._step_to_dict(s) for s in state.executed_steps],
            'executed_steps': [self._step_to_dict(s) for s in state.executed_steps],

            'reflections': state.reflections,
            'structured_reflections': [
                {
                    'step': r.step_number,
                    'status': r.validation_status.value,
                    'confidence': r.confidence_score,
                    'uncertainty': r.uncertainty.overall_uncertainty,
                    'should_replan': r.should_replan
                }
                for r in state.structured_reflections
            ],

            # ğŸ†• è‡ªé€‚åº”è§„åˆ’ä¿¡æ¯
            'adaptive_planning': {
                'target_depth': target_depth.value,
                'final_depth': len(state.executed_steps),
                'modalities_covered': analysis_state.modalities_covered,
                'entities_discovered': {
                    k: len(v) for k, v in analysis_state.discovered_entities.items()
                },
                'primary_focus': getattr(analysis_state, 'primary_focus', None)
            },

            'replanning_count': state.replanning_count,
            'confidence_score': state.confidence_score,
            'execution_time': execution_time,
            'total_steps': len(state.executed_steps),
            'schema_paths_used': state.schema_paths_used,
            'intermediate_data': state.intermediate_data
        }

        logger.info(f"\nâœ… Completed in {execution_time:.2f}s")
        logger.info(f"   â€¢ Steps executed: {len(state.executed_steps)}")
        logger.info(f"   â€¢ Confidence: {state.confidence_score:.3f}")
        logger.info(f"   â€¢ Modalities: {', '.join(analysis_state.modalities_covered)}")

        return result

    def answer_with_visualization(self,
                                  question: str,
                                  max_iterations: int = 15,
                                  generate_plots: bool = True,
                                  output_dir: str = "./figure4_results") -> Dict[str, Any]:
        """
        å›ç­”é—®é¢˜å¹¶ç”Ÿæˆå¯è§†åŒ–ï¼ˆFigure 4å¢å¼ºç‰ˆï¼‰

        Args:
            question: é—®é¢˜
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            generate_plots: æ˜¯å¦ç”Ÿæˆå›¾è¡¨
            output_dir: å›¾è¡¨è¾“å‡ºç›®å½•

        Returns:
            åŒ…å«answerå’Œvisualization_filesçš„ç»“æœ
        """
        # 1. æ­£å¸¸æ‰§è¡Œåˆ†æ
        result = self.answer(question, max_iterations)

        # 2. å¦‚æœæ˜¯Figure 4ç±»å‹çš„åˆ†æï¼Œç”Ÿæˆå›¾è¡¨
        if generate_plots:
            analysis_type = self._detect_analysis_type_from_result(result)

            if analysis_type == 'figure4_mismatch':
                logger.info("\n" + "=" * 70)
                logger.info("ğŸ¨ GENERATING FIGURE 4 VISUALIZATIONS")
                logger.info("=" * 70)

                try:
                    visualization_files = generate_figure4_from_agent_result(
                        result,
                        output_dir
                    )

                    result['visualization_files'] = visualization_files
                    result['visualization_output_dir'] = output_dir

                    logger.info(f"\nâœ… Generated {len(visualization_files)} figures:")
                    for name, path in visualization_files.items():
                        logger.info(f"   â€¢ {name}: {path}")

                except Exception as e:
                    logger.error(f"âŒ Visualization generation failed: {e}")
                    import traceback
                    traceback.print_exc()
                    result['visualization_error'] = str(e)

        return result

    def _detect_analysis_type_from_result(self, result: Dict) -> str:
        """
        ä»ç»“æœä¸­æ£€æµ‹åˆ†æç±»å‹

        Returns:
            'figure4_mismatch' | 'figure3_focus' | 'other'
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰mismatchè®¡ç®—
        has_mismatch = any(
            'mismatch' in step['purpose'].lower()
            for step in result.get('executed_steps', [])
        )

        # æ£€æŸ¥æ˜¯å¦æ˜¯systematic screening
        has_screening = any(
            'systematic' in step['purpose'].lower() or
            'top' in step['purpose'].lower() and 'region' in step['purpose'].lower()
            for step in result.get('executed_steps', [])
        )

        if has_mismatch and has_screening:
            return 'figure4_mismatch'

        return 'other'

    # ==================== è¾…åŠ©æ–¹æ³• ====================
    def _select_planner(self, state, question: str) -> str:
        """
        æ™ºèƒ½é€‰æ‹©è§„åˆ’å™¨ï¼ˆå¢å¼ºç‰ˆ v2.0ï¼‰

        ğŸ”§ å…³é”®ä¿®å¤ï¼š
        1. å¢å¼ºsystematic screeningæ£€æµ‹
        2. æ·»åŠ æ›´å¤šæ¯”è¾ƒå…³é”®è¯
        3. æ”¹è¿›æ—¥å¿—

        Returns:
            'focus_driven' | 'comparative' | 'adaptive'
        """
        q_lower = question.lower()

        logger.info(f"   ğŸ¯ Selecting planner for: {question[:60]}...")

        # ====== Priority 1: æ¯”è¾ƒæŸ¥è¯¢ â†’ Comparative ======
        compare_keywords = [
            'compare', 'comparison', 'comparing',
            'versus', 'vs ', 'vs.', ' vs',
            'difference between', 'differences between',
            'contrast', 'contrasting',
            'distinguish', 'differentiate',
        ]

        for keyword in compare_keywords:
            if keyword in q_lower:
                logger.info(f"      Comparison keyword '{keyword}' detected â†’ comparative")
                return 'comparative'

        # ====== Priority 2: Systematic screening â†’ Comparative ======
        # ğŸ”§ å¢å¼ºæ£€æµ‹é€»è¾‘

        # å…³é”®è¯æ£€æµ‹
        systematic_keywords = [
            'which regions', 'which brain regions', 'which areas',
            'find all', 'identify all', 'list all',
            'screen', 'screening', 'systematic', 'systematically',
            'highest', 'top regions', 'top brain regions',
            'most', 'strongest', 'largest',
            'mismatch', 'discordant', 'inconsistent', 'divergent',
            'show', 'exhibit', 'demonstrate', 'display', 'have'
        ]

        # æ£€æŸ¥æ˜¯å¦åŒ…å«systematicå…³é”®è¯
        has_systematic_keyword = any(kw in q_lower for kw in systematic_keywords)

        # æ¨¡å¼æ£€æµ‹
        has_which = 'which' in q_lower
        has_superlative = any(w in q_lower for w in ['highest', 'top', 'most', 'strongest', 'largest', 'best', 'worst'])
        has_mismatch = any(w in q_lower for w in ['mismatch', 'discordant', 'inconsistent', 'divergent'])
        has_show_verb = any(w in q_lower for w in ['show', 'exhibit', 'demonstrate', 'display', 'have'])

        # ç»„åˆåˆ¤æ–­
        is_systematic = False
        reason = ""

        if has_which and has_superlative:
            is_systematic = True
            reason = "which + superlative pattern"
        elif has_which and has_mismatch:
            is_systematic = True
            reason = "which + mismatch pattern"
        elif has_which and has_show_verb:
            is_systematic = True
            reason = "which + show/exhibit pattern"
        elif has_systematic_keyword:
            is_systematic = True
            reason = f"systematic keyword detected"

        if is_systematic:
            logger.info(f"      Systematic screening detected ({reason}) â†’ comparative")
            return 'comparative'

        # ====== Priority 3: Focus-driven â†’ æœ‰regionsçš„æ·±åº¦æŸ¥è¯¢ ======
        if 'Region' in state.discovered_entities:
            n_regions = len(state.discovered_entities.get('Region', []))
            if n_regions > 0:
                logger.info(f"      {n_regions} regions found â†’ focus_driven")
                return 'focus_driven'

        # ====== Priority 4: Focus-driven â†’ GeneæŸ¥è¯¢ä¸”æœ‰æ·±åº¦æ„å›¾ ======
        if 'GeneMarker' in state.discovered_entities:
            deep_intent_keywords = [
                'tell me about', 'about',
                'analyze', 'analysis', 'characterize', 'characterization',
                'comprehensive', 'detailed', 'in-depth'
            ]

            if any(kw in q_lower for kw in deep_intent_keywords):
                logger.info(f"      Gene query with deep intent â†’ focus_driven")
                return 'focus_driven'

        # ====== Default: Adaptive ======
        logger.info(f"      Default â†’ adaptive")
        return 'adaptive'

    def _classify_question_intent(self, question: str) -> str:
        """åˆ†ç±»é—®é¢˜æ„å›¾"""
        question_lower = question.lower()

        if any(w in question_lower for w in ['compare', 'difference', 'versus', 'vs']):
            return 'comparison'
        elif any(w in question_lower for w in ['comprehensive', 'detailed', 'everything']):
            return 'comprehensive'
        elif any(w in question_lower for w in ['why', 'explain', 'how']):
            return 'explanatory'
        elif any(w in question_lower for w in ['which', 'find', 'identify']):
            return 'screening'
        else:
            return 'simple_query'


    def _convert_candidate_to_reasoning(self, candidate, step_number, analysis_state):
        """è½¬æ¢CandidateStep (ä¿®å¤ç‰ˆ)"""
        params = candidate.parameters.copy()

        # ğŸ”§ æ™ºèƒ½åˆ¤æ–­action
        has_cypher = bool(candidate.cypher_template and candidate.cypher_template.strip())

        if not has_cypher:
            # ç‰¹æ®Šæ­¥éª¤
            if 'statistical' in candidate.step_type.lower() or 'fdr' in candidate.step_id.lower():
                action = 'execute_statistical'
            elif 'multi-modal' in candidate.step_type.lower() or 'mismatch' in candidate.step_id.lower():
                action = 'execute_fingerprint'
            else:
                action = 'execute_cypher'
        else:
            action = 'execute_cypher'

        return ReasoningStep(
            step_number=step_number,
            purpose=candidate.purpose,
            action=action,  # ğŸ”§ æ­£ç¡®çš„action
            rationale=candidate.rationale,
            expected_result=candidate.expected_data,
            query_or_params={
                'query': candidate.cypher_template,
                'params': params
            },
            modality=candidate.step_type,
            depends_on=getattr(candidate, 'depends_on', [])
        )

    def _update_analysis_state(self,
                               analysis_state,
                               step: ReasoningStep,
                               result: Dict,
                               candidate):
        """
        æ›´æ–°åˆ†æçŠ¶æ€ï¼ˆå¢å¼ºç‰ˆ v2.0ï¼‰

        ğŸ”§ å…³é”®ä¿®å¤ï¼š
        1. å¤šå­—æ®µå…¼å®¹çš„ProjectionTargetæå–
        2. æ™ºèƒ½fallbackæœºåˆ¶
        3. å¢å¼ºæ—¥å¿—
        """
        # è®°å½•æ‰§è¡Œçš„æ­¥éª¤
        analysis_state.executed_steps.append({
            'purpose': step.purpose,
            'modality': step.modality,
            'row_count': len(result.get('data', [])),
            'step_id': candidate.step_id
        })

        # æ›´æ–°modalityè¦†ç›–
        if step.modality and step.modality not in analysis_state.modalities_covered:
            analysis_state.modalities_covered.append(step.modality)

        # ğŸ†• æå–æ–°å‘ç°çš„å®ä½“
        data = result.get('data', [])
        if not data:
            return

        first_row = data[0]

        # ====== æå–Regions ======
        if 'region' in first_row or 'acronym' in first_row:
            regions = list(set([
                row.get('region') or row.get('acronym')
                for row in data
                if row.get('region') or row.get('acronym')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Region', [])
            for r in regions:
                if r and r not in existing:
                    existing.append(r)

        # ====== æå–Clusters ======
        if 'cluster' in first_row or 'cluster_name' in first_row:
            clusters = list(set([
                row.get('cluster') or row.get('cluster_name')
                for row in data
                if row.get('cluster') or row.get('cluster_name')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Cluster', [])
            for c in clusters:
                if c and c not in existing:
                    existing.append(c)

        # ====== æå–Subclasses ======
        if 'subclass' in first_row or 'subclass_name' in first_row:
            subclasses = list(set([
                row.get('subclass') or row.get('subclass_name')
                for row in data
                if row.get('subclass') or row.get('subclass_name')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Subclass', [])
            for s in subclasses:
                if s and s not in existing:
                    existing.append(s)

        # ====== ğŸ”§ å¢å¼º: æå–Projection Targets (å¤šç­–ç•¥) ======

        # ç­–ç•¥1: æ£€æŸ¥å¸¸è§å­—æ®µå
        target_field_candidates = [
            'target', 'target_region', 'target_acronym', 'target_name',
            'tgt', 'tgt_region', 'projection_target',
            'downstream', 'downstream_region',
            'dest', 'destination', 'to_region'  # æ·»åŠ æ›´å¤šå¯èƒ½çš„å­—æ®µå
        ]

        targets_found = []
        matched_field = None

        for field in target_field_candidates:
            if field in first_row:
                matched_field = field
                targets_found = [
                    row.get(field)
                    for row in data
                    if row.get(field) and isinstance(row.get(field), str)
                ]
                if targets_found:
                    logger.info(f"   ğŸ“ Found targets via field: '{field}'")
                    break

        # ç­–ç•¥2: å¦‚æœstep purposeåŒ…å«"projection"ä½†æ²¡æ‰¾åˆ°æ ‡å‡†å­—æ®µï¼Œæ™ºèƒ½æå–
        if not targets_found and 'projection' in step.purpose.lower():
            logger.info(f"   ğŸ” Fallback: Intelligent target extraction")

            # å°è¯•ä»æ‰€æœ‰å­—æ®µä¸­æ‰¾region-likeå€¼
            for row in data[:20]:  # æ£€æŸ¥å‰20è¡Œ
                for key, value in row.items():
                    # è·³è¿‡æ˜æ˜¾çš„sourceå­—æ®µ
                    if key in ['source', 'source_region', 'region', 'acronym']:
                        continue

                    # è¯†åˆ«å¯èƒ½çš„region acronym (2-5ä¸ªå¤§å†™å­—æ¯)
                    if isinstance(value, str) and 2 <= len(value) <= 5 and value.isupper():
                        targets_found.append(value)
                        logger.debug(f"      Found potential target: {value} (from field: {key})")

        # ç­–ç•¥3: æ£€æŸ¥stepçš„actual_resultä¸­æ˜¯å¦æœ‰summaryä¿¡æ¯
        if not targets_found and hasattr(step, 'actual_result'):
            actual = step.actual_result
            if isinstance(actual, dict) and 'summary' in actual:
                summary = actual['summary']
                if 'targets' in summary:
                    targets_found = summary['targets']
                    logger.info(f"   ğŸ“ Found targets from step summary")

        # å»é‡å¹¶æ·»åŠ åˆ°discovered_entities
        if targets_found:
            targets_unique = list(set([t for t in targets_found if t]))

            existing = analysis_state.discovered_entities.setdefault('ProjectionTarget', [])

            new_targets = []
            for t in targets_unique:
                if t and t not in existing:
                    existing.append(t)
                    new_targets.append(t)

            if new_targets:
                logger.info(f"   ğŸ“ Discovered {len(new_targets)} NEW projection targets: {new_targets[:5]}")
                logger.info(f"      Total targets now: {len(existing)}")
            else:
                logger.info(f"   ğŸ“ Found {len(targets_unique)} targets (already known)")
        else:
            # å¦‚æœæ˜¯projectionæ­¥éª¤ä½†æ²¡æ‰¾åˆ°targetsï¼Œè­¦å‘Š
            if 'projection' in step.purpose.lower():
                logger.warning(f"   âš ï¸ Projection step but no targets extracted!")
                logger.warning(f"      Available fields: {list(first_row.keys())}")
                logger.warning(f"      This may prevent closed-loop analysis")


    def _enhanced_planning_phase(self, state: EnhancedAgentState) -> Dict[str, Any]:
        """
        å¢å¼ºçš„è§„åˆ’é˜¶æ®µ

        æ­¥éª¤:
        1. æ™ºèƒ½å®ä½“è¯†åˆ« (æ— hardcodedåˆ—è¡¨!)
        2. å®ä½“èšç±»
        3. åŠ¨æ€Schemaè·¯å¾„è§„åˆ’
        4. LLMç²¾åŒ–
        """
        try:
            # Step 1: å®ä½“è¯†åˆ«
            logger.info("  [1/4] Intelligent entity recognition...")
            entity_matches = self.entity_recognizer.recognize_entities(state.question)
            state.entity_matches = entity_matches

            logger.info(f"     Found {len(entity_matches)} entity matches")
            for match in entity_matches[:5]:
                logger.info(f"       â€¢ {match.text} ({match.entity_type}) [{match.confidence:.2f}]")

            # Step 2: å®ä½“èšç±»
            logger.info("  [2/4] Entity clustering...")
            entity_clusters = self.entity_clusterer.cluster_entities(
                entity_matches,
                state.question
            )
            state.entity_clusters = entity_clusters

            logger.info(f"     Created {len(entity_clusters)} entity clusters")
            for cluster in entity_clusters:
                logger.info(f"       â€¢ {cluster.cluster_type}: {cluster.primary_entity.text}")

            # Step 3: åŠ¨æ€Schemaè·¯å¾„è§„åˆ’
            logger.info("  [3/4] Dynamic schema path planning...")
            query_plans = self.path_planner.generate_plan(entity_clusters, state.question)

            logger.info(f"     Generated {len(query_plans)} query plans")

            # è®°å½•ä½¿ç”¨çš„schemaè·¯å¾„
            for plan in query_plans:
                if plan.schema_path.hops:
                    state.schema_paths_used.append({
                        'start': plan.schema_path.start_label,
                        'end': plan.schema_path.end_label,
                        'hops': len(plan.schema_path.hops),
                        'score': plan.schema_path.score
                    })

            # Step 4: LLMç²¾åŒ–
            logger.info("  [4/4] LLM plan refinement...")
            refined_steps = self._llm_refine_plans(query_plans, state)
            state.reasoning_plan = refined_steps

            # ä¿å­˜å®ä½“åˆ°state (å…¼å®¹åŸæœ‰æ ¼å¼)
            state.entities = [
                {
                    'text': m.text,
                    'type': m.entity_type,
                    'confidence': m.confidence
                }
                for m in entity_matches[:10]
            ]

            return {'success': True}

        except Exception as e:
            logger.error(f"Enhanced planning failed: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _llm_refine_plans(self,
                          query_plans: List,
                          state: EnhancedAgentState) -> List[ReasoningStep]:
        """
        LLMç²¾åŒ–æŸ¥è¯¢è®¡åˆ’

        å°†åŠ¨æ€ç”Ÿæˆçš„QueryPlanè½¬æ¢ä¸ºReasoningStep,å¹¶è®©LLMè¡¥å……ç»†èŠ‚
        """
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        plans_dict = []
        for qp in query_plans:
            plans_dict.append({
                'step': qp.step_number,
                'purpose': qp.purpose,
                'action': qp.action,
                'query': qp.cypher_template,
                'parameters': qp.parameters,
                'modality': qp.modality,
                'depends_on': qp.depends_on,
                'schema_path_score': qp.schema_path.score if qp.schema_path else 0.0
            })

        prompt = f"""You are refining a reasoning plan for neuroscience knowledge graph analysis.

**Question:** {state.question}

**Recognized Entities:** {', '.join([e['text'] for e in state.entities])}

**Dynamically Generated Query Plans:**
{json.dumps(plans_dict, indent=2)}

Your task:
1. Review each query plan
2. Add detailed **expected_result** descriptions
3. Enhance **rationale** with domain knowledge
4. Verify Cypher query correctness
5. Add any missing steps if needed

Return a JSON object with key "steps" containing an array:
{{
  "steps": [
    {{
      "step_number": 1,
      "purpose": "...",
      "action": "execute_cypher",
      "rationale": "Detailed explanation",
      "expected_result": "Concrete prediction of what data will look like",
      "query_or_params": {{"query": "...", "params": {{}}}},
      "modality": "molecular/morphological/projection",
      "depends_on": []
    }},
    ...
  ]
}}

**Important:**
- Make rationale SPECIFIC and scientifically grounded
- Expected results should describe DATA PATTERNS (e.g., "10-20 clusters with neuron counts ranging 500-5000")
- Ensure query syntax is correct
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert neuroscientist and Neo4j query expert."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.2
            )

            result = json.loads(response.choices[0].message.content)

            # è½¬æ¢ä¸ºReasoningStep
            steps = []
            for step_dict in result.get('steps', []):
                query_or_params = step_dict.get('query_or_params', {})

                # å¤„ç†å‚æ•°æ›¿æ¢
                if isinstance(query_or_params, dict):
                    if 'query' not in query_or_params and 'query' in step_dict:
                        query_or_params = {'query': step_dict['query']}

                step = ReasoningStep(
                    step_number=step_dict.get('step_number', len(steps) + 1),
                    purpose=step_dict.get('purpose', ''),
                    action=step_dict.get('action', 'execute_cypher'),
                    rationale=step_dict.get('rationale', ''),
                    expected_result=step_dict.get('expected_result', ''),
                    query_or_params=query_or_params,
                    modality=step_dict.get('modality'),
                    depends_on=step_dict.get('depends_on', [])
                )
                steps.append(step)

            return steps

        except Exception as e:
            logger.error(f"LLM refinement failed: {e}")

            # Fallback: ç›´æ¥è½¬æ¢QueryPlan
            fallback_steps = []
            for qp in query_plans:
                step = ReasoningStep(
                    step_number=qp.step_number,
                    purpose=qp.purpose,
                    action=qp.action,
                    rationale="Automatically generated from schema path",
                    expected_result="Data matching query criteria",
                    query_or_params={'query': qp.cypher_template, 'params': qp.parameters},
                    modality=qp.modality,
                    depends_on=qp.depends_on
                )
                fallback_steps.append(step)

            return fallback_steps

    def _characterize_top_pairs(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """
        æ·±å…¥åˆ†ætop mismatch pairs (Case Study)

        ğŸ†• æ–°å¢åŠŸèƒ½:
        1. æå–top N pairs
        2. æŸ¥è¯¢æ¯ä¸ªpairçš„è¯¦ç»†æ•°æ®:
           - Morphological features
           - Projection targets
           - Molecular composition
        """
        n_top = params.get('n_top_pairs', 3)

        # ä»FDRç»“æœè·å–top pairs
        fdr_data = None
        for key, data in state.intermediate_data.items():
            if data and isinstance(data, list) and len(data) > 0:
                if 'fdr_significant' in data[0] and data[0].get('fdr_significant'):
                    fdr_data = data
                    break

        if not fdr_data:
            logger.warning("   No FDR significant pairs found, using top mismatch pairs")
            # Fallback: ä½¿ç”¨top mismatch
            for key, data in state.intermediate_data.items():
                if data and isinstance(data, list) and len(data) > 0:
                    if 'mismatch_combined' in data[0]:
                        fdr_data = sorted(data, key=lambda x: x['mismatch_combined'], reverse=True)
                        break

        if not fdr_data:
            return {'success': False, 'error': 'No mismatch data found', 'data': []}

        # é€‰æ‹©top N pairs
        top_pairs = fdr_data[:n_top]

        logger.info(f"   Analyzing top {len(top_pairs)} pairs:")
        for pair in top_pairs:
            logger.info(f"     â€¢ {pair['region1']} vs {pair['region2']}: mismatch={pair['mismatch_combined']:.3f}")

        # è¯¦ç»†åˆ†ææ¯ä¸ªpair
        detailed_results = []

        for pair in top_pairs:
            region1 = pair['region1']
            region2 = pair['region2']

            logger.info(f"   Deep characterization: {region1} vs {region2}")

            # ğŸ”¹ 1. Morphological comparison
            morph_query = """
            MATCH (n:Neuron)-[:LOCATE_AT]->(r:Region)
            WHERE r.acronym IN [$region1, $region2]
            RETURN r.acronym AS region,
                   count(n) AS neuron_count,
                   avg(n.axonal_length) AS avg_axon,
                   avg(n.dendritic_length) AS avg_dendrite,
                   avg(n.axonal_branches) AS avg_axon_branches,
                   avg(n.dendritic_branches) AS avg_dendrite_branches,
                   stdev(n.axonal_length) AS std_axon,
                   stdev(n.dendritic_length) AS std_dendrite
            """
            morph_result = self.db.run(morph_query, {'region1': region1, 'region2': region2})

            # ğŸ”¹ 2. Projection targets comparison
            proj_query = """
            MATCH (r:Region)-[p:PROJECT_TO]->(t:Region)
            WHERE r.acronym IN [$region1, $region2]
            RETURN r.acronym AS source,
                   t.acronym AS target,
                   t.name AS target_name,
                   p.weight AS weight
            ORDER BY r.acronym, p.weight DESC
            LIMIT 30
            """
            proj_result = self.db.run(proj_query, {'region1': region1, 'region2': region2})

            # ğŸ”¹ 3. Molecular composition
            mol_query = """
            MATCH (r:Region)-[:HAS_CLUSTER]->(c:Cluster)
            WHERE r.acronym IN [$region1, $region2]
            RETURN r.acronym AS region,
                   c.name AS cluster,
                   c.markers AS markers,
                   c.number_of_neurons AS neurons
            ORDER BY r.acronym, c.number_of_neurons DESC
            LIMIT 20
            """
            mol_result = self.db.run(mol_query, {'region1': region1, 'region2': region2})

            # æ•´åˆç»“æœ
            detailed_results.append({
                'pair': f"{region1}_vs_{region2}",
                'region1': region1,
                'region2': region2,
                'mismatch_score': pair['mismatch_combined'],
                'p_value': pair.get('p_value', 1.0),
                'q_value': pair.get('q_value', 1.0),
                'morphology': morph_result.get('data', []),
                'projections': proj_result.get('data', []),
                'molecular': mol_result.get('data', [])
            })

        logger.info(f"   âœ… Detailed characterization complete for {len(detailed_results)} pairs")

        return {
            'success': True,
            'data': detailed_results,
            'rows': len(detailed_results),
            'analysis_type': 'case_study'
        }

    # ==================== Execution ====================

    def _execute_step(self, step: ReasoningStep, state: EnhancedAgentState) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªæ­¥éª¤ (ä¿®å¤ç‰ˆ - æ”¯æŒcase study)"""
        start_time = time.time()

        try:
            query = step.query_or_params.get('query', '').strip()
            params = step.query_or_params.get('params', {})

            # åˆ¤æ–­æ‰§è¡Œç±»å‹
            if not query:
                # ğŸ†• Case studyæ£€æµ‹
                if 'characterize' in step.purpose.lower() and 'top' in step.purpose.lower():
                    result = self._characterize_top_pairs(params, state)
                elif 'mismatch' in step.purpose.lower():
                    result = self._execute_fingerprint_step(step, state)
                elif 'statistical' in step.purpose.lower() or 'fdr' in step.purpose.lower():
                    result = self._execute_statistical_step(step, state)
                else:
                    result = {'success': False, 'error': 'Cannot determine execution type'}
            else:
                result = self._execute_cypher_step(step, state)

            step.actual_result = result
            step.execution_time = time.time() - start_time

            step_key = f"step_{step.step_number}"
            state.intermediate_data[step_key] = result.get('data', [])

            return result

        except Exception as e:
            logger.error(f"Step execution failed: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _execute_cypher_step(self, step: ReasoningStep, state: EnhancedAgentState) -> Dict[str, Any]:
        """æ‰§è¡ŒCypheræŸ¥è¯¢æ­¥éª¤"""
        query = step.query_or_params.get('query', '').strip()
        params = step.query_or_params.get('params', {})

        # ğŸ”§ ç©ºæŸ¥è¯¢æ£€æŸ¥
        if not query:
            logger.warning(f"   Empty Cypher query - skipping")
            return {'success': False, 'error': 'Empty query', 'data': []}

        # å‚æ•°æ›¿æ¢
        if step.depends_on:
            params = self._resolve_parameters(step, state, params)

        # è‡ªåŠ¨æ·»åŠ LIMIT
        import re
        if not re.search(r'\bLIMIT\b', query, re.IGNORECASE):
            query = f"{query}\nLIMIT 100"

        return self.db.run(query, params)

    def _execute_statistical_step(self,
                                  step: ReasoningStep,
                                  state: EnhancedAgentState) -> Dict[str, Any]:
        """
        ğŸ†• æ‰§è¡Œç»Ÿè®¡æ­¥éª¤
        """
        params = step.query_or_params.get('params', {})
        test_type = params.get('test_type', 'permutation')

        logger.info(f"   ğŸ“Š Statistical test: {test_type}")

        try:
            if test_type == 'permutation':
                return self._permutation_test(params, state)

            elif test_type == 'fdr':
                return self._fdr_correction(params, state)

            elif test_type == 'correlation':
                return self._correlation_test(params, state)

            else:
                return {'success': False, 'error': f'Unknown test type: {test_type}'}

        except Exception as e:
            logger.error(f"Statistical test failed: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _execute_fingerprint_step(self,
                                  step: ReasoningStep,
                                  state: EnhancedAgentState) -> Dict[str, Any]:
        """
        ğŸ†• æ‰§è¡Œfingerprintè®¡ç®—æ­¥éª¤
        """
        params = step.query_or_params.get('params', {})
        analysis_type = params.get('analysis_type', 'cross_modal_mismatch')

        logger.info(f"   ğŸ”¬ Fingerprint analysis: {analysis_type}")

        if analysis_type == 'cross_modal_mismatch':
            return self._compute_mismatch_matrix(params, state)
        else:
            return {'success': False, 'error': f'Unknown analysis type: {analysis_type}'}

    def _resolve_parameters(self,
                            step: ReasoningStep,
                            state: EnhancedAgentState,
                            params: Dict) -> Dict:
        """è§£ææ­¥éª¤ä¾èµ–çš„å‚æ•°"""
        resolved = params.copy()

        # æŸ¥æ‰¾ä¾èµ–æ­¥éª¤çš„æ•°æ®
        for dep_num in step.depends_on:
            dep_key = f"step_{dep_num}"
            if dep_key in state.intermediate_data:
                dep_data = state.intermediate_data[dep_key]

                # æå–å¸¸ç”¨å­—æ®µ
                if dep_data:
                    # æå–region acronyms
                    regions = []
                    for row in dep_data:
                        if 'region' in row:
                            regions.append(row['region'])
                        elif 'acronym' in row:
                            regions.append(row['acronym'])

                    if regions:
                        resolved['enriched_regions'] = regions[:10]
                        resolved['target_regions'] = regions[:10]

        return resolved

    def _execute_cypher(self, query: str, params: Dict) -> Dict[str, Any]:
        """æ‰§è¡ŒCypheræŸ¥è¯¢"""
        import re

        # ç¡®ä¿æœ‰LIMIT
        if not re.search(r'\bLIMIT\b', query, re.IGNORECASE):
            query = f"{query}\nLIMIT 100"

        return self.db.run(query, params)

    def _permutation_test(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """Permutation test for morphological differences"""
        entity_a = params['entity_a']
        entity_b = params['entity_b']

        # ä»ä¹‹å‰çš„stepè·å–æ•°æ®
        morph_data = None
        for key, data in state.intermediate_data.items():
            if data and isinstance(data, list) and len(data) > 0:
                if 'region' in data[0] and ('avg_axon' in data[0] or 'avg_axon_length' in data[0]):
                    morph_data = data
                    break

        if not morph_data:
            return {'success': False, 'error': 'No morphological data found'}

        # æå–ä¸¤ç»„æ•°æ®
        group_a = [row for row in morph_data if row.get('region') == entity_a]
        group_b = [row for row in morph_data if row.get('region') == entity_b]

        if not group_a or not group_b:
            return {'success': False,
                    'error': f'Insufficient data: {entity_a}={len(group_a)}, {entity_b}={len(group_b)}'}

        # æå–axon length
        import numpy as np
        axon_key = 'avg_axon' if 'avg_axon' in group_a[0] else 'avg_axon_length'
        axon_a = np.array([row.get(axon_key, 0) or 0 for row in group_a])
        axon_b = np.array([row.get(axon_key, 0) or 0 for row in group_b])

        # ç§»é™¤é›¶å€¼
        axon_a = axon_a[axon_a > 0]
        axon_b = axon_b[axon_b > 0]

        if len(axon_a) == 0 or len(axon_b) == 0:
            return {'success': False, 'error': 'No valid morphology data'}

        # è®¡ç®—observed difference
        observed_diff = float(np.mean(axon_a) - np.mean(axon_b))

        # ğŸ¯ è°ƒç”¨ç»Ÿè®¡å·¥å…·!
        result = self.stats.permutation_test(
            observed_stat=observed_diff,
            data1=axon_a,
            data2=axon_b,
            n_permutations=1000,
            seed=42
        )

        # è®¡ç®—effect size
        effect_size = self.stats.cohens_d(axon_a, axon_b)

        # æ ¼å¼åŒ–ç»“æœ
        result_data = [{
            'comparison': f'{entity_a} vs {entity_b}',
            'feature': 'axonal_length',
            'mean_a': float(np.mean(axon_a)),
            'mean_b': float(np.mean(axon_b)),
            'observed_difference': observed_diff,
            'p_value': result['p_value'],
            'effect_size_cohens_d': effect_size,
            'significance': 'significant' if result['p_value'] < 0.05 else 'not significant',
            'interpretation': self._interpret_statistical_result(result, effect_size)
        }]

        logger.info(f"   âœ… Permutation test: p={result['p_value']:.4f}, d={effect_size:.2f}")

        return {
            'success': True,
            'data': result_data,
            'rows': len(result_data),
            'test_type': 'permutation'
        }

    def _fdr_correction(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """
        FDR correction (è¶…å¼ºè°ƒè¯•ç‰ˆ)

        ğŸ”§ å…¨é¢è°ƒè¯•å’Œå®¹é”™
        """
        alpha = params.get('alpha', 0.05)

        logger.info(f"   === FDR Correction Debug ===")
        logger.info(f"   Available data keys: {list(state.intermediate_data.keys())}")

        # ğŸ”§ å¢å¼ºæ•°æ®æŸ¥æ‰¾
        mismatch_data = None
        mismatch_key = None

        # ç­–ç•¥1: æŸ¥æ‰¾åŒ…å«'mismatch_combined'å’Œ'p_value'çš„æ•°æ®
        for key, data in state.intermediate_data.items():
            logger.debug(f"   Checking {key}: type={type(data)}, len={len(data) if isinstance(data, list) else 'N/A'}")

            if not data:
                continue

            if isinstance(data, list) and len(data) > 0:
                first_row = data[0]
                logger.debug(
                    f"     First row keys: {first_row.keys() if isinstance(first_row, dict) else 'Not a dict'}")

                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                has_mismatch = 'mismatch_combined' in first_row if isinstance(first_row, dict) else False
                has_pvalue = 'p_value' in first_row if isinstance(first_row, dict) else False

                logger.debug(f"     has_mismatch={has_mismatch}, has_pvalue={has_pvalue}")

                if has_mismatch and has_pvalue:
                    mismatch_data = data
                    mismatch_key = key
                    logger.info(f"   âœ“ Found mismatch data in {key} ({len(data)} rows)")
                    break

        # ç­–ç•¥2: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»æœ€è¿‘çš„stepè·å–
        if not mismatch_data:
            logger.warning("   Strategy 1 failed, trying strategy 2...")

            # æŒ‰keyæ’åºï¼Œæ‰¾æœ€è¿‘çš„step
            sorted_keys = sorted([k for k in state.intermediate_data.keys() if k.startswith('step_')],
                                 key=lambda x: int(x.split('_')[1]) if len(x.split('_')) > 1 and x.split('_')[
                                     1].isdigit() else 0,
                                 reverse=True)

            logger.debug(f"   Sorted keys: {sorted_keys}")

            for key in sorted_keys:
                data = state.intermediate_data[key]
                if data and isinstance(data, list) and len(data) > 0:
                    first_row = data[0]
                    if isinstance(first_row, dict) and 'mismatch_combined' in first_row:
                        logger.info(f"   âœ“ Found mismatch data in {key} (strategy 2)")
                        mismatch_data = data
                        mismatch_key = key

                        # ğŸ”§ å¦‚æœæ²¡æœ‰p_valueï¼Œæ·»åŠ é»˜è®¤å€¼
                        if 'p_value' not in first_row:
                            logger.warning(f"   Adding default p_values")
                            for row in mismatch_data:
                                if 'p_value' not in row:
                                    row['p_value'] = 1.0 - min(0.99, row.get('mismatch_combined', 0))

                        break

        # æœ€ç»ˆæ£€æŸ¥
        if not mismatch_data:
            logger.error("   âœ— No mismatch data found!")
            logger.error(f"   Available keys: {list(state.intermediate_data.keys())}")

            # æ‰“å°æ‰€æœ‰æ•°æ®çš„æ ·æœ¬
            for key, data in state.intermediate_data.items():
                if data and isinstance(data, list) and len(data) > 0:
                    logger.error(
                        f"   {key} sample: {list(data[0].keys()) if isinstance(data[0], dict) else type(data[0])}")

            return {
                'success': False,
                'error': 'No mismatch data with p-values found',
                'data': []
            }

        # æå–p-values
        p_values = []
        for row in mismatch_data:
            pval = row.get('p_value', None)
            if pval is not None:
                p_values.append(float(pval))
            else:
                logger.warning(
                    f"   Row missing p_value: {row.get('region1', 'unknown')}-{row.get('region2', 'unknown')}")
                p_values.append(1.0)

        logger.info(f"   FDR input: {len(p_values)} p-values")
        logger.info(f"   P-value range: [{min(p_values):.4f}, {max(p_values):.4f}]")
        logger.info(f"   P-values < 0.05: {sum(1 for p in p_values if p < 0.05)}")

        # ğŸ¯ æ‰§è¡ŒFDR correction
        try:
            q_values, significant = self.stats.fdr_correction(p_values, alpha)

            # æ•´åˆç»“æœ
            result_data = []
            for i, row in enumerate(mismatch_data):
                result_data.append({
                    **row,
                    'q_value': float(q_values[i]),
                    'fdr_significant': bool(significant[i])
                })

            # ç­›é€‰æ˜¾è‘—çš„
            significant_data = [r for r in result_data if r['fdr_significant']]

            logger.info(f"   âœ… FDR: {len(significant_data)}/{len(result_data)} significant (Î±={alpha})")

            if significant_data:
                top = significant_data[0]
                logger.info(f"   Top: {top['region1']}-{top['region2']}")
                logger.info(f"     Mismatch: {top['mismatch_combined']:.3f}")
                logger.info(f"     Q-value: {top['q_value']:.4f}")
            else:
                logger.warning(f"   No significant pairs after FDR correction")
                logger.warning(f"   Smallest q-value: {min(q_values):.4f}")
                logger.warning(f"   Consider: alpha={alpha} may be too stringent")

            return {
                'success': True,
                'data': significant_data,
                'rows': len(significant_data),
                'test_type': 'fdr',
                'alpha': alpha,
                'n_significant': len(significant_data),
                'n_total': len(result_data),
                'min_q_value': float(min(q_values)),
                'max_q_value': float(max(q_values))
            }

        except Exception as e:
            logger.error(f"   FDR correction failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e),
                'data': []
            }

    def _correlation_test(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """Correlation test between modalities"""
        # å®ç°correlation (å¯é€‰,æš‚æ—¶è¿”å›placeholder)
        logger.warning("Correlation test not yet implemented")
        return {'success': False, 'error': 'Not implemented'}

    # def _compute_mismatch_matrix(self, params: Dict, state: EnhancedAgentState) -> Dict:
    #     """
    #     è®¡ç®—cross-modal mismatchçŸ©é˜µ (å¯¹é½Figure 4æ–¹æ³•)
    #
    #     ğŸ¯ å…³é”®ä¿®å¤:
    #     1. å…ˆè®¡ç®—æ‰€æœ‰pairsçš„è·ç¦»çŸ©é˜µ
    #     2. å…¨å±€Min-Maxå½’ä¸€åŒ–
    #     3. ç„¶åè®¡ç®—mismatch
    #     """
    #     import time
    #     start_time = time.time()
    #
    #     # è·å–regions
    #     regions = state.analysis_state.discovered_entities.get('Region', [])
    #
    #     if not regions:
    #         for key, data in state.intermediate_data.items():
    #             if data and isinstance(data, list) and len(data) > 0:
    #                 if 'region' in data[0]:
    #                     regions = list(set([row['region'] for row in data if row.get('region')]))
    #                     break
    #
    #     max_regions = params.get('max_regions', 15)
    #     regions = regions[:max_regions]
    #
    #     if len(regions) < 2:
    #         return {'success': False, 'error': 'Need at least 2 regions'}
    #
    #     n = len(regions)
    #     logger.info(f"   ğŸš€ Computing mismatch (Figure 4 method) for {n} regions...")
    #
    #     # ğŸš€ Step 1: æ‰¹é‡è·å–fingerprints
    #     logger.info(f"   ğŸ“Š Step 1/4: Batch fetching fingerprints...")
    #
    #     fingerprints = {}
    #     failed_regions = []
    #
    #     for region in regions:
    #         try:
    #             mol = self.fingerprint.compute_molecular_fingerprint(region)
    #             morph = self.fingerprint.compute_morphological_fingerprint(region)
    #             proj = self.fingerprint.compute_projection_fingerprint(region)
    #
    #             if mol is not None and morph is not None and proj is not None:
    #                 fingerprints[region] = {
    #                     'molecular': mol,
    #                     'morphological': morph,
    #                     'projection': proj
    #                 }
    #             else:
    #                 failed_regions.append(region)
    #
    #         except Exception as e:
    #             logger.warning(f"      Failed {region}: {e}")
    #             failed_regions.append(region)
    #
    #     valid_regions = [r for r in regions if r not in failed_regions]
    #     n_valid = len(valid_regions)
    #
    #     logger.info(f"      âœ“ Got fingerprints: {len(fingerprints)}/{n}")
    #
    #     if n_valid < 2:
    #         return {'success': False, 'error': 'Insufficient valid regions'}
    #     # ğŸ†• Step 1.5: Z-scoreæ ‡å‡†åŒ–å½¢æ€æŒ‡çº¹ (Figure 4æ–¹æ³•)
    #     logger.info(f"   ğŸ”§ Step 1.5/4: Z-score standardization of morphology...")
    #     import numpy as np
    #     if len(fingerprints) >= 2:
    #         # æå–æ‰€æœ‰å½¢æ€æŒ‡çº¹
    #         all_morph = []
    #         for region in valid_regions:
    #             morph = fingerprints[region]['morphological']
    #             all_morph.append(morph)
    #
    #         all_morph = np.array(all_morph)  # (N_regions, 8)
    #
    #         logger.info(f"      Morphology array shape: {all_morph.shape}")
    #
    #         # å¤„ç†dendriticç‰¹å¾çš„0å€¼ (ç´¢å¼•4-7)
    #         dendritic_indices = [4, 5, 6, 7]
    #         for i in dendritic_indices:
    #             col = all_morph[:, i].copy()
    #             zero_mask = np.abs(col) < 1e-6
    #             n_zeros = zero_mask.sum()
    #             if n_zeros > 0:
    #                 logger.info(f"      Dendritic feature {i}: excluding {n_zeros}/{len(col)} zeros")
    #                 col[zero_mask] = np.nan
    #                 all_morph[:, i] = col
    #
    #         # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è¿›è¡Œz-score
    #         from scipy.stats import zscore
    #         for i in range(all_morph.shape[1]):
    #             col = all_morph[:, i]
    #             valid = ~np.isnan(col)
    #             if valid.sum() > 1:
    #                 col[valid] = zscore(col[valid])
    #                 all_morph[:, i] = col
    #
    #         # æ›´æ–°fingerprints
    #         for idx, region in enumerate(valid_regions):
    #             fingerprints[region]['morphological'] = all_morph[idx]
    #
    #         logger.info(f"      âœ“ Z-score standardization complete")
    #
    #     # ğŸš€ Step 2: æ„å»ºè·ç¦»çŸ©é˜µ (NxN)
    #     logger.info(f"   ğŸ“ Step 2/4: Building distance matrices...")
    #
    #     import numpy as np
    #     from scipy.spatial.distance import cosine, euclidean
    #
    #     mol_dist_matrix = np.zeros((n_valid, n_valid))
    #     morph_dist_matrix = np.zeros((n_valid, n_valid))
    #     proj_dist_matrix = np.zeros((n_valid, n_valid))
    #
    #     # åœ¨Step 2: æ„å»ºè·ç¦»çŸ©é˜µä¸­
    #     for i, region_a in enumerate(valid_regions):
    #         for j, region_b in enumerate(valid_regions):
    #             if i == j:
    #                 mol_dist_matrix[i, j] = 0
    #                 morph_dist_matrix[i, j] = 0
    #                 proj_dist_matrix[i, j] = 0
    #                 continue
    #
    #             fp_a = fingerprints[region_a]
    #             fp_b = fingerprints[region_b]
    #
    #             # åˆ†å­è·ç¦» (ä¿æŒä¸å˜)
    #             try:
    #                 mol_dist_matrix[i, j] = cosine(fp_a['molecular'], fp_b['molecular'])
    #             except:
    #                 mol_dist_matrix[i, j] = np.nan
    #
    #             # ğŸ”§ å½¢æ€è·ç¦» (ä¿®å¤ - ä½¿ç”¨Euclidean)
    #             try:
    #                 morph_a = fp_a['morphological']
    #                 morph_b = fp_b['morphological']
    #
    #                 # æ£€æŸ¥NaN
    #                 valid_mask = ~(np.isnan(morph_a) | np.isnan(morph_b))
    #
    #                 if valid_mask.sum() >= 4:  # è‡³å°‘4ä¸ªæœ‰æ•ˆç»´åº¦
    #                     # ğŸ¯ ä½¿ç”¨Euclideanè·ç¦»ï¼ˆä¸æ˜¯cosineï¼‰
    #                     morph_dist_matrix[i, j] = euclidean(
    #                         morph_a[valid_mask],
    #                         morph_b[valid_mask]
    #                     )
    #                 else:
    #                     morph_dist_matrix[i, j] = np.nan
    #             except Exception as e:
    #                 logger.debug(f"      Morph distance failed {region_a}-{region_b}: {e}")
    #                 morph_dist_matrix[i, j] = np.nan
    #
    #             # æŠ•å°„è·ç¦» (ä¿æŒä¸å˜)
    #             try:
    #                 proj_dist_matrix[i, j] = cosine(fp_a['projection'], fp_b['projection'])
    #             except:
    #                 proj_dist_matrix[i, j] = np.nan
    #
    #     print(f"      âœ“ Distance matrices built")
    #     # åœ¨ "âœ“ Distance matrices built" åé¢æ·»åŠ 
    #     print(
    #         f"      Molecular distance range: [{np.nanmin(mol_dist_matrix):.3f}, {np.nanmax(mol_dist_matrix):.3f}]")
    #     print(
    #         f"      Morphology distance range: [{np.nanmin(morph_dist_matrix):.3f}, {np.nanmax(morph_dist_matrix):.3f}]")
    #     print(
    #         f"      Projection distance range: [{np.nanmin(proj_dist_matrix):.3f}, {np.nanmax(proj_dist_matrix):.3f}]")
    #
    #     # ç»Ÿè®¡NaNæ•°é‡
    #     n_total = mol_dist_matrix.size
    #     n_mol_nan = np.isnan(mol_dist_matrix).sum()
    #     n_morph_nan = np.isnan(morph_dist_matrix).sum()
    #     n_proj_nan = np.isnan(proj_dist_matrix).sum()
    #
    #     print(
    #         f"      NaN counts: mol={n_mol_nan}/{n_total}, morph={n_morph_nan}/{n_total}, proj={n_proj_nan}/{n_total}")
    #
    #     # ğŸš€ Step 3: Min-Maxå½’ä¸€åŒ– (å…¨å±€)
    #     print(f"   ğŸ”§ Step 3/4: Normalizing distance matrices...")
    #
    #     def minmax_normalize(matrix):
    #         """Min-Maxå½’ä¸€åŒ–åˆ°[0,1]"""
    #         valid = ~np.isnan(matrix)
    #         if valid.sum() == 0:
    #             return matrix
    #
    #         vmin = matrix[valid].min()
    #         vmax = matrix[valid].max()
    #
    #         if vmax - vmin < 1e-9:
    #             return np.zeros_like(matrix)
    #
    #         normalized = (matrix - vmin) / (vmax - vmin)
    #         return normalized
    #
    #     mol_norm = minmax_normalize(mol_dist_matrix)
    #     morph_norm = minmax_normalize(morph_dist_matrix)
    #     proj_norm = minmax_normalize(proj_dist_matrix)
    #
    #     print(f"      âœ“ Normalization complete")
    #     print(f"      Normalized molecular range: [{np.nanmin(mol_norm):.3f}, {np.nanmax(mol_norm):.3f}]")
    #     print(f"      Normalized morphology range: [{np.nanmin(morph_norm):.3f}, {np.nanmax(morph_norm):.3f}]")
    #     print(f"      Normalized projection range: [{np.nanmin(proj_norm):.3f}, {np.nanmax(proj_norm):.3f}]")
    #
    #     # ğŸš€ Step 4: è®¡ç®—Mismatch (å½’ä¸€åŒ–è·ç¦»çš„å·®å¼‚)
    #     print(f"   ğŸ§® Step 4/4: Computing mismatches...")
    #
    #     mismatch_results = []
    #
    #     from itertools import combinations
    #
    #     for i, region1 in enumerate(valid_regions):
    #         for j, region2 in enumerate(valid_regions):
    #             if i >= j:  # åªè®¡ç®—ä¸Šä¸‰è§’
    #                 continue
    #
    #             # Mismatch = |normalized_distance_A - normalized_distance_B|
    #             mismatch_GM = abs(mol_norm[i, j] - morph_norm[i, j])
    #             mismatch_GP = abs(mol_norm[i, j] - proj_norm[i, j])
    #             mismatch_MP = abs(morph_norm[i, j] - proj_norm[i, j])
    #
    #             mismatch_combined = (mismatch_GM + mismatch_GP + mismatch_MP) / 3
    #
    #             # ç›¸ä¼¼åº¦ (ç”¨äºæŠ¥å‘Š)
    #             sim_molecular = 1 - mol_dist_matrix[i, j]
    #             sim_morphological = 1 - morph_norm[i, j]  # å½’ä¸€åŒ–åçš„
    #             sim_projection = 1 - proj_dist_matrix[i, j]
    #
    #             mismatch_results.append({
    #                 'region1': region1,
    #                 'region2': region2,
    #                 'mismatch_GM': float(mismatch_GM),
    #                 'mismatch_GP': float(mismatch_GP),
    #                 'mismatch_MP': float(mismatch_MP),
    #                 'mismatch_combined': float(mismatch_combined),
    #                 'sim_molecular': float(sim_molecular),
    #                 'sim_morphological': float(sim_morphological),
    #                 'sim_projection': float(sim_projection),
    #                 # è·ç¦»å€¼ (è°ƒè¯•ç”¨)
    #                 'dist_molecular': float(mol_dist_matrix[i, j]),
    #                 'dist_morphological': float(morph_dist_matrix[i, j]),
    #                 'dist_projection': float(proj_dist_matrix[i, j])
    #             })
    #
    #     # ç»Ÿè®¡æ£€éªŒ
    #     all_mismatches = [r['mismatch_combined'] for r in mismatch_results]
    #     mean_m = np.mean(all_mismatches)
    #     std_m = np.std(all_mismatches)
    #
    #     for result in mismatch_results:
    #         m = result['mismatch_combined']
    #
    #         if std_m > 0:
    #             z_score = (m - mean_m) / std_m
    #             from scipy import stats
    #             p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))
    #         else:
    #             z_score = 0
    #             p_value = 1.0
    #
    #         result['p_value'] = float(p_value)
    #         result['z_score'] = float(z_score)
    #         result['effect_size'] = float(m)
    #         result['n_permutations'] = 0
    #
    #     mismatch_results.sort(key=lambda x: x['mismatch_combined'], reverse=True)
    #
    #     elapsed = time.time() - start_time
    #
    #     print(f"   âœ… Completed in {elapsed:.1f}s")
    #     print(f"      Total pairs: {len(mismatch_results)}")
    #
    #     if mismatch_results:
    #         top = mismatch_results[0]
    #         print(f"      Top: {top['region1']}-{top['region2']}")
    #         print(f"        Mismatch: {top['mismatch_combined']:.3f}")
    #         print(f"        P-value: {top['p_value']:.4f}")
    #
    #         # ğŸ” æ˜¾ç¤ºtop 5ç”¨äºéªŒè¯
    #         print(f"      Top 5 pairs:")
    #         for i, pair in enumerate(mismatch_results[:5], 1):
    #             print(f"        {i}. {pair['region1']}-{pair['region2']}: {pair['mismatch_combined']:.3f}")
    #
    #     return {
    #         'success': True,
    #         'data': mismatch_results,
    #         'rows': len(mismatch_results),
    #         'analysis_type': 'cross_modal_mismatch',
    #         'computation_time': elapsed,
    #         'method': 'figure4_compatible'
    #     }
    def _compute_mismatch_matrix(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """
        è®¡ç®—cross-modal mismatchçŸ©é˜µ (å®Œå…¨å¯¹é½Ground Truth result4.py)

        ğŸ”§ å…³é”®ä¿®å¤ï¼š
        1. Step 0: å…¨å±€å½¢æ€æ ‡å‡†åŒ–ï¼ˆä¸€æ¬¡æ€§ï¼Œæ‰€æœ‰regionsï¼‰
        2. Step 1: ä½¿ç”¨ç¼“å­˜çš„æ ‡å‡†åŒ–æ•°æ®
        3. Step 2: æ„å»ºè·ç¦»çŸ©é˜µï¼ˆå½¢æ€è·ç¦»ç”¨Euclideanï¼‰
        4. Step 3: Min-Maxå½’ä¸€åŒ–
        5. Step 4: è®¡ç®—Mismatchå’Œç›¸ä¼¼åº¦ï¼ˆç»Ÿä¸€ç”¨å½’ä¸€åŒ–è·ç¦»ï¼‰
        """
        import time
        start_time = time.time()

        # è·å–regions
        regions = state.analysis_state.discovered_entities.get('Region', [])

        if not regions:
            for key, data in state.intermediate_data.items():
                if data and isinstance(data, list) and len(data) > 0:
                    if 'region' in data[0]:
                        regions = list(set([row['region'] for row in data if row.get('region')]))
                        break

        max_regions = params.get('max_regions', 30)
        regions = regions[:max_regions]

        if len(regions) < 2:
            return {'success': False, 'error': 'Need at least 2 regions'}

        n = len(regions)
        logger.info(f"   ğŸš€ Computing mismatch (Figure 4 method) for {n} regions...")

        # ğŸ”§ Step 0: å…¨å±€å½¢æ€æ ‡å‡†åŒ–ï¼ˆåªåšä¸€æ¬¡ï¼‰
        logger.info(f"   ğŸ”§ Step 0/4: Global morphology standardization...")
        if not hasattr(self.fingerprint, '_morph_cache'):
            self.fingerprint.standardize_morphology_globally(regions)

        # ğŸš€ Step 1: æ‰¹é‡è·å–fingerprints
        logger.info(f"   ğŸ“Š Step 1/4: Batch fetching fingerprints...")

        fingerprints = {}
        failed_regions = []

        for region in regions:
            try:
                mol = self.fingerprint.compute_molecular_fingerprint(region)
                # ğŸ”§ ä½¿ç”¨ç¼“å­˜çš„å…¨å±€æ ‡å‡†åŒ–å½¢æ€æ•°æ®
                morph = self.fingerprint._morph_cache.get(region)
                proj = self.fingerprint.compute_projection_fingerprint(region)

                if mol is not None and morph is not None and proj is not None:
                    fingerprints[region] = {
                        'molecular': mol,
                        'morphological': morph,  # å·²ç»æ˜¯z-scoredçš„
                        'projection': proj
                    }
                else:
                    failed_regions.append(region)

            except Exception as e:
                logger.warning(f"      Failed {region}: {e}")
                failed_regions.append(region)

        valid_regions = [r for r in regions if r not in failed_regions]
        n_valid = len(valid_regions)

        logger.info(f"      âœ“ Got fingerprints: {len(fingerprints)}/{n}")

        if n_valid < 2:
            return {'success': False, 'error': 'Insufficient valid regions'}

        # ğŸš€ Step 2: æ„å»ºè·ç¦»çŸ©é˜µ (NxN)
        logger.info(f"   ğŸ“ Step 2/4: Building distance matrices...")

        import numpy as np
        from scipy.spatial.distance import cosine, euclidean

        mol_dist_matrix = np.zeros((n_valid, n_valid))
        morph_dist_matrix = np.zeros((n_valid, n_valid))
        proj_dist_matrix = np.zeros((n_valid, n_valid))

        for i, region_a in enumerate(valid_regions):
            for j, region_b in enumerate(valid_regions):
                if i == j:
                    mol_dist_matrix[i, j] = 0
                    morph_dist_matrix[i, j] = 0
                    proj_dist_matrix[i, j] = 0
                    continue

                fp_a = fingerprints[region_a]
                fp_b = fingerprints[region_b]

                # åˆ†å­è·ç¦» (cosine)
                try:
                    mol_dist_matrix[i, j] = cosine(fp_a['molecular'], fp_b['molecular'])
                except:
                    mol_dist_matrix[i, j] = np.nan

                # ğŸ”§ å½¢æ€è·ç¦» (Euclidean on z-scored features)
                try:
                    morph_a = fp_a['morphological']
                    morph_b = fp_b['morphological']

                    # æ£€æŸ¥NaN
                    valid_mask = ~(np.isnan(morph_a) | np.isnan(morph_b))

                    if valid_mask.sum() >= 4:  # è‡³å°‘4ä¸ªæœ‰æ•ˆç»´åº¦
                        morph_dist_matrix[i, j] = euclidean(
                            morph_a[valid_mask],
                            morph_b[valid_mask]
                        )
                    else:
                        morph_dist_matrix[i, j] = np.nan
                except Exception as e:
                    logger.debug(f"      Morph distance failed {region_a}-{region_b}: {e}")
                    morph_dist_matrix[i, j] = np.nan

                # æŠ•å°„è·ç¦» (cosine)
                try:
                    proj_dist_matrix[i, j] = cosine(fp_a['projection'], fp_b['projection'])
                except:
                    proj_dist_matrix[i, j] = np.nan

        logger.info(f"      âœ“ Distance matrices built")
        logger.info(
            f"      Molecular distance range: [{np.nanmin(mol_dist_matrix):.3f}, {np.nanmax(mol_dist_matrix):.3f}]")
        logger.info(
            f"      Morphology distance range: [{np.nanmin(morph_dist_matrix):.3f}, {np.nanmax(morph_dist_matrix):.3f}]")
        logger.info(
            f"      Projection distance range: [{np.nanmin(proj_dist_matrix):.3f}, {np.nanmax(proj_dist_matrix):.3f}]")

        # ç»Ÿè®¡NaNæ•°é‡
        n_total = mol_dist_matrix.size
        n_mol_nan = np.isnan(mol_dist_matrix).sum()
        n_morph_nan = np.isnan(morph_dist_matrix).sum()
        n_proj_nan = np.isnan(proj_dist_matrix).sum()

        logger.info(
            f"      NaN counts: mol={n_mol_nan}/{n_total}, morph={n_morph_nan}/{n_total}, proj={n_proj_nan}/{n_total}")

        # ğŸš€ Step 3: Min-Maxå½’ä¸€åŒ– (å…¨å±€)
        logger.info(f"   ğŸ”§ Step 3/4: Normalizing distance matrices...")

        def minmax_normalize(matrix):
            """Min-Maxå½’ä¸€åŒ–åˆ°[0,1]"""
            valid = ~np.isnan(matrix)
            if valid.sum() == 0:
                return matrix

            vmin = matrix[valid].min()
            vmax = matrix[valid].max()

            if vmax - vmin < 1e-9:
                return np.zeros_like(matrix)

            normalized = (matrix - vmin) / (vmax - vmin)
            return normalized

        mol_norm = minmax_normalize(mol_dist_matrix)
        morph_norm = minmax_normalize(morph_dist_matrix)
        proj_norm = minmax_normalize(proj_dist_matrix)

        logger.info(f"      âœ“ Normalization complete")
        logger.info(f"      Normalized molecular range: [{np.nanmin(mol_norm):.3f}, {np.nanmax(mol_norm):.3f}]")
        logger.info(f"      Normalized morphology range: [{np.nanmin(morph_norm):.3f}, {np.nanmax(morph_norm):.3f}]")
        logger.info(f"      Normalized projection range: [{np.nanmin(proj_norm):.3f}, {np.nanmax(proj_norm):.3f}]")

        # ğŸš€ Step 4: è®¡ç®—Mismatch (å½’ä¸€åŒ–è·ç¦»çš„å·®å¼‚)
        logger.info(f"   ğŸ§® Step 4/4: Computing mismatches...")

        mismatch_results = []

        for i, region1 in enumerate(valid_regions):
            for j, region2 in enumerate(valid_regions):
                if i >= j:  # åªè®¡ç®—ä¸Šä¸‰è§’
                    continue

                # Mismatch = |normalized_distance_A - normalized_distance_B|
                mismatch_GM = abs(mol_norm[i, j] - morph_norm[i, j])
                mismatch_GP = abs(mol_norm[i, j] - proj_norm[i, j])
                mismatch_MP = abs(morph_norm[i, j] - proj_norm[i, j])

                mismatch_combined = (mismatch_GM + mismatch_GP + mismatch_MP) / 3

                # ğŸ”§ Fix: ç›¸ä¼¼åº¦ç»Ÿä¸€ä½¿ç”¨å½’ä¸€åŒ–è·ç¦»
                sim_molecular = 1 - mol_norm[i, j]  # â† ä¿®å¤ï¼šç”¨å½’ä¸€åŒ–çš„
                sim_morphological = 1 - morph_norm[i, j]  # â† å·²ç»å¯¹çš„
                sim_projection = 1 - proj_norm[i, j]  # â† ä¿®å¤ï¼šç”¨å½’ä¸€åŒ–çš„

                mismatch_results.append({
                    'region1': region1,
                    'region2': region2,
                    'mismatch_GM': float(mismatch_GM),
                    'mismatch_GP': float(mismatch_GP),
                    'mismatch_MP': float(mismatch_MP),
                    'mismatch_combined': float(mismatch_combined),
                    'sim_molecular': float(sim_molecular),
                    'sim_morphological': float(sim_morphological),
                    'sim_projection': float(sim_projection),
                    # å½’ä¸€åŒ–è·ç¦»ï¼ˆè°ƒè¯•ç”¨ï¼‰
                    'dist_molecular_norm': float(mol_norm[i, j]),
                    'dist_morphological_norm': float(morph_norm[i, j]),
                    'dist_projection_norm': float(proj_norm[i, j]),
                })

        # ç»Ÿè®¡æ£€éªŒ
        all_mismatches = [r['mismatch_combined'] for r in mismatch_results]
        mean_m = np.mean(all_mismatches)
        std_m = np.std(all_mismatches)

        for result in mismatch_results:
            m = result['mismatch_combined']

            if std_m > 0:
                z_score = (m - mean_m) / std_m
                from scipy import stats
                p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))
            else:
                z_score = 0
                p_value = 1.0

            result['p_value'] = float(p_value)
            result['z_score'] = float(z_score)
            result['effect_size'] = float(m)
            result['n_permutations'] = 0

        mismatch_results.sort(key=lambda x: x['mismatch_combined'], reverse=True)

        elapsed = time.time() - start_time

        logger.info(f"   âœ… Completed in {elapsed:.1f}s")
        logger.info(f"      Total pairs: {len(mismatch_results)}")

        if mismatch_results:
            top = mismatch_results[0]
            logger.info(f"      Top: {top['region1']}-{top['region2']}")
            logger.info(f"        Mismatch: {top['mismatch_combined']:.3f}")
            logger.info(f"        P-value: {top['p_value']:.4f}")

            # ğŸ” æ˜¾ç¤ºtop 5ç”¨äºéªŒè¯
            logger.info(f"      Top 5 pairs:")
            for i, pair in enumerate(mismatch_results[:5], 1):
                logger.info(f"        {i}. {pair['region1']}-{pair['region2']}: {pair['mismatch_combined']:.3f}")

        return {
            'success': True,
            'data': mismatch_results,
            'rows': len(mismatch_results),
            'analysis_type': 'cross_modal_mismatch',
            'computation_time': elapsed,
            'method': 'figure4_fully_aligned'
        }

    def _compute_cosine_similarity(self, vec1, vec2):
        """
        å¿«é€Ÿè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦

        ğŸš€ ä¼˜åŒ–: ä½¿ç”¨NumPyå‘é‡åŒ–æ“ä½œ
        """
        import numpy as np

        if not vec1 or not vec2:
            return 0.0

        # è½¬æ¢ä¸ºNumPyæ•°ç»„
        v1 = np.array(vec1, dtype=float)
        v2 = np.array(vec2, dtype=float)

        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        if len(v1) != len(v2):
            # Padæˆ–truncate
            max_len = max(len(v1), len(v2))
            if len(v1) < max_len:
                v1 = np.pad(v1, (0, max_len - len(v1)))
            if len(v2) < max_len:
                v2 = np.pad(v2, (0, max_len - len(v2)))

        # ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(v1, v2)
        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return float(dot_product / (norm1 * norm2))

    def _interpret_statistical_result(self, test_result: Dict, effect_size: float) -> str:
        """è§£é‡Šç»Ÿè®¡ç»“æœ"""
        p_value = test_result['p_value']

        if p_value < 0.001:
            sig_level = "highly significant (p < 0.001)"
        elif p_value < 0.01:
            sig_level = "very significant (p < 0.01)"
        elif p_value < 0.05:
            sig_level = "significant (p < 0.05)"
        else:
            sig_level = "not significant (p â‰¥ 0.05)"

        if abs(effect_size) > 0.8:
            effect_desc = "large effect size"
        elif abs(effect_size) > 0.5:
            effect_desc = "medium effect size"
        elif abs(effect_size) > 0.2:
            effect_desc = "small effect size"
        else:
            effect_desc = "negligible effect size"

        return f"The difference is {sig_level} with a {effect_desc} (Cohen's d = {effect_size:.2f})"

    def _resolve_parameters(self,
                            step: ReasoningStep,
                            state: EnhancedAgentState,
                            params: Dict) -> Dict:
        """è§£ææ­¥éª¤ä¾èµ–çš„å‚æ•°"""
        resolved = params.copy()

        # æŸ¥æ‰¾ä¾èµ–æ­¥éª¤çš„æ•°æ®
        for dep_num in step.depends_on:
            dep_key = f"step_{dep_num}"
            if dep_key in state.intermediate_data:
                dep_data = state.intermediate_data[dep_key]

                if not dep_data:
                    continue

                # æå–å¸¸ç”¨å­—æ®µ
                # æå–region acronyms
                regions = []
                for row in dep_data:
                    if 'region' in row:
                        regions.append(row['region'])
                    elif 'acronym' in row:
                        regions.append(row['acronym'])

                if regions:
                    resolved['enriched_regions'] = list(set(regions))[:10]
                    resolved['target_regions'] = list(set(regions))[:10]

                # æå–targets
                targets = []
                for row in dep_data:
                    if 'target' in row:
                        targets.append(row['target'])
                    elif 'target_region' in row:
                        targets.append(row['target_region'])

                if targets:
                    resolved['targets'] = list(set(targets))[:10]

        return resolved

    # ==================== Intelligent Replanning ====================

    def _intelligent_replan(self, state: EnhancedAgentState, from_step: int) -> bool:
        """
        æ™ºèƒ½é‡è§„åˆ’

        ä½¿ç”¨:
        - ç»“æ„åŒ–åæ€çš„å»ºè®®
        - æ›¿ä»£å‡è®¾
        - Schemaä¸­çš„æ›¿ä»£è·¯å¾„
        """
        logger.info(f"ğŸ”„ Intelligent replanning from step {from_step}")
        state.replanning_count += 1

        # è·å–æœ€è¿‘çš„ç»“æ„åŒ–åæ€
        if state.structured_reflections:
            last_reflection = state.structured_reflections[-1]

            # ä½¿ç”¨åæ€ä¸­çš„å»ºè®®
            logger.info(f"   Using reflection recommendations:")
            for rec in last_reflection.next_step_recommendations:
                logger.info(f"     â€¢ {rec}")

            # å¦‚æœæœ‰æ›¿ä»£å‡è®¾,å°è¯•ä½¿ç”¨
            if last_reflection.alternative_hypotheses:
                logger.info(f"   Found {len(last_reflection.alternative_hypotheses)} alternative hypotheses")

        # é‡æ–°ç”Ÿæˆè®¡åˆ’ (ä½¿ç”¨ç°æœ‰å®ä½“)
        try:
            query_plans = self.path_planner.generate_plan(
                state.entity_clusters,
                state.question
            )

            # æ›¿æ¢å‰©ä½™æ­¥éª¤
            new_steps = self._llm_refine_plans(query_plans, state)

            # æ›´æ–°plan,ä¿ç•™å·²æ‰§è¡Œçš„
            state.reasoning_plan = state.reasoning_plan[:from_step - 1] + new_steps

            logger.info(f"   âœ… Replanned with {len(new_steps)} new steps")
            return True

        except Exception as e:
            logger.error(f"   âŒ Replanning failed: {e}")
            return False

    # ==================== Answer Synthesis ====================

    def _synthesize_answer(self, state: EnhancedAgentState) -> str:
        """
        åˆæˆæœ€ç»ˆç­”æ¡ˆ (å¢å¼ºç‰ˆ - ç§‘å­¦å™äº‹)
        """
        # å‡†å¤‡è¯æ®æ‘˜è¦
        evidence = []
        for step in state.executed_steps:
            if step.actual_result and step.actual_result.get('success'):
                data_count = len(step.actual_result.get('data', []))
                evidence.append(f"- Step {step.step_number}: {step.purpose} ({data_count} results)")

        evidence_text = "\n".join(evidence)

        # å‡†å¤‡å…³é”®å‘ç°
        key_data = {}
        for step in state.executed_steps:
            if step.actual_result and step.actual_result.get('success'):
                data = step.actual_result.get('data', [])
                if data:
                    key_data[f"step_{step.step_number}"] = data[:5]  # Top 5

        # å‡†å¤‡ç»“æ„åŒ–åæ€æ‘˜è¦
        reflection_summary = []
        for r in state.structured_reflections:
            reflection_summary.append(
                f"Step {r.step_number}: {r.validation_status.value} (confidence: {r.confidence_score:.2f})"
            )

        # ğŸ†• æ£€æµ‹åˆ†æç±»å‹
        analysis_type = self._detect_analysis_type(state)

        # ğŸ†• å‡†å¤‡PRIMARY FOCUSä¿¡æ¯
        primary_focus_info = ""
        if hasattr(state.analysis_state, 'primary_focus') and state.analysis_state.primary_focus:
            focus = state.analysis_state.primary_focus
            supporting = focus.supporting_data
            primary_focus_info = f"""
    **PRIMARY FOCUS IDENTIFIED:**
    - Region: {focus.entity_id}
    - Enrichment: {supporting.get('total_neurons', 'N/A')} neurons across {supporting.get('cluster_count', 'N/A')} clusters
    - This region shows the highest enrichment and was selected for deep characterization
    """

        prompt = f"""Synthesize a comprehensive, publication-quality answer based on the multi-step analysis.

    **CRITICAL: Write as a SCIENTIFIC NARRATIVE, not a data report!**

    **Original Question:** {state.question}

    **Analysis Type Detected:** {analysis_type}

    **Entities Recognized:** {', '.join([e['text'] for e in state.entities[:5]])}

    {primary_focus_info}

    **Reasoning Steps Executed:**
    {chr(10).join([f"{i + 1}. {s.purpose}" for i, s in enumerate(state.executed_steps)])}

    **Evidence Collected:**
    {evidence_text}

    **Key Findings (quantitative data):**
    {json.dumps(key_data, indent=2, default=str)[:3000]}

    **Structured Reflections:**
    {chr(10).join(reflection_summary)}

    **Your Task:**

    Write a comprehensive answer with the following structure:

    ### [Title - Generate an engaging title]

    #### Introduction (1 paragraph)
    - Open with the biological significance
    - State the main finding concisely

    #### Multi-Modal Analysis Results

    **1. Molecular Characterization**
    - Cite SPECIFIC numbers (e.g., "18,474 neurons across 4 clusters")
    - Mention key markers and cell types
    - Use quantitative language

    **2. Spatial Distribution**
    - List regions with enrichment metrics
    - Highlight PRIMARY focus if identified
    - Use percentages and rankings

    **3. Morphological Features** (if available)
    - Report mean Â± SD for axonal/dendritic measurements
    - Compare to baseline if applicable
    - Interpret structural specializations

    **4. Connectivity Patterns** (if available)
    - Describe projection targets with weights
    - Categorize by functional systems (sensory/motor/associative)
    - Mention top 3-5 targets quantitatively

    **5. Target Characterization (CLOSED LOOP)** (if available)
    - Describe cell type composition of projection targets
    - Connect back to molecular findings
    - Emphasize circuit-level integration

    **6. Statistical Validation** (if available)
    - Report p-values and effect sizes
    - Mention significance levels
    - Interpret biological meaning

    #### Integration and Implications
    - Connect molecular â†’ morphological â†’ projection findings
    - Propose functional hypotheses
    - Discuss circuit-level organization

    #### Limitations and Uncertainties
    - Acknowledge data gaps honestly
    - Cite confidence scores from reflections
    - Suggest validation approaches

    **Writing Style:**
    - Use ACTIVE voice ("Our analysis revealed..." not "It was found...")
    - Connect findings CAUSALLY ("Because X, we examined Y, which revealed Z")
    - Emphasize QUANTITATIVE data (numbers, percentages, statistics)
    - Make it VISUAL-READY (structure data for plotting)
    - Be HONEST about uncertainties

    **Avoid:**
    - Lists without narrative flow
    - Vague statements ("some regions", "several")
    - Overconfident claims
    - Jargon without explanation

    Generate a publication-quality narrative now.
    """

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system",
                     "content": "You are a neuroscience writer synthesizing research analysis results into publication-quality narratives."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1500
            )

            answer = response.choices[0].message.content.strip()
            state.final_answer = answer

            # ä¼°ç®—ç½®ä¿¡åº¦
            state.confidence_score = self._estimate_confidence(state)

            return answer

        except Exception as e:
            logger.error(f"Synthesis failed: {e}")

            # Enhanced fallback: Generate structured answer from collected data
            return self._generate_fallback_answer(state)

    def _generate_fallback_answer(self, state: EnhancedAgentState) -> str:
        """
        Generate a structured answer from collected data when LLM synthesis fails.
        This provides useful output even without OpenAI API access.
        """
        lines = ["## Analysis Results (Auto-Generated Summary)", ""]

        # Question
        lines.append(f"**Question:** {state.question}")
        lines.append("")

        # Entities found
        if state.entities:
            lines.append("### Entities Identified")
            for e in state.entities[:5]:
                lines.append(f"- **{e.get('text', 'Unknown')}** ({e.get('type', 'Unknown')})")
            lines.append("")

        # Primary focus if identified
        if hasattr(state, 'analysis_state') and hasattr(state.analysis_state, 'primary_focus'):
            focus = state.analysis_state.primary_focus
            if focus:
                lines.append("### Primary Focus Region")
                lines.append(f"- **Region:** {focus.entity_id}")
                if focus.supporting_data:
                    sd = focus.supporting_data
                    lines.append(f"- **Total Neurons:** {sd.get('total_neurons', 'N/A'):,}")
                    lines.append(f"- **Cluster Count:** {sd.get('cluster_count', 'N/A')}")
                lines.append("")

        # Step results
        if state.executed_steps:
            lines.append("### Analysis Steps Completed")
            for step in state.executed_steps:
                result = step.actual_result or {}
                success = result.get('success', False)
                row_count = result.get('rows', 0)
                status = "âœ“" if success else "âœ—"
                lines.append(f"{status} **Step {step.step_number}:** {step.purpose}")
                if success and row_count > 0:
                    lines.append(f"  - Retrieved {row_count} data points")

                    # Extract key findings
                    data = result.get('data', [])
                    if data and isinstance(data, list) and len(data) > 0:
                        first_row = data[0]
                        if isinstance(first_row, dict):
                            # Show top results based on step type
                            if 'region' in first_row or 'acronym' in first_row:
                                regions = [r.get('region') or r.get('acronym') for r in data[:5]]
                                lines.append(f"  - Top regions: {', '.join(filter(None, regions))}")
                            if 'target' in first_row:
                                targets = [r.get('target') for r in data[:5]]
                                lines.append(f"  - Top targets: {', '.join(filter(None, targets))}")
                            if 'cluster' in first_row:
                                clusters = [r.get('cluster') for r in data[:3]]
                                lines.append(f"  - Clusters: {', '.join(filter(None, clusters))}")
                lines.append("")

        # Modalities covered
        if hasattr(state, 'analysis_state') and state.analysis_state.modalities_covered:
            lines.append("### Modalities Analyzed")
            for mod in state.analysis_state.modalities_covered:
                lines.append(f"- {mod.capitalize()}")
            lines.append("")

        # Discovered entities
        if hasattr(state, 'analysis_state') and state.analysis_state.discovered_entities:
            lines.append("### Entities Discovered During Analysis")
            for etype, entities in state.analysis_state.discovered_entities.items():
                if entities:
                    lines.append(f"- **{etype}:** {len(entities)} found")
                    if len(entities) <= 5:
                        lines.append(f"  - {', '.join(str(e) for e in entities)}")
            lines.append("")

        # Summary
        lines.append("### Summary")
        lines.append(f"- **Steps Executed:** {len(state.executed_steps)}")
        lines.append(f"- **Modalities:** {', '.join(state.analysis_state.modalities_covered) if hasattr(state, 'analysis_state') else 'N/A'}")

        state.confidence_score = self._estimate_confidence(state)
        lines.append(f"- **Confidence:** {state.confidence_score:.2f}")
        lines.append("")
        lines.append("*Note: This is an auto-generated summary. LLM-based narrative synthesis was unavailable.*")

        return "\n".join(lines)

    def _detect_analysis_type(self, state: EnhancedAgentState) -> str:
        """æ£€æµ‹åˆ†æç±»å‹"""
        step_purposes = [s.purpose.lower() for s in state.executed_steps]

        if any('compare' in p or 'versus' in p for p in step_purposes):
            return "Comparative Analysis"
        elif any('mismatch' in p or 'screening' in p for p in step_purposes):
            return "Systematic Screening (Figure 4 type)"
        elif any('primary focus' in p or 'closed loop' in p for p in step_purposes):
            return "Focus-Driven Deep Analysis (Figure 3 type)"
        else:
            return "General Multi-Modal Analysis"

    # ==================== Utilities ====================

    def _step_to_dict(self, step: ReasoningStep) -> Dict:
        """è½¬æ¢æ­¥éª¤ä¸ºå­—å…¸ (ä¿®å¤ç‰ˆ - ä¿ç•™å®Œæ•´actual_result)"""
        step_dict = {
            'step_number': step.step_number,
            'purpose': step.purpose,
            'action': step.action,
            'rationale': step.rationale,
            'expected_result': step.expected_result,
            'actual_result_summary': {
                'success': step.actual_result.get('success') if step.actual_result else False,
                'row_count': len(step.actual_result.get('data', [])) if step.actual_result else 0
            },
            'reflection': step.reflection,
            'validation_passed': step.validation_passed,
            'execution_time': step.execution_time,
            'modality': step.modality
        }

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¿ç•™å®Œæ•´çš„actual_resultç”¨äºç»˜å›¾
        if step.actual_result:
            step_dict['actual_result'] = step.actual_result

        return step_dict

    # aipom_v10_production.py, Line ~1160
    def _estimate_confidence(self, state: EnhancedAgentState) -> float:
        """ä¼°ç®—ç½®ä¿¡åº¦ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""

        if not state.structured_reflections:
            return 0.5

        # ä½¿ç”¨ç»“æ„åŒ–åæ€çš„ç½®ä¿¡åº¦
        confidences = [r.confidence_score for r in state.structured_reflections]
        avg_confidence = sum(confidences) / len(confidences)

        # ğŸ”§ ä¿®å¤ï¼šä¸æƒ©ç½šæ­¥éª¤å°‘äºè®¡åˆ’
        # å› ä¸ºadaptiveå¯èƒ½åˆç†åœ°æå‰ç»ˆæ­¢
        if state.reasoning_plan:
            completion_rate = len(state.executed_steps) / len(state.reasoning_plan)
        else:
            completion_rate = 1.0  # æ²¡æœ‰è®¡åˆ’ï¼Œè®¤ä¸ºæ˜¯å®Œæˆçš„

        # ğŸ”§ æ”¾å®½completion_rateçš„å½±å“
        completion_factor = 0.85 + 0.15 * completion_rate  # åŸæ¥æ˜¯ 0.7 + 0.3 * rate

        # Factor 2: é‡è§„åˆ’æƒ©ç½š
        replan_penalty = 0.95 ** state.replanning_count

        # ç»¼åˆ
        final_confidence = avg_confidence * completion_factor * replan_penalty

        return min(1.0, max(0.0, final_confidence))

    def _build_error_response(self, question: str, error: str, start_time: float) -> Dict:
        """æ„å»ºé”™è¯¯å“åº”"""
        return {
            'question': question,
            'answer': f"Analysis failed: {error}",
            'error': error,
            'execution_time': time.time() - start_time,
            'success': False,
            'entities_recognized': [],
            'reasoning_plan': [],
            'executed_steps': [],
            'reflections': [],
            'confidence_score': 0.0
        }

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        self.db.close()


# ==================== Test ====================

def test_v10_agent():
    """æµ‹è¯•V10 agent"""
    import os

    print("\n" + "=" * 80)
    print("AIPOM-CoT V10 PRODUCTION TEST")
    print("=" * 80)

    agent = AIPOMCoTV10(
        neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        neo4j_user=os.getenv("NEO4J_USER", "neo4j"),
        neo4j_pwd=os.getenv("NEO4J_PASSWORD", "neuroxiv"),
        database=os.getenv("NEO4J_DATABASE", "neo4j"),
        schema_json_path="./schema_output/schema.json",
        openai_api_key=os.getenv("OPENAI_API_KEY", ''),
        model="gpt-4o"
    )

    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "Tell me about Car3+ neurons",
        "Compare Pvalb and Sst interneurons in MOs",
        "What are the projection targets of the claustrum?"
    ]

    for q in test_questions:
        print(f"\n{'=' * 80}")
        print(f"Q: {q}")
        print('=' * 80)

        result = agent.answer(q, max_iterations=8)

        print(f"\nâœ… Results:")
        print(f"   Entities: {len(result['entities_recognized'])}")
        print(f"   Steps: {result['total_steps']}")
        print(f"   Confidence: {result['confidence_score']:.3f}")
        print(f"   Time: {result['execution_time']:.2f}s")
        print(f"\nğŸ’¡ Answer:\n{result['answer'][:300]}...\n")

    agent.close()


def test_car3_comprehensive():
    """æµ‹è¯•Car3çš„å®Œæ•´åˆ†æ"""

    agent = AIPOMCoTV10(
        neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        neo4j_user=os.getenv("NEO4J_USER", "neo4j"),
        neo4j_pwd=os.getenv("NEO4J_PASSWORD", "neuroxiv"),
        database=os.getenv("NEO4J_DATABASE", "neo4j"),
        schema_json_path="./schema_output/schema.json",
        openai_api_key=os.getenv("OPENAI_API_KEY", ''),
        model="gpt-4o"
    )

    # ğŸ¯ å…³é”®: ä½¿ç”¨"comprehensive"è§¦å‘æ·±åº¦åˆ†æ
    # question = "Give me a comprehensive analysis of Car3+ neurons"
    question = "Which brain region pairs show the highest cross-modal mismatch between molecular fingerprints, morphological features, and projection patterns among the top 30 brain regions with most neurons?"
    # result = agent.answer(question, max_iterations=12)
    result = agent.answer_with_visualization(
        question,
        max_iterations=10,
        generate_plots=True,
        output_dir='./figure4_automatic_output'
    )
    print("\n" + "=" * 80)
    print("FIGURE 3 STORY ARC ANALYSIS")
    print("=" * 80)

    print(f"\nTarget Depth: {result['adaptive_planning']['target_depth']}")
    print(f"Steps Executed: {result['adaptive_planning']['final_depth']}")
    print(f"Modalities: {', '.join(result['adaptive_planning']['modalities_covered'])}")

    print("\n" + "-" * 80)
    print("STEP-BY-STEP NARRATIVE:")
    print("-" * 80)

    for i, step in enumerate(result['executed_steps'], 1):
        print(f"\n{i}. {step['purpose']}")
        print(f"   Modality: {step['modality']}")
        print(f"   Data: {step['actual_result_summary']['row_count']} rows")
        print(f"   Confidence: {step['reflection']}")

    print("\n" + "-" * 80)
    print("ENTITIES DISCOVERED:")
    print("-" * 80)
    for entity_type, count in result['adaptive_planning']['entities_discovered'].items():
        print(f"  â€¢ {entity_type}: {count}")

    print("\n" + "-" * 80)
    print("VALIDATION CHECKLIST:")
    print("-" * 80)

    modalities = result['adaptive_planning']['modalities_covered']
    entities = result['adaptive_planning']['entities_discovered']

    checks = {
        'Has molecular analysis': 'molecular' in modalities,
        'Has morphological analysis': 'morphological' in modalities,
        'Has projection analysis': 'projection' in modalities,
        'Found regions': 'Region' in entities and entities['Region'] > 0,
        'Found projection targets': 'ProjectionTarget' in entities and entities['ProjectionTarget'] > 0,
        'Analyzed target composition': any(
            'target' in s['purpose'].lower() and 'composition' in s['purpose'].lower() for s in
            result['executed_steps'])
    }

    for check, passed in checks.items():
        status = "âœ…" if passed else "âŒ"
        print(f"  {status} {check}")

    # è®¡ç®—å®Œæ•´æ€§åˆ†æ•°
    completeness = sum(checks.values()) / len(checks) * 100
    print(f"\nğŸ“Š Story Completeness: {completeness:.0f}%")

    if completeness >= 80:
        print("\nğŸ‰ âœ… FIGURE 3 COMPLETE STORY ARC ACHIEVED!")
    else:
        print(f"\nâš ï¸  Story incomplete - missing {100 - completeness:.0f}% of elements")

    print("\n" + "=" * 80)
    print("FINAL ANSWER:")
    print("=" * 80)
    print(result['answer'])

    agent.close()

    return result

# ==================== CLI Interface (CC_SPEC_MS) ====================

def create_agent_from_args(args) -> 'AIPOMCoTV10':
    """Create agent with env var priority for Neo4j connection"""
    return AIPOMCoTV10(
        neo4j_uri=os.getenv("NEO4J_URI", "bolt://100.88.72.32:7687"),
        neo4j_user=os.getenv("NEO4J_USER", "neo4j"),
        neo4j_pwd=os.getenv("NEO4J_PASSWORD", "neuroxiv"),
        database=os.getenv("NEO4J_DATABASE", "neo4j"),
        schema_json_path="./schema_output/schema.json",
        openai_api_key=os.getenv("OPENAI_API_KEY", ''),
        model="gpt-4o"
    )


def run_chat_mode(args):
    """
    Chat mode with intent gating.
    SMALLTALK triggers zero KG queries; other intents are budgeted.
    """
    router = IntentRouter()
    query = args.query or input("Enter your query: ")

    # Classify intent
    intent = router.classify(query)
    logger.info(f"Intent classified: {intent.value}")

    # Handle SMALLTALK without KG queries
    if intent == IntentType.SMALLTALK:
        response = get_smalltalk_response(query)
        print(f"\n{response}")
        return {'intent': intent.value, 'response': response, 'kg_queries': 0}

    # Get budget limits
    budget_limits = get_budget_for_intent(intent, args.budget)
    logger.info(f"Budget limits: queries={budget_limits.max_kg_queries}, rows={budget_limits.row_limit}")

    # Initialize provenance
    prov = create_provenance_logger(run_id=f"chat_{int(time.time())}", seed=args.seed)
    prov.log_run_start(mode='chat', intent=intent.value, query=query, budget=args.budget)

    # Create agent and run
    agent = create_agent_from_args(args)

    try:
        # Set determinism
        random.seed(args.seed)
        np.random.seed(args.seed)

        # Run with budget-aware max iterations
        result = agent.answer(query, max_iterations=budget_limits.max_plan_steps or 6)

        prov.log_run_end(
            termination_reason='completed',
            total_steps=result.get('total_steps', 0),
            total_kg_queries=len(result.get('executed_steps', [])),
            execution_time=result.get('execution_time', 0),
            success=True
        )

        print(f"\n{'='*80}")
        print("ANSWER:")
        print('='*80)
        print(result['answer'])

        return result
    finally:
        agent.close()


def _write_fail_fast_report(output_dir: Path, case_name: str, result: Dict,
                             evidence: EvidenceBuffer, prov: ProvenanceLogger, seed: int):
    """Write FAILED report with full diagnostics when evidence is missing.

    Called when kg_query_count==0, evidence_coverage==0, or execution errors occur.
    Never outputs numeric narrative without evidence â€” writes diagnostics instead.
    """
    lines = [
        f"# MS {case_name.upper()} Report â€” FAILED",
        "",
        f"**Status:** FAILED â€” No KG evidence collected",
        f"**Seed:** {seed}",
        f"**KG Queries:** {evidence.get_kg_query_count()}",
        f"**Evidence Coverage:** {evidence.get_coverage_rate():.0%}",
        "",
        "## Diagnostics",
        "",
        f"- Error: {result.get('error', 'Unknown')}",
        f"- Neo4j connected: {result.get('neo4j_status', {}).get('connected', 'Unknown')}",
        f"- Neo4j error: {result.get('neo4j_status', {}).get('error', 'N/A')}",
        "",
        "## Evidence Buffer State",
        "",
        evidence.to_markdown(),
        "",
        "## Traceback",
        "",
        "```",
        result.get('traceback', 'No traceback available'),
        "```",
    ]
    report_path = output_dir / "report.md"
    report_path.parent.mkdir(parents=True, exist_ok=True)
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(lines))

    # Also log to provenance
    prov.log_reflect(
        step_number=0,
        validation_status='fail_fast',
        confidence=0.0,
        should_replan=False,
        recommendations=[f'FAIL-FAST: {result.get("error", "No evidence")}']
    )


def run_analysis_mode(args):
    """
    Scientific analysis mode with deterministic execution and provenance tracing.
    Available analyses: reasoning, fingerprint, circuit.
    """
    if not args.analysis:
        print("Error: --analysis is required for analysis mode (reasoning, fingerprint, or circuit)")
        return None

    # Set determinism
    random.seed(args.seed)
    np.random.seed(args.seed)

    # Create output directories
    output_base = Path(f"./outputs/{args.analysis}_analysis")
    output_base.mkdir(parents=True, exist_ok=True)
    (output_base / "data").mkdir(exist_ok=True)

    # Initialize provenance
    prov = create_provenance_logger(run_id=f"{args.analysis}_analysis", seed=args.seed)
    prov.log_run_start(
        mode='analysis',
        intent='SCIENTIFIC_ANALYSIS',
        query=f"Analysis: {args.analysis}",
        budget='heavy',
        snapshot_id=args.snapshot_id
    )

    if args.analysis == 'reasoning':
        result = run_reasoning_demo(args, output_base, prov)
    elif args.analysis == 'fingerprint':
        result = run_fingerprint_analysis(args, output_base, prov)
    elif args.analysis == 'circuit':
        result = run_circuit_discovery(args, output_base, prov)
    else:
        print(f"Unknown analysis type: {args.analysis}")
        return None

    print(f"\nOutputs saved to: {output_base}")
    print(f"Provenance trace: {prov.get_trace_path()}")

    return result


def run_reasoning_demo(args, output_base: Path, prov: ProvenanceLogger) -> Dict:
    """
    Reasoning Demo: AIPOM-CoT reasoning workflow demonstration.
    Demonstrates the agent's multi-step reasoning and evidence gathering.
    Expected: >=4 plan steps, >=2 modalities, structured report.
    """
    prompt = "è¯·å±•ç¤ºä½ çš„AIPOM-CoTæ¨ç†æµç¨‹ï¼Œå¹¶è§£é‡Šä½ å°†å¦‚ä½•ä»çŸ¥è¯†å›¾è°±ä¸­è·å–è¯æ®ã€‚"

    prov.log_think("Starting reasoning demonstration", {'prompt': prompt})

    agent = create_agent_from_args(args)
    evidence = EvidenceBuffer()

    try:
        result = agent.answer(prompt, max_iterations=args.max_depth or 12)

        # Log plan
        prov.log_plan(
            plan_steps=[{'step': i+1, 'purpose': s['purpose']} for i, s in enumerate(result.get('executed_steps', []))],
            planner_type='adaptive'
        )

        # Record evidence for each step
        for i, step in enumerate(result.get('executed_steps', []), 1):
            modality = step.get('modality', 'general')
            if step.get('actual_result_summary', {}).get('row_count', 0) > 0:
                evidence.add_evidence(
                    modality=modality,
                    source_step=i,
                    query=f"Step {i}: {step['purpose']}",
                    data=[{'summary': step.get('actual_result_summary', {})}]
                )
            prov.log_act(
                step_number=i,
                action_type='reasoning_step',
                purpose=step['purpose'],
                result_summary=step.get('actual_result_summary', {})
            )

        # Generate report
        report_content = generate_analysis_report(
            analysis_type='reasoning',
            prompt=prompt,
            result=result,
            evidence=evidence,
            seed=args.seed
        )

        # Save outputs
        with open(output_base / "report.md", 'w', encoding='utf-8') as f:
            f.write(report_content)

        # Save step details as JSON
        with open(output_base / "data" / "steps.json", 'w', encoding='utf-8') as f:
            json.dump(result.get('executed_steps', []), f, indent=2, ensure_ascii=False, default=str)

        prov.log_run_end(
            termination_reason='completed',
            total_steps=result.get('total_steps', 0),
            total_kg_queries=evidence.get_kg_query_count(),
            execution_time=result.get('execution_time', 0)
        )

        return result
    finally:
        agent.close()


def run_fingerprint_analysis(args, output_base: Path, prov: ProvenanceLogger) -> Dict:
    """
    Cross-Modal Brain Region Fingerprint Analysis.

    AGENT-DRIVEN IMPLEMENTATION: Uses FingerprintAgent with full TPAR workflow.
    The agent reasons about which fingerprints to compute and how to analyze them.

    Outputs:
    - data/similarity_molecule.csv
    - data/similarity_morphology.csv
    - data/similarity_projection.csv
    - data/mismatch_mol_morph.csv
    - data/mismatch_mol_proj.csv
    - figures/*.png
    - report.md
    """
    from fingerprint_agent import FingerprintAgent

    prov.log_think("Starting fingerprint analysis: Cross-modal mismatch analysis (agent-driven)", {
        'implementation': 'FingerprintAgent with TPAR workflow',
        'top_n_regions': args.top_n_regions if hasattr(args, 'top_n_regions') else 30
    })

    # Initialize agent with TPAR reasoning
    agent = FingerprintAgent(
        seed=args.seed,
        output_dir=str(output_base),
        top_n_regions=getattr(args, 'top_n_regions', 30)
    )

    # Run agent analysis (full TPAR workflow)
    result = agent.run()

    # Get evidence from agent
    evidence_summary = result.get('evidence', {})
    kg_query_count = evidence_summary.get('kg_query_count', 0)
    coverage_rate = evidence_summary.get('coverage_rate', 0)

    if not result.get('success'):
        # Create evidence buffer for fail report
        evidence = EvidenceBuffer()
        _write_fail_fast_report(output_base, 'fingerprint', result, evidence, prov, args.seed)
        print(f"FAIL: Fingerprint analysis agent execution failed: {result.get('error')}")
        return result

    if kg_query_count == 0 or coverage_rate == 0:
        result['success'] = False
        result['error'] = f'FAILED: NO EVIDENCE (kg_queries={kg_query_count}, coverage={coverage_rate:.0%})'
        evidence = EvidenceBuffer()
        _write_fail_fast_report(output_base, 'fingerprint', result, evidence, prov, args.seed)
        print(f"FAIL: Fingerprint analysis - {result['error']}")
        return result

    print(f"SUCCESS: Fingerprint analysis completed with {kg_query_count} KG queries, coverage={coverage_rate:.0%}")
    print(f"Files generated: {list(result.get('files', {}).keys())}")

    return result


def run_circuit_discovery(args, output_base: Path, prov: ProvenanceLogger) -> Dict:
    """
    Gene-Centric Neural Circuit Discovery (default gene: Car3).
    AGENT-DRIVEN IMPLEMENTATION: Uses CircuitAgent with full TPAR workflow.
    The agent reasons about gene circuit analysis and generates comprehensive panels.
    Outputs: subclass list, region enrichment, morphology counts, projections.
    """
    from circuit_agent import CircuitAgent

    gene = args.gene or "Car3"

    prov.log_think(f"Starting circuit discovery: {gene} neuron profiling (agent-driven)", {'gene': gene})

    # Initialize agent with TPAR reasoning
    agent = CircuitAgent(
        gene=gene,
        seed=args.seed,
        output_dir=str(output_base)
    )

    # Run agent analysis (full TPAR workflow including report generation)
    result = agent.run()

    # Get evidence from agent
    evidence_summary = result.get('evidence', {})
    kg_count = evidence_summary.get('kg_query_count', 0)
    coverage = evidence_summary.get('coverage_rate', 0)

    if not result.get('success'):
        # Create evidence buffer for fail report
        evidence = EvidenceBuffer()
        _write_fail_fast_report(output_base, 'circuit', result, evidence, prov, args.seed)
        print(f"FAIL: Circuit discovery agent execution failed: {result.get('error')}")
        return result

    if kg_count == 0 or coverage == 0:
        result['success'] = False
        result['error'] = f'FAILED: NO EVIDENCE (kg_queries={kg_count}, coverage={coverage:.0%})'
        evidence = EvidenceBuffer()
        _write_fail_fast_report(output_base, 'circuit', result, evidence, prov, args.seed)
        print(f"FAIL: Circuit discovery - {result['error']}")
        return result

    print(f"SUCCESS: Circuit discovery completed with {kg_count} KG queries, coverage={coverage:.0%}")
    print(f"Files generated: {list(result.get('files', {}).keys())}")

    return result


def run_kg_mode(args):
    """
    General KG query mode with entity/attribute resolution.
    Supports: entity lookup, attribute queries, constrained traversal.
    """
    query = args.query
    if not query:
        print("Error: --query is required for kg mode")
        return None

    router = IntentRouter()
    intent = router.classify(query)
    logger.info(f"Intent: {intent.value}")

    # Set determinism
    random.seed(args.seed)
    np.random.seed(args.seed)

    # Get budget
    budget_limits = get_budget_for_intent(intent, args.budget)

    # Initialize provenance
    prov = create_provenance_logger(run_id=f"kg_{int(time.time())}", seed=args.seed)
    prov.log_run_start(mode='kg', intent=intent.value, query=query, budget=args.budget)

    agent = create_agent_from_args(args)

    try:
        result = agent.answer(query, max_iterations=min(budget_limits.max_plan_steps, args.max_depth or 6))

        prov.log_run_end(
            termination_reason='completed',
            total_steps=result.get('total_steps', 0),
            total_kg_queries=len(result.get('executed_steps', [])),
            execution_time=result.get('execution_time', 0)
        )

        print(f"\n{'='*80}")
        print("ANSWER:")
        print('='*80)
        print(result['answer'])

        return result
    finally:
        agent.close()


def save_fingerprint_csvs(result: Dict, data_dir: Path, prov: ProvenanceLogger):
    """Extract and save CSV data from fingerprint analysis"""
    data_dir.mkdir(parents=True, exist_ok=True)

    # Try to extract mismatch data from executed steps
    for step in result.get('executed_steps', []):
        actual = step.get('actual_result', {})
        if actual.get('success') and actual.get('data'):
            data = actual['data']
            purpose = step.get('purpose', '').lower()

            # Detect data type and save
            if data and isinstance(data[0], dict):
                if 'mismatch_GM' in data[0] or 'mismatch_combined' in data[0]:
                    # Mismatch data
                    df = pd.DataFrame(data)
                    df.to_csv(data_dir / "mismatch_pairs.csv", index=False)
                    prov.log_act(
                        step_number=step.get('step_number', 0),
                        action_type='save_csv',
                        purpose='Save mismatch pairs',
                        result_summary={'file': 'mismatch_pairs.csv', 'rows': len(data)}
                    )

    # If visualization files were generated, log them
    if 'visualization_files' in result:
        for name, path in result['visualization_files'].items():
            logger.info(f"Generated: {name} -> {path}")


def save_circuit_csvs(result: Dict, data_dir: Path, gene: str, prov: ProvenanceLogger):
    """Extract and save CSV data from circuit analysis"""
    data_dir.mkdir(parents=True, exist_ok=True)

    # Extract from intermediate_data
    intermediate = result.get('intermediate_data', {})

    for key, data in intermediate.items():
        if data and isinstance(data, list) and len(data) > 0:
            df = pd.DataFrame(data)

            # Determine filename based on content
            if 'region' in str(data[0].keys()).lower() and 'enrichment' in key.lower():
                filename = f"{gene}_region_enrichment.csv"
            elif 'subclass' in str(data[0].keys()).lower():
                filename = f"{gene}_subclass_list.csv"
            elif 'target' in str(data[0].keys()).lower():
                filename = f"{gene}_projection_targets.csv"
            else:
                filename = f"{gene}_{key}.csv"

            df.to_csv(data_dir / filename, index=False)
            prov.log_act(
                step_number=0,
                action_type='save_csv',
                purpose=f'Save {filename}',
                result_summary={'file': filename, 'rows': len(data)}
            )


def generate_analysis_report(analysis_type: str, prompt: str, result: Dict,
                       evidence: EvidenceBuffer, seed: int) -> str:
    """Generate markdown report for analysis cases"""
    lines = [
        f"# MS {analysis_type.upper()} Report",
        "",
        f"**Generated:** {time.strftime('%Y-%m-%d %H:%M:%S')}",
        f"**Seed:** {seed}",
        "",
        "## Query",
        "",
        f"> {prompt}",
        "",
        "## Execution Summary",
        "",
        f"- **Total Steps:** {result.get('total_steps', 0)}",
        f"- **Execution Time:** {result.get('execution_time', 0):.2f}s",
        f"- **Confidence:** {result.get('confidence_score', 0):.3f}",
        "",
        "## Modalities Covered",
        ""
    ]

    modalities = result.get('adaptive_planning', {}).get('modalities_covered', [])
    for mod in modalities:
        lines.append(f"- {mod}")

    lines.extend([
        "",
        "## Answer",
        "",
        result.get('answer', 'No answer generated'),
        "",
        "## Reasoning Steps",
        ""
    ])

    for i, step in enumerate(result.get('executed_steps', []), 1):
        lines.append(f"### Step {i}: {step.get('purpose', 'Unknown')}")
        lines.append("")
        lines.append(f"- **Modality:** {step.get('modality', 'N/A')}")
        summary = step.get('actual_result_summary', {})
        lines.append(f"- **Results:** {summary.get('row_count', 0)} rows")
        lines.append(f"- **Reflection:** {step.get('reflection', 'N/A')}")
        lines.append("")

    # Add evidence summary
    lines.append(evidence.to_markdown())

    return "\n".join(lines)


def main():
    """Main CLI entry point"""
    parser = argparse.ArgumentParser(
        description='AIPOM-CoT V10 Production - Neuroscience KG Agent',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Chat mode (default)
  python aipom_v10_production.py --mode chat --query "ä½ å¥½"
  python aipom_v10_production.py --mode chat --query "What is Car3?"

  # MS reproduction mode
  python aipom_v10_production.py --mode analysis --analysis reasoning --seed 42
  python aipom_v10_production.py --mode analysis --analysis fingerprint --seed 42
  python aipom_v10_production.py --mode analysis --analysis circuit --seed 42

  # General KG query mode
  python aipom_v10_production.py --mode kg --query "HIP æœ‰ä»€ä¹ˆå±æ€§"
        """
    )

    parser.add_argument('--mode', choices=['chat', 'analysis', 'kg'], default='chat',
                        help='Operation mode (default: chat)')
    parser.add_argument('--analysis', choices=['reasoning', 'fingerprint', 'circuit'],
                        help='Analysis type (required for analysis mode)')
    parser.add_argument('--query', type=str,
                        help='Natural language query')
    parser.add_argument('--gene', type=str, default='Car3',
                        help='Gene marker for circuit analysis (default: Car3)')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed for reproducibility (default: 42)')
    parser.add_argument('--snapshot-id', type=str, default=None,
                        help='Optional snapshot ID for logging')
    parser.add_argument('--max-depth', type=int, default=15,
                        help='Maximum planning depth (default: 15)')
    parser.add_argument('--budget', choices=['light', 'standard', 'heavy'], default='light',
                        help='Budget level for chat/kg modes (default: light)')
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='Enable verbose logging')

    args = parser.parse_args()

    # Configure logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # Log startup info
    logger.info(f"AIPOM-CoT V10 Production")
    logger.info(f"Mode: {args.mode}, Seed: {args.seed}, Budget: {args.budget}")

    # Dispatch to appropriate mode
    if args.mode == 'chat':
        return run_chat_mode(args)
    elif args.mode == 'analysis':
        return run_analysis_mode(args)
    elif args.mode == 'kg':
        return run_kg_mode(args)


if __name__ == "__main__":
    main()